This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: models/
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
services/
  cosyvoice/
    proto/
      __init__.py
      cosyvoice_pb2_grpc.py
      cosyvoice_pb2.py
      cosyvoice.proto
    cache.py
    client.py
    service.py
  hls/
    proto/
      __init__.py
      hls_service_pb2_grpc.py
      hls_service_pb2.py
      hls_service.proto
    __init__.py
    client.py
    server.py
    service.py
    storage.py
utils/
  concurrency.py
  ffmpeg_utils.py
  log_config.py
  redis_utils.py
  sentence_logger.py
  serialization.py
  task_state.py
  task_storage.py
  temp_file_manager.py
  worker_decorators.py
workers/
  asr_worker/
    auto_sense.py
    sentence_tools.py
    worker.py
  audio_gen_worker/
    audio_gener.py
    timestamp_adjuster.py
    worker.py
  duration_worker/
    duration_aligner.py
    worker.py
  mixer_worker/
    media_mixer.py
    worker.py
  modelin_worker/
    model_in.py
    worker.py
  segment_worker/
    audio_separator.py
    video_segmenter.py
    worker.py
  translation_worker/
    translation/
      __init__.py
      deepseek_client.py
      gemini_client.py
      glm4_client.py
      prompt.py
      translator.py
    worker.py
  tts_worker/
    tts_token_gener.py
    worker.py
__init__.py
.cursorignore
.env.example
api.py
config.py
postcss.config.json
run_cosyvoice_service.py
run_hls_service.py
video_translator.py
worker_launcher.py

================================================================
Files
================================================================

================
File: services/cosyvoice/proto/__init__.py
================
from .cosyvoice_pb2 import *
from .cosyvoice_pb2_grpc import *

================
File: services/cosyvoice/proto/cosyvoice_pb2_grpc.py
================
# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!
"""Client and server classes corresponding to protobuf-defined services."""
import grpc

from . import cosyvoice_pb2 as cosyvoice__pb2


class CosyVoiceServiceStub(object):
    """========== Service 定义 ==========
    """

    def __init__(self, channel):
        """Constructor.

        Args:
            channel: A grpc.Channel.
        """
        self.NormalizeText = channel.unary_unary(
                '/cosyvoice.CosyVoiceService/NormalizeText',
                request_serializer=cosyvoice__pb2.NormalizeTextRequest.SerializeToString,
                response_deserializer=cosyvoice__pb2.NormalizeTextResponse.FromString,
                )
        self.ExtractSpeakerFeatures = channel.unary_unary(
                '/cosyvoice.CosyVoiceService/ExtractSpeakerFeatures',
                request_serializer=cosyvoice__pb2.ExtractSpeakerFeaturesRequest.SerializeToString,
                response_deserializer=cosyvoice__pb2.ExtractSpeakerFeaturesResponse.FromString,
                )
        self.GenerateTTSTokens = channel.unary_unary(
                '/cosyvoice.CosyVoiceService/GenerateTTSTokens',
                request_serializer=cosyvoice__pb2.GenerateTTSTokensRequest.SerializeToString,
                response_deserializer=cosyvoice__pb2.GenerateTTSTokensResponse.FromString,
                )
        self.Token2Wav = channel.unary_unary(
                '/cosyvoice.CosyVoiceService/Token2Wav',
                request_serializer=cosyvoice__pb2.Token2WavRequest.SerializeToString,
                response_deserializer=cosyvoice__pb2.Token2WavResponse.FromString,
                )
        self.Cleanup = channel.unary_unary(
                '/cosyvoice.CosyVoiceService/Cleanup',
                request_serializer=cosyvoice__pb2.CleanupRequest.SerializeToString,
                response_deserializer=cosyvoice__pb2.CleanupResponse.FromString,
                )


class CosyVoiceServiceServicer(object):
    """========== Service 定义 ==========
    """

    def NormalizeText(self, request, context):
        """Missing associated documentation comment in .proto file."""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details('Method not implemented!')
        raise NotImplementedError('Method not implemented!')

    def ExtractSpeakerFeatures(self, request, context):
        """Missing associated documentation comment in .proto file."""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details('Method not implemented!')
        raise NotImplementedError('Method not implemented!')

    def GenerateTTSTokens(self, request, context):
        """Missing associated documentation comment in .proto file."""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details('Method not implemented!')
        raise NotImplementedError('Method not implemented!')

    def Token2Wav(self, request, context):
        """Missing associated documentation comment in .proto file."""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details('Method not implemented!')
        raise NotImplementedError('Method not implemented!')

    def Cleanup(self, request, context):
        """Missing associated documentation comment in .proto file."""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details('Method not implemented!')
        raise NotImplementedError('Method not implemented!')


def add_CosyVoiceServiceServicer_to_server(servicer, server):
    rpc_method_handlers = {
            'NormalizeText': grpc.unary_unary_rpc_method_handler(
                    servicer.NormalizeText,
                    request_deserializer=cosyvoice__pb2.NormalizeTextRequest.FromString,
                    response_serializer=cosyvoice__pb2.NormalizeTextResponse.SerializeToString,
            ),
            'ExtractSpeakerFeatures': grpc.unary_unary_rpc_method_handler(
                    servicer.ExtractSpeakerFeatures,
                    request_deserializer=cosyvoice__pb2.ExtractSpeakerFeaturesRequest.FromString,
                    response_serializer=cosyvoice__pb2.ExtractSpeakerFeaturesResponse.SerializeToString,
            ),
            'GenerateTTSTokens': grpc.unary_unary_rpc_method_handler(
                    servicer.GenerateTTSTokens,
                    request_deserializer=cosyvoice__pb2.GenerateTTSTokensRequest.FromString,
                    response_serializer=cosyvoice__pb2.GenerateTTSTokensResponse.SerializeToString,
            ),
            'Token2Wav': grpc.unary_unary_rpc_method_handler(
                    servicer.Token2Wav,
                    request_deserializer=cosyvoice__pb2.Token2WavRequest.FromString,
                    response_serializer=cosyvoice__pb2.Token2WavResponse.SerializeToString,
            ),
            'Cleanup': grpc.unary_unary_rpc_method_handler(
                    servicer.Cleanup,
                    request_deserializer=cosyvoice__pb2.CleanupRequest.FromString,
                    response_serializer=cosyvoice__pb2.CleanupResponse.SerializeToString,
            ),
    }
    generic_handler = grpc.method_handlers_generic_handler(
            'cosyvoice.CosyVoiceService', rpc_method_handlers)
    server.add_generic_rpc_handlers((generic_handler,))


 # This class is part of an EXPERIMENTAL API.
class CosyVoiceService(object):
    """========== Service 定义 ==========
    """

    @staticmethod
    def NormalizeText(request,
            target,
            options=(),
            channel_credentials=None,
            call_credentials=None,
            insecure=False,
            compression=None,
            wait_for_ready=None,
            timeout=None,
            metadata=None):
        return grpc.experimental.unary_unary(request, target, '/cosyvoice.CosyVoiceService/NormalizeText',
            cosyvoice__pb2.NormalizeTextRequest.SerializeToString,
            cosyvoice__pb2.NormalizeTextResponse.FromString,
            options, channel_credentials,
            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)

    @staticmethod
    def ExtractSpeakerFeatures(request,
            target,
            options=(),
            channel_credentials=None,
            call_credentials=None,
            insecure=False,
            compression=None,
            wait_for_ready=None,
            timeout=None,
            metadata=None):
        return grpc.experimental.unary_unary(request, target, '/cosyvoice.CosyVoiceService/ExtractSpeakerFeatures',
            cosyvoice__pb2.ExtractSpeakerFeaturesRequest.SerializeToString,
            cosyvoice__pb2.ExtractSpeakerFeaturesResponse.FromString,
            options, channel_credentials,
            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)

    @staticmethod
    def GenerateTTSTokens(request,
            target,
            options=(),
            channel_credentials=None,
            call_credentials=None,
            insecure=False,
            compression=None,
            wait_for_ready=None,
            timeout=None,
            metadata=None):
        return grpc.experimental.unary_unary(request, target, '/cosyvoice.CosyVoiceService/GenerateTTSTokens',
            cosyvoice__pb2.GenerateTTSTokensRequest.SerializeToString,
            cosyvoice__pb2.GenerateTTSTokensResponse.FromString,
            options, channel_credentials,
            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)

    @staticmethod
    def Token2Wav(request,
            target,
            options=(),
            channel_credentials=None,
            call_credentials=None,
            insecure=False,
            compression=None,
            wait_for_ready=None,
            timeout=None,
            metadata=None):
        return grpc.experimental.unary_unary(request, target, '/cosyvoice.CosyVoiceService/Token2Wav',
            cosyvoice__pb2.Token2WavRequest.SerializeToString,
            cosyvoice__pb2.Token2WavResponse.FromString,
            options, channel_credentials,
            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)

    @staticmethod
    def Cleanup(request,
            target,
            options=(),
            channel_credentials=None,
            call_credentials=None,
            insecure=False,
            compression=None,
            wait_for_ready=None,
            timeout=None,
            metadata=None):
        return grpc.experimental.unary_unary(request, target, '/cosyvoice.CosyVoiceService/Cleanup',
            cosyvoice__pb2.CleanupRequest.SerializeToString,
            cosyvoice__pb2.CleanupResponse.FromString,
            options, channel_credentials,
            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)

================
File: services/cosyvoice/proto/cosyvoice_pb2.py
================
# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: cosyvoice.proto
"""Generated protocol buffer code."""
from google.protobuf import descriptor as _descriptor
from google.protobuf import descriptor_pool as _descriptor_pool
from google.protobuf import symbol_database as _symbol_database
from google.protobuf.internal import builder as _builder
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()




DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x0f\x63osyvoice.proto\x12\tcosyvoice\"$\n\x14NormalizeTextRequest\x12\x0c\n\x04text\x18\x01 \x01(\t\"*\n\x15NormalizeTextResponse\x12\x11\n\ttext_uuid\x18\x01 \x01(\t\"Y\n\x1d\x45xtractSpeakerFeaturesRequest\x12\x14\n\x0cspeaker_uuid\x18\x01 \x01(\t\x12\r\n\x05\x61udio\x18\x02 \x01(\x0c\x12\x13\n\x0bsample_rate\x18\x03 \x01(\x05\"1\n\x1e\x45xtractSpeakerFeaturesResponse\x12\x0f\n\x07success\x18\x01 \x01(\x08\"C\n\x18GenerateTTSTokensRequest\x12\x11\n\ttext_uuid\x18\x01 \x01(\t\x12\x14\n\x0cspeaker_uuid\x18\x02 \x01(\t\"A\n\x19GenerateTTSTokensResponse\x12\x13\n\x0b\x64uration_ms\x18\x01 \x01(\x05\x12\x0f\n\x07success\x18\x02 \x01(\x08\"J\n\x10Token2WavRequest\x12\x11\n\ttext_uuid\x18\x01 \x01(\t\x12\x14\n\x0cspeaker_uuid\x18\x02 \x01(\t\x12\r\n\x05speed\x18\x03 \x01(\x02\"8\n\x11Token2WavResponse\x12\r\n\x05\x61udio\x18\x01 \x01(\x0c\x12\x14\n\x0c\x64uration_sec\x18\x02 \x01(\x02\"2\n\x0e\x43leanupRequest\x12\x0c\n\x04uuid\x18\x01 \x01(\t\x12\x12\n\nis_speaker\x18\x02 \x01(\x08\"\"\n\x0f\x43leanupResponse\x12\x0f\n\x07success\x18\x01 \x01(\x08\x32\xbf\x03\n\x10\x43osyVoiceService\x12R\n\rNormalizeText\x12\x1f.cosyvoice.NormalizeTextRequest\x1a .cosyvoice.NormalizeTextResponse\x12m\n\x16\x45xtractSpeakerFeatures\x12(.cosyvoice.ExtractSpeakerFeaturesRequest\x1a).cosyvoice.ExtractSpeakerFeaturesResponse\x12^\n\x11GenerateTTSTokens\x12#.cosyvoice.GenerateTTSTokensRequest\x1a$.cosyvoice.GenerateTTSTokensResponse\x12\x46\n\tToken2Wav\x12\x1b.cosyvoice.Token2WavRequest\x1a\x1c.cosyvoice.Token2WavResponse\x12@\n\x07\x43leanup\x12\x19.cosyvoice.CleanupRequest\x1a\x1a.cosyvoice.CleanupResponseb\x06proto3')

_globals = globals()
_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)
_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'cosyvoice_pb2', _globals)
if _descriptor._USE_C_DESCRIPTORS == False:

  DESCRIPTOR._options = None
  _globals['_NORMALIZETEXTREQUEST']._serialized_start=30
  _globals['_NORMALIZETEXTREQUEST']._serialized_end=66
  _globals['_NORMALIZETEXTRESPONSE']._serialized_start=68
  _globals['_NORMALIZETEXTRESPONSE']._serialized_end=110
  _globals['_EXTRACTSPEAKERFEATURESREQUEST']._serialized_start=112
  _globals['_EXTRACTSPEAKERFEATURESREQUEST']._serialized_end=201
  _globals['_EXTRACTSPEAKERFEATURESRESPONSE']._serialized_start=203
  _globals['_EXTRACTSPEAKERFEATURESRESPONSE']._serialized_end=252
  _globals['_GENERATETTSTOKENSREQUEST']._serialized_start=254
  _globals['_GENERATETTSTOKENSREQUEST']._serialized_end=321
  _globals['_GENERATETTSTOKENSRESPONSE']._serialized_start=323
  _globals['_GENERATETTSTOKENSRESPONSE']._serialized_end=388
  _globals['_TOKEN2WAVREQUEST']._serialized_start=390
  _globals['_TOKEN2WAVREQUEST']._serialized_end=464
  _globals['_TOKEN2WAVRESPONSE']._serialized_start=466
  _globals['_TOKEN2WAVRESPONSE']._serialized_end=522
  _globals['_CLEANUPREQUEST']._serialized_start=524
  _globals['_CLEANUPREQUEST']._serialized_end=574
  _globals['_CLEANUPRESPONSE']._serialized_start=576
  _globals['_CLEANUPRESPONSE']._serialized_end=610
  _globals['_COSYVOICESERVICE']._serialized_start=613
  _globals['_COSYVOICESERVICE']._serialized_end=1060
# @@protoc_insertion_point(module_scope)

================
File: services/cosyvoice/proto/cosyvoice.proto
================
syntax = "proto3";

package cosyvoice;

// ========== Service 定义 ==========
service CosyVoiceService {
  rpc NormalizeText (NormalizeTextRequest) returns (NormalizeTextResponse);
  rpc ExtractSpeakerFeatures (ExtractSpeakerFeaturesRequest) returns (ExtractSpeakerFeaturesResponse);
  rpc GenerateTTSTokens (GenerateTTSTokensRequest) returns (GenerateTTSTokensResponse);
  rpc Token2Wav (Token2WavRequest) returns (Token2WavResponse);
  rpc Cleanup (CleanupRequest) returns (CleanupResponse);
}

// ========== RPC 请求与响应 ==========

// 1) NormalizeText
message NormalizeTextRequest {
  string text = 1;
}

message NormalizeTextResponse {
  string text_uuid = 1;  // 返回文本特征的UUID
}

// 2) ExtractSpeakerFeatures
message ExtractSpeakerFeaturesRequest {
  string speaker_uuid = 1;  // 说话人特征的UUID
  bytes audio = 2;         // 音频数据
  int32 sample_rate = 3;
}

message ExtractSpeakerFeaturesResponse {
  bool success = 1;
}

// 3) GenerateTTSTokens
message GenerateTTSTokensRequest {
  string text_uuid = 1;     // 文本特征的UUID
  string speaker_uuid = 2;  // 说话人特征的UUID
}

message GenerateTTSTokensResponse {
  int32 duration_ms = 1;
  bool success = 2;
}

// 4) Token2Wav
message Token2WavRequest {
  string text_uuid = 1;     // 文本特征的UUID
  string speaker_uuid = 2;  // 说话人特征的UUID
  float speed = 3;
}

message Token2WavResponse {
  bytes audio = 1;
  float duration_sec = 2;
}

// 5) Cleanup
message CleanupRequest {
  string uuid = 1;          // 要清理的UUID（可以是text_uuid或speaker_uuid）
  bool is_speaker = 2;      // 是否清理说话人特征
}

message CleanupResponse {
  bool success = 1;
}

================
File: services/cosyvoice/cache.py
================
import threading
from typing import Dict, Optional, List, Any
import torch

class TextFeatureData:
    """存储文本特征数据"""
    def __init__(self):
        self.normalized_texts: List[str] = []
        self.text_tokens: List[torch.Tensor] = []  # [batch_size, seq_len]
        self.tts_tokens: List[List[int]] = []      # TTS tokens
        self.tts_segment_uuids: List[str] = []

class SpeakerFeatureData:
    """存储说话人特征数据，保持完整特征字典"""
    def __init__(self):
        self.features: Optional[Dict[str, Any]] = None  # 存储完整的特征字典

class FeaturesCache:
    def __init__(self):
        self._text_cache: Dict[str, TextFeatureData] = {}      # text_uuid -> TextFeatureData
        self._speaker_cache: Dict[str, SpeakerFeatureData] = {}  # speaker_uuid -> SpeakerFeatureData
        self._lock = threading.Lock()
    
    def get_text(self, text_uuid: str) -> Optional[TextFeatureData]:
        """获取文本特征数据"""
        with self._lock:
            return self._text_cache.get(text_uuid)
    
    def get_speaker(self, speaker_uuid: str) -> Optional[Dict[str, Any]]:
        """获取说话人特征字典"""
        with self._lock:
            data = self._speaker_cache.get(speaker_uuid)
            return data.features if data else None
    
    def set_text(self, text_uuid: str, data: TextFeatureData) -> None:
        """设置文本特征数据"""
        with self._lock:
            self._text_cache[text_uuid] = data
    
    def set_speaker(self, speaker_uuid: str, features: Dict[str, Any]) -> None:
        """设置说话人特征字典"""
        with self._lock:
            data = SpeakerFeatureData()
            data.features = features
            self._speaker_cache[speaker_uuid] = data
    
    def update_text_features(self, text_uuid: str, normalized_texts: List[str], text_tokens: List[torch.Tensor]) -> bool:
        """更新文本特征"""
        with self._lock:
            if text_uuid not in self._text_cache:
                data = TextFeatureData()
                data.normalized_texts = normalized_texts
                data.text_tokens = text_tokens
                self._text_cache[text_uuid] = data
            else:
                self._text_cache[text_uuid].normalized_texts = normalized_texts
                self._text_cache[text_uuid].text_tokens = text_tokens
            return True
    
    def update_speaker_features(self, speaker_uuid: str, features: Dict[str, Any]) -> bool:
        """更新说话人特征字典"""
        with self._lock:
            data = SpeakerFeatureData()
            data.features = features
            self._speaker_cache[speaker_uuid] = data
            return True
    
    def update_tts_tokens(self, text_uuid: str, tokens: List[List[int]], segment_uuids: List[str]) -> bool:
        """更新TTS tokens"""
        with self._lock:
            if text_uuid not in self._text_cache:
                return False
            data = self._text_cache[text_uuid]
            data.tts_tokens = tokens
            data.tts_segment_uuids = segment_uuids
            return True
    
    def delete(self, uuid: str, is_speaker: bool = False) -> None:
        """删除特征数据"""
        with self._lock:
            if is_speaker:
                self._speaker_cache.pop(uuid, None)
            else:
                self._text_cache.pop(uuid, None)
    
    def exists_text(self, text_uuid: str) -> bool:
        """检查文本UUID是否存在"""
        with self._lock:
            return text_uuid in self._text_cache
    
    def exists_speaker(self, speaker_uuid: str) -> bool:
        """检查说话人UUID是否存在"""
        with self._lock:
            return speaker_uuid in self._speaker_cache

================
File: services/cosyvoice/client.py
================
import grpc
import numpy as np
import torch
import logging
from typing import Tuple, Optional

from .proto import cosyvoice_pb2
from .proto import cosyvoice_pb2_grpc

logger = logging.getLogger(__name__)

class CosyVoiceClient:
    def __init__(self, address="localhost:50052"):
        self.channel = grpc.insecure_channel(address)
        self.stub = cosyvoice_pb2_grpc.CosyVoiceServiceStub(self.channel)

    def normalize_text(self, text: str) -> str:
        """
        文本标准化，返回文本UUID
        """
        try:
            req = cosyvoice_pb2.NormalizeTextRequest(text=text)
            resp = self.stub.NormalizeText(req)
            return resp.text_uuid
        except Exception as e:
            logger.error(f"NormalizeText调用失败: {e}")
            raise

    def extract_speaker_features(self, speaker_uuid: str, audio: np.ndarray, sr: int = 24000) -> bool:
        """
        提取说话人特征。
        :param speaker_uuid: 说话人标识
        :param audio: 输入音频（numpy.ndarray）
        :param sr: 采样率，默认24000
        :return: 是否成功
        """
        try:
            # 直接将numpy array转换为bytes
            audio_bytes = audio.tobytes()
            req = cosyvoice_pb2.ExtractSpeakerFeaturesRequest(
                speaker_uuid=speaker_uuid,
                audio=audio_bytes,
                sample_rate=sr
            )
            resp = self.stub.ExtractSpeakerFeatures(req)
            return resp.success
        except Exception as e:
            logger.error(f"ExtractSpeakerFeatures调用失败: {e}")
            raise

    def generate_tts_tokens(self, text_uuid: str, speaker_uuid: str) -> Tuple[int, bool]:
        """
        根据文本UUID和说话人UUID生成TTS tokens并返回预估时长
        """
        try:
            req = cosyvoice_pb2.GenerateTTSTokensRequest(text_uuid=text_uuid, speaker_uuid=speaker_uuid)
            resp = self.stub.GenerateTTSTokens(req)
            return resp.duration_ms, resp.success
        except Exception as e:
            logger.error(f"GenerateTTSTokens调用失败: {e}")
            raise

    def token2wav(self, text_uuid: str, speaker_uuid: str, speed: float = 1.0) -> Tuple[np.ndarray, float]:
        """
        将文本UUID和说话人UUID转换为音频
        """
        try:
            req = cosyvoice_pb2.Token2WavRequest(text_uuid=text_uuid, speaker_uuid=speaker_uuid, speed=speed)
            resp = self.stub.Token2Wav(req)
            audio_np = np.frombuffer(resp.audio, dtype=np.int16).astype(np.float32) / (2**15)
            return audio_np, resp.duration_sec
        except Exception as e:
            logger.error(f"Token2Wav调用失败: {e}")
            raise

    def cleanup(self, uuid: str, is_speaker: bool = False) -> bool:
        """
        清理服务端缓存
        """
        try:
            req = cosyvoice_pb2.CleanupRequest(uuid=uuid, is_speaker=is_speaker)
            resp = self.stub.Cleanup(req)
            return resp.success
        except Exception as e:
            logger.error(f"Cleanup调用失败: {e}")
            raise

================
File: services/cosyvoice/service.py
================
# services/cosyvoice/service.py

import logging
import numpy as np
import torch
import grpc
from concurrent import futures
import uuid
import os
import soundfile as sf

from .proto import cosyvoice_pb2
from .proto import cosyvoice_pb2_grpc
from .cache import FeaturesCache, TextFeatureData, SpeakerFeatureData

# 假设我们使用 CosyVoice2 作为模型
from models.CosyVoice.cosyvoice.cli.cosyvoice import CosyVoice2

logger = logging.getLogger(__name__)

class CosyVoiceServiceServicer(cosyvoice_pb2_grpc.CosyVoiceServiceServicer):
    def __init__(self, model_path="models/CosyVoice/pretrained_models/CosyVoice2-0.5B"):
        try:
            self.cosyvoice = CosyVoice2(model_path)
            self.frontend = self.cosyvoice.frontend
            self.model = self.cosyvoice.model
            self.sample_rate = self.cosyvoice.sample_rate
            self.cache = FeaturesCache()
            
            logger.info('CosyVoice服务初始化成功')
        except Exception as e:
            logger.error(f'CosyVoice服务初始化失败: {e}')
            raise

    def _generate_uuid(self) -> str:
        """生成唯一的UUID"""
        return str(uuid.uuid4())

    def NormalizeText(self, request, context):
        """
        文本标准化，生成文本特征数据并缓存
        """
        try:
            text = request.text or ""
            normalized_texts = self.frontend.text_normalize(text, split=True, text_frontend=False)
            text_tokens = []
            
            for seg in normalized_texts:
                tokens, _ = self.frontend._extract_text_token(seg)
                text_tokens.append(tokens)

            text_uuid = self._generate_uuid()
            self.cache.update_text_features(text_uuid, normalized_texts, text_tokens)

            return cosyvoice_pb2.NormalizeTextResponse(text_uuid=text_uuid)
        except Exception as e:
            logger.error(f"NormalizeText失败: {e}")
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(str(e))
            return cosyvoice_pb2.NormalizeTextResponse()

    def ExtractSpeakerFeatures(self, request, context):
        """
        提取说话人特征并缓存
        """
        try:
            speaker_uuid = request.speaker_uuid
            # 转换为tensor，确保数组可写
            audio_np = np.frombuffer(request.audio, dtype=np.float32).copy()
            audio = torch.from_numpy(audio_np).unsqueeze(0)  # [1, T]

            # 使用tensor调用frontend，这里的sample_rate是目标采样率
            features = self.frontend.frontend_cross_lingual("", audio, request.sample_rate)
            
            # 直接存储完整的特征字典
            success = self.cache.update_speaker_features(speaker_uuid, features)

            if not success:
                context.set_code(grpc.StatusCode.INTERNAL)
                context.set_details("更新说话人特征缓存失败")
                return cosyvoice_pb2.ExtractSpeakerFeaturesResponse()

            return cosyvoice_pb2.ExtractSpeakerFeaturesResponse(success=True)
        except Exception as e:
            logger.error(f"ExtractSpeakerFeatures失败: {e}")
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(str(e))
            return cosyvoice_pb2.ExtractSpeakerFeaturesResponse()

    def GenerateTTSTokens(self, request, context):
        """
        使用文本UUID和说话人UUID生成TTS tokens并更新缓存
        """
        try:
            text_uuid = request.text_uuid
            speaker_uuid = request.speaker_uuid
            text_features = self.cache.get_text(text_uuid)
            speaker_features = self.cache.get_speaker(speaker_uuid)

            if not text_features:
                context.set_code(grpc.StatusCode.NOT_FOUND)
                context.set_details(f"Text UUID {text_uuid} 不存在")
                return cosyvoice_pb2.GenerateTTSTokensResponse()
            if not speaker_features:
                context.set_code(grpc.StatusCode.NOT_FOUND)
                context.set_details(f"Speaker UUID {speaker_uuid} 不存在")
                return cosyvoice_pb2.GenerateTTSTokensResponse()

            total_duration_ms = 0
            tts_tokens = []
            segment_uuids = []

            for i, text_tokens in enumerate(text_features.text_tokens):
                seg_uuid = f"{text_uuid}_seg_{i}"
                segment_uuids.append(seg_uuid)

                with self.model.lock:
                    self.model.tts_speech_token_dict[seg_uuid] = []
                    self.model.llm_end_dict[seg_uuid] = False

                try:
                    self.model.llm_job(
                        text=text_tokens,
                        prompt_text=torch.zeros(1, 0, dtype=torch.int32),
                        llm_prompt_speech_token=speaker_features.get('llm_prompt_speech_token', torch.zeros(1, 0, dtype=torch.int32)),
                        llm_embedding=speaker_features.get('llm_embedding', torch.zeros(0, 192)),
                        uuid=seg_uuid
                    )

                    seg_tokens = self.model.tts_speech_token_dict[seg_uuid]
                    tts_tokens.append(seg_tokens)
                    total_duration_ms += len(seg_tokens) / 25.0 * 1000

                finally:
                    self.model.tts_speech_token_dict.pop(seg_uuid, None)
                    self.model.llm_end_dict.pop(seg_uuid, None)

            success = self.cache.update_tts_tokens(text_uuid, tts_tokens, segment_uuids)
            if not success:
                context.set_code(grpc.StatusCode.INTERNAL)
                context.set_details("更新TTS tokens缓存失败")
                return cosyvoice_pb2.GenerateTTSTokensResponse()

            return cosyvoice_pb2.GenerateTTSTokensResponse(
                duration_ms=int(total_duration_ms),
                success=True
            )

        except Exception as e:
            logger.error(f"GenerateTTSTokens失败: {e}")
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(str(e))
            return cosyvoice_pb2.GenerateTTSTokensResponse()

    def Token2Wav(self, request, context):
        """
        使用文本UUID和说话人UUID将TTS tokens转换为音频
        """
        try:
            text_uuid = request.text_uuid
            speaker_uuid = request.speaker_uuid
            text_features = self.cache.get_text(text_uuid)
            speaker_features = self.cache.get_speaker(speaker_uuid)

            if not text_features:
                context.set_code(grpc.StatusCode.NOT_FOUND)
                context.set_details(f"Text UUID {text_uuid} 不存在")
                return cosyvoice_pb2.Token2WavResponse()
            if not speaker_features:
                context.set_code(grpc.StatusCode.NOT_FOUND)
                context.set_details(f"Speaker UUID {speaker_uuid} 不存在")
                return cosyvoice_pb2.Token2WavResponse()

            speed = request.speed
            audio_pieces = []
            total_duration_sec = 0.0

            for seg_tokens, seg_uuid in zip(text_features.tts_tokens, text_features.tts_segment_uuids):
                if not seg_tokens:
                    continue

                self.model.hift_cache_dict[seg_uuid] = None
                try:
                    seg_audio_out = self.model.token2wav(
                        token=torch.tensor(seg_tokens).unsqueeze(0),
                        prompt_token=speaker_features.get('flow_prompt_speech_token', torch.zeros(1, 0, dtype=torch.int32)),
                        prompt_feat=speaker_features.get('prompt_speech_feat', torch.zeros(1, 0, 80)),
                        embedding=speaker_features.get('llm_embedding', torch.zeros(0)),
                        uuid=seg_uuid,
                        token_offset=0,
                        finalize=True,
                        speed=speed
                    )
                finally:
                    self.model.hift_cache_dict.pop(seg_uuid, None)

                # 处理音频：移除batch维度，如果是多通道则取平均
                seg_audio = seg_audio_out.cpu().numpy()
                if seg_audio.ndim > 1:
                    seg_audio = seg_audio.mean(axis=0)  # 如果还有多个通道，取平均得到单通道
                audio_pieces.append(seg_audio)
                total_duration_sec += len(seg_audio) / self.sample_rate

            if not audio_pieces:
                logger.warning(f"Token2Wav: Text UUID {text_uuid} 未生成任何音频")
                return cosyvoice_pb2.Token2WavResponse()

            final_audio = np.concatenate(audio_pieces)
            audio_int16 = (final_audio * (2**15)).astype(np.int16).tobytes()

            return cosyvoice_pb2.Token2WavResponse(
                audio=audio_int16,
                duration_sec=total_duration_sec
            )

        except Exception as e:
            logger.error(f"Token2Wav失败: {e}")
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(str(e))
            return cosyvoice_pb2.Token2WavResponse()

    def Cleanup(self, request, context):
        """
        清理指定UUID的文本或说话人特征
        """
        try:
            uuid = request.uuid
            is_speaker = request.is_speaker
            self.cache.delete(uuid, is_speaker)
            return cosyvoice_pb2.CleanupResponse(success=True)
        except Exception as e:
            logger.error(f"Cleanup失败: {e}")
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(str(e))
            return cosyvoice_pb2.CleanupResponse(success=False)

def serve(args):
    host = getattr(args, 'host', '0.0.0.0')
    port = getattr(args, 'port', 50052)
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
    cosyvoice_pb2_grpc.add_CosyVoiceServiceServicer_to_server(
        CosyVoiceServiceServicer(args.model_dir), server
    )
    address = f'{host}:{port}'
    server.add_insecure_port(address)
    server.start()
    logger.info(f'CosyVoice服务已启动: {address}')
    server.wait_for_termination()

================
File: services/hls/proto/__init__.py
================
from .hls_service_pb2 import *
from .hls_service_pb2_grpc import *

================
File: services/hls/proto/hls_service_pb2_grpc.py
================
# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!
"""Client and server classes corresponding to protobuf-defined services."""
from google.protobuf import descriptor as _descriptor
from google.protobuf import message as _message
from google.protobuf import reflection as _reflection
from google.protobuf import symbol_database as _symbol_database
import grpc

from . import hls_service_pb2 as hls__service__pb2


class HLSServiceStub(object):
    """HLS服务定义
    """

    def __init__(self, channel):
        """Constructor.

        Args:
            channel: A grpc.Channel.
        """
        self.InitTask = channel.unary_unary(
                '/hls.HLSService/InitTask',
                request_serializer=hls__service__pb2.InitTaskRequest.SerializeToString,
                response_deserializer=hls__service__pb2.InitTaskResponse.FromString,
                )
        self.AddSegment = channel.unary_unary(
                '/hls.HLSService/AddSegment',
                request_serializer=hls__service__pb2.AddSegmentRequest.SerializeToString,
                response_deserializer=hls__service__pb2.AddSegmentResponse.FromString,
                )
        self.FinalizeTask = channel.unary_unary(
                '/hls.HLSService/FinalizeTask',
                request_serializer=hls__service__pb2.FinalizeTaskRequest.SerializeToString,
                response_deserializer=hls__service__pb2.FinalizeTaskResponse.FromString,
                )
        self.CleanupTask = channel.unary_unary(
                '/hls.HLSService/CleanupTask',
                request_serializer=hls__service__pb2.CleanupTaskRequest.SerializeToString,
                response_deserializer=hls__service__pb2.CleanupTaskResponse.FromString,
                )


class HLSServiceServicer(object):
    """HLS服务定义
    """

    def InitTask(self, request, context):
        """初始化任务
        """
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details('Method not implemented!')
        raise NotImplementedError('Method not implemented!')

    def AddSegment(self, request, context):
        """添加视频片段
        """
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details('Method not implemented!')
        raise NotImplementedError('Method not implemented!')

    def FinalizeTask(self, request, context):
        """完成任务
        """
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details('Method not implemented!')
        raise NotImplementedError('Method not implemented!')

    def CleanupTask(self, request, context):
        """清理任务资源
        """
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details('Method not implemented!')
        raise NotImplementedError('Method not implemented!')


def add_HLSServiceServicer_to_server(servicer, server):
    rpc_method_handlers = {
            'InitTask': grpc.unary_unary_rpc_method_handler(
                    servicer.InitTask,
                    request_deserializer=hls__service__pb2.InitTaskRequest.FromString,
                    response_serializer=hls__service__pb2.InitTaskResponse.SerializeToString,
            ),
            'AddSegment': grpc.unary_unary_rpc_method_handler(
                    servicer.AddSegment,
                    request_deserializer=hls__service__pb2.AddSegmentRequest.FromString,
                    response_serializer=hls__service__pb2.AddSegmentResponse.SerializeToString,
            ),
            'FinalizeTask': grpc.unary_unary_rpc_method_handler(
                    servicer.FinalizeTask,
                    request_deserializer=hls__service__pb2.FinalizeTaskRequest.FromString,
                    response_serializer=hls__service__pb2.FinalizeTaskResponse.SerializeToString,
            ),
            'CleanupTask': grpc.unary_unary_rpc_method_handler(
                    servicer.CleanupTask,
                    request_deserializer=hls__service__pb2.CleanupTaskRequest.FromString,
                    response_serializer=hls__service__pb2.CleanupTaskResponse.SerializeToString,
            ),
    }
    generic_handler = grpc.method_handlers_generic_handler(
            'hls.HLSService', rpc_method_handlers)
    server.add_generic_rpc_handlers((generic_handler,))


 # This class is part of an EXPERIMENTAL API.
class HLSService(object):
    """HLS服务定义
    """

    @staticmethod
    def InitTask(request,
            target,
            options=(),
            channel_credentials=None,
            call_credentials=None,
            insecure=False,
            compression=None,
            wait_for_ready=None,
            timeout=None,
            metadata=None):
        return grpc.experimental.unary_unary(request, target, '/hls.HLSService/InitTask',
            hls__service__pb2.InitTaskRequest.SerializeToString,
            hls__service__pb2.InitTaskResponse.FromString,
            options, channel_credentials,
            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)

    @staticmethod
    def AddSegment(request,
            target,
            options=(),
            channel_credentials=None,
            call_credentials=None,
            insecure=False,
            compression=None,
            wait_for_ready=None,
            timeout=None,
            metadata=None):
        return grpc.experimental.unary_unary(request, target, '/hls.HLSService/AddSegment',
            hls__service__pb2.AddSegmentRequest.SerializeToString,
            hls__service__pb2.AddSegmentResponse.FromString,
            options, channel_credentials,
            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)

    @staticmethod
    def FinalizeTask(request,
            target,
            options=(),
            channel_credentials=None,
            call_credentials=None,
            insecure=False,
            compression=None,
            wait_for_ready=None,
            timeout=None,
            metadata=None):
        return grpc.experimental.unary_unary(request, target, '/hls.HLSService/FinalizeTask',
            hls__service__pb2.FinalizeTaskRequest.SerializeToString,
            hls__service__pb2.FinalizeTaskResponse.FromString,
            options, channel_credentials,
            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)

    @staticmethod
    def CleanupTask(request,
            target,
            options=(),
            channel_credentials=None,
            call_credentials=None,
            insecure=False,
            compression=None,
            wait_for_ready=None,
            timeout=None,
            metadata=None):
        return grpc.experimental.unary_unary(request, target, '/hls.HLSService/CleanupTask',
            hls__service__pb2.CleanupTaskRequest.SerializeToString,
            hls__service__pb2.CleanupTaskResponse.FromString,
            options, channel_credentials,
            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)

================
File: services/hls/proto/hls_service_pb2.py
================
# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: hls_service.proto
"""Generated protocol buffer code."""
from google.protobuf import descriptor as _descriptor
from google.protobuf import descriptor_pool as _descriptor_pool
from google.protobuf import symbol_database as _symbol_database
from google.protobuf.internal import builder as _builder
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()




DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x11hls_service.proto\x12\x03hls\"\"\n\x0fInitTaskRequest\x12\x0f\n\x07task_id\x18\x01 \x01(\t\"4\n\x10InitTaskResponse\x12\x0f\n\x07success\x18\x01 \x01(\x08\x12\x0f\n\x07message\x18\x02 \x01(\t\"Q\n\x11\x41\x64\x64SegmentRequest\x12\x0f\n\x07task_id\x18\x01 \x01(\t\x12\x14\n\x0csegment_path\x18\x02 \x01(\t\x12\x15\n\rsegment_index\x18\x03 \x01(\x05\"K\n\x12\x41\x64\x64SegmentResponse\x12\x0f\n\x07success\x18\x01 \x01(\x08\x12\x0f\n\x07message\x18\x02 \x01(\t\x12\x13\n\x0bsegment_url\x18\x03 \x01(\t\"&\n\x13\x46inalizeTaskRequest\x12\x0f\n\x07task_id\x18\x01 \x01(\t\"N\n\x14\x46inalizeTaskResponse\x12\x0f\n\x07success\x18\x01 \x01(\x08\x12\x0f\n\x07message\x18\x02 \x01(\t\x12\x14\n\x0cplaylist_url\x18\x03 \x01(\t\"%\n\x12\x43leanupTaskRequest\x12\x0f\n\x07task_id\x18\x01 \x01(\t\"7\n\x13\x43leanupTaskResponse\x12\x0f\n\x07success\x18\x01 \x01(\x08\x12\x0f\n\x07message\x18\x02 \x01(\t2\x8b\x02\n\nHLSService\x12\x37\n\x08InitTask\x12\x14.hls.InitTaskRequest\x1a\x15.hls.InitTaskResponse\x12=\n\nAddSegment\x12\x16.hls.AddSegmentRequest\x1a\x17.hls.AddSegmentResponse\x12\x43\n\x0c\x46inalizeTask\x12\x18.hls.FinalizeTaskRequest\x1a\x19.hls.FinalizeTaskResponse\x12@\n\x0b\x43leanupTask\x12\x17.hls.CleanupTaskRequest\x1a\x18.hls.CleanupTaskResponseb\x06proto3')

_globals = globals()
_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)
_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'hls_service_pb2', _globals)
if _descriptor._USE_C_DESCRIPTORS == False:

  DESCRIPTOR._options = None
  _globals['_INITTASKREQUEST']._serialized_start=26
  _globals['_INITTASKREQUEST']._serialized_end=60
  _globals['_INITTASKRESPONSE']._serialized_start=62
  _globals['_INITTASKRESPONSE']._serialized_end=114
  _globals['_ADDSEGMENTREQUEST']._serialized_start=116
  _globals['_ADDSEGMENTREQUEST']._serialized_end=197
  _globals['_ADDSEGMENTRESPONSE']._serialized_start=199
  _globals['_ADDSEGMENTRESPONSE']._serialized_end=274
  _globals['_FINALIZETASKREQUEST']._serialized_start=276
  _globals['_FINALIZETASKREQUEST']._serialized_end=314
  _globals['_FINALIZETASKRESPONSE']._serialized_start=316
  _globals['_FINALIZETASKRESPONSE']._serialized_end=394
  _globals['_CLEANUPTASKREQUEST']._serialized_start=396
  _globals['_CLEANUPTASKREQUEST']._serialized_end=433
  _globals['_CLEANUPTASKRESPONSE']._serialized_start=435
  _globals['_CLEANUPTASKRESPONSE']._serialized_end=490
  _globals['_HLSSERVICE']._serialized_start=493
  _globals['_HLSSERVICE']._serialized_end=760
# @@protoc_insertion_point(module_scope)

================
File: services/hls/proto/hls_service.proto
================
syntax = "proto3";

package hls;

// HLS服务定义
service HLSService {
  // 初始化任务
  rpc InitTask(InitTaskRequest) returns (InitTaskResponse);
  
  // 添加视频片段
  rpc AddSegment(AddSegmentRequest) returns (AddSegmentResponse);
  
  // 完成任务
  rpc FinalizeTask(FinalizeTaskRequest) returns (FinalizeTaskResponse);
  
  // 清理任务资源
  rpc CleanupTask(CleanupTaskRequest) returns (CleanupTaskResponse);
}

// 初始化任务请求
message InitTaskRequest {
  string task_id = 1;
}

message InitTaskResponse {
  bool success = 1;
  string message = 2;
}

// 添加片段请求
message AddSegmentRequest {
  string task_id = 1;
  string segment_path = 2;  // 暂时保留本地路径
  int32 segment_index = 3;
}

message AddSegmentResponse {
  bool success = 1;
  string message = 2;
  string segment_url = 3;  // 返回可访问的URL
}

// 完成任务请求
message FinalizeTaskRequest {
  string task_id = 1;
}

message FinalizeTaskResponse {
  bool success = 1;
  string message = 2;
  string playlist_url = 3;  // 返回最终的播放列表URL
}

// 清理任务请求
message CleanupTaskRequest {
  string task_id = 1;
}

message CleanupTaskResponse {
  bool success = 1;
  string message = 2;
}

================
File: services/hls/__init__.py
================
from .service import HLSService
from .client import HLSClient
from .server import HLSServicer, serve

__all__ = ['HLSService', 'HLSClient', 'HLSServicer', 'serve']

================
File: services/hls/client.py
================
import logging
import grpc
from pathlib import Path
import asyncio
from .proto import hls_service_pb2
from .proto import hls_service_pb2_grpc
from config import Config

logger = logging.getLogger(__name__)

class HLSClient:
    """HLS gRPC客户端"""
    
    def __init__(self, config):
        self.config = config
        self.channel = None
        self.stub = None
        self._max_retries = 3
        self._retry_delay = 1.0  # 重试延迟（秒）
        
    @classmethod
    async def create(cls, config):
        """异步工厂方法创建客户端实例"""
        self = cls(config)
        await self._init_client()
        return self
        
    async def _init_client(self):
        """异步初始化客户端"""
        try:
            host = self.config.HLS_SERVICE_HOST
            port = self.config.HLS_SERVICE_PORT
            self.channel = grpc.aio.insecure_channel(f"{host}:{port}")
            self.stub = hls_service_pb2_grpc.HLSServiceStub(self.channel)
        except Exception as e:
            logger.error(f"初始化HLS客户端失败: {str(e)}")
            raise

    async def _ensure_connection(self):
        """确保连接可用，如果断开则尝试重连"""
        if not self.channel or self.channel.get_state() in [grpc.ChannelConnectivity.SHUTDOWN, grpc.ChannelConnectivity.TRANSIENT_FAILURE]:
            await self._init_client()
            
    async def _retry_rpc(self, rpc_func, *args, **kwargs):
        """带重试机制的 RPC 调用"""
        for attempt in range(self._max_retries):
            try:
                await self._ensure_connection()
                return await rpc_func(*args, **kwargs)
            except grpc.aio.AioRpcError as e:
                if e.code() == grpc.StatusCode.UNAVAILABLE:
                    if attempt < self._max_retries - 1:
                        logger.warning(f"RPC调用失败，{self._retry_delay}秒后重试 (尝试 {attempt + 1}/{self._max_retries})")
                        await asyncio.sleep(self._retry_delay)
                        continue
                raise
        return False
        
    async def init_task(self, task_id: str) -> bool:
        """初始化HLS任务"""
        try:
            async def _init():
                request = hls_service_pb2.InitTaskRequest(task_id=task_id)
                response = await self.stub.InitTask(request)
                if not response.success:
                    logger.error(f"初始化任务失败: {response.message}")
                return response.success
            return await self._retry_rpc(_init)
        except Exception as e:
            logger.error(f"调用InitTask失败: {str(e)}")
            return False
            
    async def add_segment(self, task_id: str, segment_path: Path, segment_index: int) -> bool:
        """添加视频片段"""
        try:
            async def _add():
                request = hls_service_pb2.AddSegmentRequest(
                    task_id=task_id,
                    segment_path=str(segment_path),
                    segment_index=segment_index
                )
                response = await self.stub.AddSegment(request)
                if not response.success:
                    logger.error(f"添加片段失败: {response.message}")
                return response.success
            return await self._retry_rpc(_add)
        except Exception as e:
            logger.error(f"调用AddSegment失败: {str(e)}")
            return False
            
    async def finalize_task(self, task_id: str) -> bool:
        """完成HLS任务"""
        try:
            async def _finalize():
                request = hls_service_pb2.FinalizeTaskRequest(task_id=task_id)
                response = await self.stub.FinalizeTask(request)
                if not response.success:
                    logger.error(f"完成任务失败: {response.message}")
                return response.success
            return await self._retry_rpc(_finalize)
        except Exception as e:
            logger.error(f"调用FinalizeTask失败: {str(e)}")
            return False
            
    async def cleanup_task(self, task_id: str) -> bool:
        """清理任务资源"""
        try:
            async def _cleanup():
                request = hls_service_pb2.CleanupTaskRequest(task_id=task_id)
                response = await self.stub.CleanupTask(request)
                if not response.success:
                    logger.error(f"清理任务失败: {response.message}")
                return response.success
            return await self._retry_rpc(_cleanup)
        except Exception as e:
            logger.error(f"调用CleanupTask失败: {str(e)}")
            return False
            
    async def close(self):
        """关闭gRPC通道"""
        if self.channel:
            await self.channel.close()

================
File: services/hls/server.py
================
import logging
from concurrent import futures
import grpc
from pathlib import Path

from .proto import hls_service_pb2
from .proto import hls_service_pb2_grpc
from .service import HLSService
from .storage import LocalStorageService
from config import Config

logger = logging.getLogger(__name__)

class HLSServicer(hls_service_pb2_grpc.HLSServiceServicer):
    """HLS gRPC服务实现"""
    
    def __init__(self, config: Config):
        self.config = config
        self.storage_service = LocalStorageService(config)
        self.hls_service = HLSService(config, self.storage_service)
        self._active_tasks = set()  # 跟踪活动的任务
        
    async def InitTask(self, request, context):
        """初始化HLS任务"""
        try:
            task_id = request.task_id
            if task_id in self._active_tasks:
                return hls_service_pb2.InitTaskResponse(
                    success=False,
                    message=f"任务 {task_id} 已存在"
                )
            
            await self.hls_service.init_task(task_id)
            self._active_tasks.add(task_id)
            return hls_service_pb2.InitTaskResponse(
                success=True,
                message=f"任务 {task_id} 初始化成功"
            )
        except Exception as e:
            logger.error(f"初始化任务失败: {str(e)}")
            return hls_service_pb2.InitTaskResponse(
                success=False,
                message=str(e)
            )
    
    async def AddSegment(self, request, context):
        """添加视频片段"""
        try:
            task_id = request.task_id
            if task_id not in self._active_tasks:
                return hls_service_pb2.AddSegmentResponse(
                    success=False,
                    message=f"任务 {task_id} 不存在或未初始化"
                )
                
            segment_path = Path(request.segment_path)
            if not segment_path.exists():
                return hls_service_pb2.AddSegmentResponse(
                    success=False,
                    message=f"片段文件不存在: {segment_path}"
                )
                
            success = await self.hls_service.add_segment(
                task_id,
                segment_path,
                request.segment_index
            )
            
            if success:
                return hls_service_pb2.AddSegmentResponse(
                    success=True,
                    message="片段添加成功",
                    segment_url=f"/segments/{task_id}/segment_{request.segment_index}.ts"
                )
            else:
                return hls_service_pb2.AddSegmentResponse(
                    success=False,
                    message="片段添加失败"
                )
                
        except Exception as e:
            logger.error(f"添加片段失败: {str(e)}")
            return hls_service_pb2.AddSegmentResponse(
                success=False,
                message=str(e)
            )
    
    async def FinalizeTask(self, request, context):
        """完成HLS任务"""
        try:
            task_id = request.task_id
            if task_id not in self._active_tasks:
                return hls_service_pb2.FinalizeTaskResponse(
                    success=False,
                    message=f"任务 {task_id} 不存在或未初始化"
                )
            
            await self.hls_service.finalize_task(task_id)
            return hls_service_pb2.FinalizeTaskResponse(
                success=True,
                message=f"任务 {task_id} 完成",
                playlist_url=f"/playlists/{task_id}.m3u8"
            )
        except Exception as e:
            logger.error(f"完成任务失败: {str(e)}")
            return hls_service_pb2.FinalizeTaskResponse(
                success=False,
                message=str(e)
            )
    
    async def CleanupTask(self, request, context):
        """清理任务资源"""
        try:
            task_id = request.task_id
            if task_id not in self._active_tasks:
                return hls_service_pb2.CleanupTaskResponse(
                    success=False,
                    message=f"任务 {task_id} 不存在或未初始化"
                )
            
            await self.hls_service.cleanup_task(task_id)
            self._active_tasks.remove(task_id)
            return hls_service_pb2.CleanupTaskResponse(
                success=True,
                message=f"任务 {task_id} 资源已清理"
            )
        except Exception as e:
            logger.error(f"清理任务失败: {str(e)}")
            return hls_service_pb2.CleanupTaskResponse(
                success=False,
                message=str(e)
            )

def serve(config: Config):
    """启动 gRPC 服务器"""
    server = grpc.aio.server(futures.ThreadPoolExecutor(max_workers=10))
    hls_service_pb2_grpc.add_HLSServiceServicer_to_server(
        HLSServicer(config), server
    )
    server.add_insecure_port(f"{config.HLS_GRPC_HOST}:{config.HLS_GRPC_PORT}")
    return server

================
File: services/hls/service.py
================
import m3u8
import logging
from pathlib import Path
from typing import Optional, Dict
from .storage import StorageService
from utils.ffmpeg_utils import FFmpegTool

logger = logging.getLogger(__name__)

class HLSService:
    """HLS 流媒体服务"""
    
    def __init__(self, config, storage_service: StorageService):
        self.config = config
        self.storage = storage_service
        self.playlists: Dict[str, m3u8.M3U8] = {}
        self.logger = logger
        self.ffmpeg_tool = FFmpegTool()
        
    def _create_playlist(self, task_id: str) -> m3u8.M3U8:
        """创建新的播放列表"""
        playlist = m3u8.M3U8()
        playlist.version = 3
        playlist.target_duration = self.config.HLS_SEGMENT_DURATION
        playlist.media_sequence = 0
        playlist.playlist_type = 'VOD'
        playlist.is_endlist = False
        
        self.playlists[task_id] = playlist
        return playlist
        
    async def init_task(self, task_id: str) -> None:
        """初始化任务的HLS资源"""
        playlist = self._create_playlist(task_id)
        await self.storage.update_playlist(task_id, playlist.dumps())
        
    async def add_segment(self, task_id: str, segment_path: Path, segment_index: int) -> bool:
        """添加新的视频片段"""
        try:
            # 1. 使用 FFmpeg 进行 HLS 分段
            segment_filename = f'segment_{segment_index}_%03d.ts'
            target_dir = self.config.PUBLIC_DIR / "segments" / task_id
            target_dir.mkdir(parents=True, exist_ok=True)
            
            segment_pattern = str(target_dir / segment_filename)
            temp_playlist_path = target_dir / f'temp_{segment_index}.m3u8'

            # 调用 FFmpeg 进行分段，添加更多参数以确保准确切片
            await self.ffmpeg_tool.hls_segment(
                input_path=str(segment_path),               
                hls_time=self.config.HLS_TIME,
                segment_pattern=segment_pattern,
                playlist_path=str(temp_playlist_path),
            )

            # 2. 读取临时播放列表并更新主播放列表
            temp_m3u8 = m3u8.load(str(temp_playlist_path))
            
            playlist = self.playlists.get(task_id)
            if not playlist:
                playlist = self._create_playlist(task_id)

            # 添加不连续标记
            discontinuity_segment = m3u8.Segment(discontinuity=True)
            playlist.segments.append(discontinuity_segment)

            # 添加分片
            for segment in temp_m3u8.segments:
                segment.uri = f"/segments/{task_id}/{Path(segment.uri).name}"
                playlist.segments.append(segment)

            # 3. 保存更新后的播放列表
            await self.storage.update_playlist(
                task_id,
                playlist.dumps()
            )
            
            # 4. 清理临时播放列表
            if temp_playlist_path.exists():
                temp_playlist_path.unlink()

            self.logger.debug(f"已添加分片 {segment_index} 到任务 {task_id}")
            return True
            
        except Exception as e:
            self.logger.error(f"添加分片失败: {e}")
            return False
            
    async def finalize_task(self, task_id: str) -> None:
        """完成任务的HLS处理"""
        try:
            playlist = self.playlists.get(task_id)
            if playlist:
                playlist.is_endlist = True
                await self.storage.update_playlist(
                    task_id,
                    playlist.dumps()
                )
                # 清理缓存
                self.playlists.pop(task_id, None)
                self.logger.info(f"任务 {task_id} 的HLS流已完成")
        except Exception as e:
            self.logger.error(f"完成HLS流失败: {e}")
            
    async def cleanup_task(self, task_id: str) -> None:
        """清理任务的所有HLS资源"""
        try:
            await self.storage.cleanup_task(task_id)
            self.playlists.pop(task_id, None)
            self.logger.info(f"已清理任务 {task_id} 的HLS资源")
        except Exception as e:
            self.logger.error(f"清理HLS资源失败: {e}")

================
File: services/hls/storage.py
================
from abc import ABC, abstractmethod
from pathlib import Path
import logging
import shutil
import aiofiles
from typing import Optional

logger = logging.getLogger(__name__)

class StorageService(ABC):
    """存储服务抽象基类"""
    
    @abstractmethod
    async def upload_segment(self, task_id: str, segment_path: Path, segment_index: int) -> str:
        """
        上传分片并返回访问路径
        Args:
            task_id: 任务ID
            segment_path: 分片文件路径
            segment_index: 分片索引
        Returns:
            str: 分片访问路径
        """
        pass
    
    @abstractmethod
    async def update_playlist(self, task_id: str, playlist_content: str) -> str:
        """
        更新播放列表并返回访问路径
        Args:
            task_id: 任务ID
            playlist_content: m3u8内容
        Returns:
            str: 播放列表访问路径
        """
        pass
        
    @abstractmethod
    async def cleanup_task(self, task_id: str) -> None:
        """清理任务相关的存储资源"""
        pass

class LocalStorageService(StorageService):
    """本地文件系统存储实现"""
    
    def __init__(self, config):
        self.config = config
        self.public_dir = config.PUBLIC_DIR
        
    async def upload_segment(self, task_id: str, segment_path: Path, segment_index: int) -> str:
        """本地存储实现 - 复制文件到公共目录"""
        target_dir = self.public_dir / "segments" / task_id
        target_dir.mkdir(parents=True, exist_ok=True)
        
        target_path = target_dir / f"segment_{segment_index}.ts"
        shutil.copy2(str(segment_path), str(target_path))
        
        return f"/segments/{task_id}/segment_{segment_index}.ts"
        
    async def update_playlist(self, task_id: str, playlist_content: str) -> str:
        """本地存储实现 - 写入m3u8文件"""
        playlist_path = self.public_dir / "playlists" / f"{task_id}.m3u8"
        async with aiofiles.open(playlist_path, 'w') as f:
            await f.write(playlist_content)
        return f"/playlists/{task_id}.m3u8"
        
    async def cleanup_task(self, task_id: str) -> None:
        """清理任务文件"""
        # 清理分片
        segment_dir = self.public_dir / "segments" / task_id
        if segment_dir.exists():
            shutil.rmtree(str(segment_dir))
            
        # 清理播放列表    
        playlist_path = self.public_dir / "playlists" / f"{task_id}.m3u8"
        if playlist_path.exists():
            playlist_path.unlink()

================
File: utils/concurrency.py
================
# utils/concurrency.py
import functools
import asyncio
import os
from concurrent.futures import ThreadPoolExecutor

CPU_COUNT = os.cpu_count() or 1
GLOBAL_EXECUTOR = ThreadPoolExecutor(max_workers=CPU_COUNT)

async def run_sync(func, *args, **kwargs):
    loop = asyncio.get_running_loop()
    partial_func = functools.partial(func, *args, **kwargs)
    return await loop.run_in_executor(GLOBAL_EXECUTOR, partial_func)

================
File: utils/ffmpeg_utils.py
================
# --------------------------------------
# utils/ffmpeg_utils.py
# 彻底移除 force_style, 仅使用 .ass 内部样式
# --------------------------------------
import asyncio
import logging
from pathlib import Path
from typing import List, Tuple, Optional

logger = logging.getLogger(__name__)

class FFmpegTool:
    """
    统一封装 FFmpeg 常见用法的工具类。
    通过异步方式执行 ffmpeg 命令，并在出错时抛出异常。
    """

    async def run_command(self, cmd: List[str]) -> Tuple[bytes, bytes]:
        """
        运行 ffmpeg 命令，返回 (stdout, stderr)。
        若命令返回码非 0，则抛出 RuntimeError。
        """
        logger.debug(f"[FFmpegTool] Running command: {' '.join(cmd)}")
        process = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        stdout, stderr = await process.communicate()

        if process.returncode != 0:
            error_msg = stderr.decode() or "Unknown error"
            logger.error(f"[FFmpegTool] Command failed with error: {error_msg}")
            raise RuntimeError(f"FFmpeg command failed: {error_msg}")

        return stdout, stderr

    async def extract_audio(
        self,
        input_path: str,
        output_path: str,
        start: float = 0.0,
        duration: Optional[float] = None
    ) -> None:
        """
        提取音频，可选指定起始时间与持续时长。
        输出为单声道 PCM float32 (48k/16k 视需求).
        如果视频没有音频流，则生成一个空的音频文件。
        """
        # 首先检查视频是否有音频流
        has_audio = await self._check_audio_stream(input_path)
        
        if has_audio:
            cmd = ["ffmpeg", "-y", "-i", input_path]
            if start > 0:
                cmd += ["-ss", str(start)]
            if duration is not None:
                cmd += ["-t", str(duration)]

            cmd += [
                "-vn",                # 去掉视频
                "-acodec", "pcm_f32le",
                "-ac", "1",
                output_path
            ]
            await self.run_command(cmd)
        else:
            # 如果没有音频流，生成一个空的音频文件
            logger.warning(f"[FFmpegTool] 视频 {input_path} 没有音频流，生成空音频文件")
            # 获取视频时长
            video_duration = await self._get_video_duration(input_path)
            if duration is not None:
                video_duration = min(video_duration, duration)
            
            # 生成静音音频
            cmd = [
                "ffmpeg", "-y",
                "-f", "lavfi",
                "-i", f"anullsrc=r=48000:cl=mono",
                "-t", str(video_duration),
                "-acodec", "pcm_f32le",
                "-ac", "1",
                output_path
            ]
            await self.run_command(cmd)

    async def extract_video(
        self,
        input_path: str,
        output_path: str,
        start: float = 0.0,
        duration: Optional[float] = None
    ) -> None:
        """
        提取纯视频（去掉音轨），可选指定起始时间与持续时长。
        """
        cmd = ["ffmpeg", "-y", "-i", input_path]
        if start > 0:
            cmd += ["-ss", str(start)]
        if duration is not None:
            cmd += ["-t", str(duration)]

        cmd += [
            "-an",                # 去掉音频
            "-c:v", "libx264",
            "-preset", "ultrafast",
            "-crf", "18",
            "-tune", "fastdecode",
            output_path
        ]
        await self.run_command(cmd)

    async def hls_segment(
        self,
        input_path: str,
        segment_pattern: str,
        playlist_path: str,
        hls_time: int = 10,
        hls_flags: str = "independent_segments",
    ) -> None:
        """
        将输入视频切分成HLS分片
        Args:
            input_path: 输入视频路径
            segment_pattern: 分片文件名模式
            playlist_path: 临时m3u8文件路径
            hls_time: 每个分片的目标时长(秒)
            hls_flags: HLS特殊标志
            extra_options: 额外的 FFmpeg 选项
        """
        cmd = [
            "ffmpeg", "-y",
            "-i", input_path,
            "-c", "copy",
            "-f", "hls",
            "-hls_time", str(hls_time),
            "-hls_list_size", "0",
            "-hls_segment_type", "mpegts",
            "-hls_segment_filename", segment_pattern,
            playlist_path
        ]
        
        await self.run_command(cmd)

    async def cut_video_track(
        self,
        input_path: str,
        output_path: str,
        start: float,
        end: float
    ) -> None:
        """
        截取 [start, end] 的无声视频段，end为绝对秒数。
        """
        duration = end - start
        if duration <= 0:
            raise ValueError(f"Invalid duration: {duration}")

        cmd = [
            "ffmpeg", "-y",
            "-i", input_path,
            "-ss", str(start),
            "-t", str(duration),
            "-c:v", "libx264",
            "-preset", "superfast",
            "-an",  # 去除音轨
            "-vsync", "vfr",
            output_path
        ]
        await self.run_command(cmd)

    async def cut_video_with_audio(
        self,
        input_video_path: str,
        input_audio_path: str,
        output_path: str
    ) -> None:
        """
        将无声视频与音频合并 (视频copy，音频AAC)。
        """
        cmd = [
            "ffmpeg", "-y",
            "-i", input_video_path,
            "-i", input_audio_path,
            "-c:v", "copy",
            "-c:a", "aac",
            output_path
        ]
        await self.run_command(cmd)

    async def cut_video_with_subtitles_and_audio(
        self,
        input_video_path: str,
        input_audio_path: str,
        subtitles_path: str,
        output_path: str
    ) -> None:
        """
        将无声视频 + 音频 + .ass字幕 合并输出到 output_path。
        (无 force_style, 由 .ass 内样式全权决定)
        
        若字幕渲染失败，则回退到仅合并音视频。
        """
        # 检查输入文件是否存在
        for file_path in [input_video_path, input_audio_path, subtitles_path]:
            if not Path(file_path).exists():
                raise FileNotFoundError(f"文件不存在: {file_path}")

        # 构建"subtitles"过滤器, 不带 force_style
        escaped_path = subtitles_path.replace(':', r'\\:')

        try:
            # 方式1: subtitles 滤镜
            # 当前设置是合理的，但需要注意：
            cmd = [
                "ffmpeg", "-y",
                "-i", input_video_path,
                "-i", input_audio_path,
                "-filter_complex",
                f"[0:v]scale=1920:-2:flags=lanczos,subtitles='{escaped_path}'[v]",  # 修改点说明：
                # 1. scale=1920:-2 保持宽高比，-2 保证高度为偶数（兼容编码要求）
                # 2. flags=lanczos 使用高质量的缩放算法
                # 3. 滤镜顺序：先缩放视频，再加字幕（确保字幕在缩放后的画面上）
                "-map", "[v]",
                "-map", "1:a",
                "-c:v", "libx264",
                "-preset", "superfast",
                "-crf", "23",  # 建议添加 CRF 参数控制视频质量
                "-c:a", "aac",
                output_path
            ]
            await self.run_command(cmd)

        except RuntimeError as e:
            logger.warning(f"[FFmpegTool] subtitles滤镜方案失败: {str(e)}")
            # 方式2: 最终回退 - 仅合并音视频
            cmd = [
                "ffmpeg", "-y",
                "-i", input_video_path,
                "-i", input_audio_path,
                "-c:v", "copy",
                "-c:a", "aac",
                output_path
            ]
            await self.run_command(cmd)
            logger.warning("[FFmpegTool] 已跳过字幕，仅合并音视频")

    async def _check_audio_stream(self, input_path: str) -> bool:
        """检查视频是否有音频流"""
        cmd = ["ffprobe", "-v", "error", "-select_streams", "a", "-show_entries", 
               "stream=codec_type", "-of", "json", input_path]
        try:
            stdout, _ = await self.run_command(cmd)
            import json
            result = json.loads(stdout)
            return 'streams' in result and len(result['streams']) > 0
        except Exception as e:
            logger.error(f"[FFmpegTool] 检查音频流失败: {e}")
            return False
            
    async def _get_video_duration(self, input_path: str) -> float:
        """获取视频时长"""
        cmd = ["ffprobe", "-v", "error", "-show_entries", "format=duration", 
               "-of", "default=noprint_wrappers=1:nokey=1", input_path]
        stdout, _ = await self.run_command(cmd)
        return float(stdout.decode().strip())

    async def get_duration(self, input_path: str) -> float:
        """
        调用 ffprobe 获取输入文件的时长(秒)。
        """
        cmd = [
            "ffprobe",
            "-v", "error",
            "-show_entries", "format=duration",
            "-of", "default=noprint_wrappers=1:nokey=1",
            input_path
        ]
        stdout, stderr = await self.run_command(cmd)
        return float(stdout.decode().strip())

================
File: utils/log_config.py
================
#!/usr/bin/env python3
"""
日志配置模块 - 提供统一的日志配置功能
"""
import sys
import logging
from pathlib import Path
from typing import Optional, Union, List

# 默认日志目录
DEFAULT_LOG_DIR = Path("/tmp/cosysense_logs")

# 关键模块日志路径
CORE_MODULES = [
    'workers',
    'workers.segment_worker',
    'workers.asr_worker', 
    'workers.mixer_worker',
    'workers.translation_worker',
    'workers.modelin_worker',
    'workers.tts_worker',
    'workers.duration_worker',
    'workers.audio_gen_worker',
    'utils',
    'utils.worker_decorators',
    'utils.redis_utils',
    'utils.task_state'
]

def configure_logging(
    log_dir: Optional[Union[str, Path]] = None,
    console_level: int = logging.INFO,
    file_level: int = logging.DEBUG,
    modules: Optional[List[str]] = None
) -> Path:
    """
    配置日志系统
    
    Args:
        log_dir: 日志目录路径，默认为 /tmp/cosysense_logs
        console_level: 控制台日志级别，默认为 INFO
        file_level: 文件日志级别，默认为 DEBUG
        modules: 需要特别配置的模块，默认为预定义的核心模块列表
        
    Returns:
        日志目录路径
    """
    # 设置日志目录
    if log_dir is None:
        log_dir = DEFAULT_LOG_DIR
    else:
        log_dir = Path(log_dir)
    
    # 确保日志目录存在
    log_dir.mkdir(exist_ok=True, parents=True)
    
    # 配置根日志器
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.DEBUG)  # 总是设置为最低级别，让处理器决定显示级别
    
    # 清除已有处理器
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)
    
    # 添加控制台处理器
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(console_level)
    console_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    console_handler.setFormatter(console_formatter)
    root_logger.addHandler(console_handler)
    
    # 添加文件处理器 - 所有日志
    main_log_file = log_dir / "worker.log"
    file_handler = logging.FileHandler(main_log_file)
    file_handler.setLevel(file_level)
    file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(process)d - %(message)s')
    file_handler.setFormatter(file_formatter)
    root_logger.addHandler(file_handler)
    
    # 为特定模块配置日志器
    if modules is None:
        modules = CORE_MODULES
    
    for module_name in modules:
        logger = logging.getLogger(module_name)
        logger.setLevel(logging.DEBUG)  # 每个模块的日志器设置为最低级别
        
        # 为worker模块添加单独的日志文件
        if module_name.startswith('workers.') and '.' in module_name:
            worker_name = module_name.split('.')[-1]
            worker_log_file = log_dir / f"{worker_name}.log"
            
            # 检查是否已有该处理器
            has_handler = any(
                isinstance(h, logging.FileHandler) and 
                getattr(h, 'baseFilename', '') == str(worker_log_file)
                for h in logger.handlers
            )
            
            if not has_handler:
                worker_handler = logging.FileHandler(worker_log_file)
                worker_handler.setLevel(file_level)
                worker_handler.setFormatter(file_formatter)
                logger.addHandler(worker_handler)
    
    # 记录配置完成信息
    logging.info(f"日志系统配置完成。主日志文件: {main_log_file}")
    return log_dir

def get_logger(name: str, log_dir: Optional[Union[str, Path]] = None) -> logging.Logger:
    """
    获取已配置的日志器，如果日志系统未配置则先配置
    
    Args:
        name: 日志器名称
        log_dir: 日志目录，如果日志系统未配置时使用
        
    Returns:
        配置好的日志器
    """
    logger = logging.getLogger(name)
    
    # 检查是否需要配置日志系统
    root_logger = logging.getLogger()
    if not root_logger.handlers:
        configure_logging(log_dir=log_dir)
    
    return logger

================
File: utils/redis_utils.py
================
import json
import logging
import aioredis
from pathlib import Path
import pickle
from utils.task_state import TaskState

logger = logging.getLogger(__name__)

# Redis 连接配置
REDIS_URL = 'redis://localhost'
REDIS_TASK_STATE_PREFIX = 'task_state:'
REDIS_TASK_STATE_EXPIRY = 86400 * 7  # 7天过期时间

async def get_redis_connection():
    """获取 Redis 连接"""
    return await aioredis.create_redis_pool(REDIS_URL)

async def save_task_state(task_state: TaskState):
    """保存任务状态到 Redis 和文件"""
    # 保存到文件
    task_dir = task_state.task_paths.task_dir
    task_dir.mkdir(parents=True, exist_ok=True)
    state_file = task_dir / f"{task_state.task_id}_state.pkl"
    
    with open(state_file, 'wb') as f:
        pickle.dump(task_state, f)
    
    # 保存到 Redis
    redis = await get_redis_connection()
    try:
        key = f"{REDIS_TASK_STATE_PREFIX}{task_state.task_id}"
        task_dict = task_state.to_dict()
        await redis.set(key, json.dumps(task_dict), expire=REDIS_TASK_STATE_EXPIRY)
        logger.debug(f"任务状态已保存到 Redis: {task_state.task_id}")
    except Exception as e:
        logger.error(f"保存任务状态到 Redis 失败: {e}")
    finally:
        redis.close()
        await redis.wait_closed()
    
    return state_file

async def load_task_state(task_id: str, task_dir: Path = None) -> TaskState:
    """
    从 Redis 或文件加载任务状态
    优先从 Redis 加载，如果失败则从文件加载
    """
    # 尝试从 Redis 加载
    redis = await get_redis_connection()
    try:
        key = f"{REDIS_TASK_STATE_PREFIX}{task_id}"
        data = await redis.get(key, encoding='utf-8')
        if data:
            task_dict = json.loads(data)
            task_state = TaskState.from_dict(task_dict)
            logger.debug(f"从 Redis 加载任务状态: {task_id}")
            return task_state
    except Exception as e:
        logger.error(f"从 Redis 加载任务状态失败: {e}")
    finally:
        redis.close()
        await redis.wait_closed()
    
    # 如果 Redis 加载失败且提供了 task_dir，则从文件加载
    if task_dir:
        try:
            state_file = task_dir / f"{task_id}_state.pkl"
            with open(state_file, 'rb') as f:
                task_state = pickle.load(f)
            logger.debug(f"从文件加载任务状态: {task_id}")
            return task_state
        except Exception as e:
            logger.error(f"从文件加载任务状态失败: {e}")
    
    raise ValueError(f"无法加载任务状态: {task_id}")

async def update_task_state(task_state: TaskState):
    """更新 Redis 中的任务状态"""
    await save_task_state(task_state)

async def delete_task_state(task_id: str):
    """删除 Redis 中的任务状态"""
    redis = await get_redis_connection()
    try:
        key = f"{REDIS_TASK_STATE_PREFIX}{task_id}"
        await redis.delete(key)
        logger.debug(f"已删除 Redis 中的任务状态: {task_id}")
    except Exception as e:
        logger.error(f"删除 Redis 中的任务状态失败: {e}")
    finally:
        redis.close()
        await redis.wait_closed()

async def push_to_queue(queue_name: str, item: dict):
    """将任务推送到指定的 Redis 队列"""
    redis = await get_redis_connection()
    try:
        await redis.rpush(queue_name, json.dumps(item))
        logger.debug(f"已推送任务到队列: {queue_name}")
    except Exception as e:
        logger.error(f"推送任务到队列失败: {e}")
    finally:
        redis.close()
        await redis.wait_closed()

async def get_queue_length(queue_name: str) -> int:
    """获取队列长度"""
    redis = await get_redis_connection()
    try:
        length = await redis.llen(queue_name)
        return length
    except Exception as e:
        logger.error(f"获取队列长度失败: {e}")
        return 0
    finally:
        redis.close()
        await redis.wait_closed()

================
File: utils/sentence_logger.py
================
import logging
import json
from pathlib import Path
from typing import List, Dict, Any
import asyncio
from utils.worker_decorators import handle_errors

logger = logging.getLogger(__name__)

class SentenceLogger:
    """句子日志记录器"""
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self._lock = asyncio.Lock()
    
    def _format_sentence(self, sentence: Dict[str, Any]) -> Dict[str, Any]:
        return {
            'id': getattr(sentence, 'sentence_id', -1),
            'start_time': getattr(sentence, 'start', 0),
            'end_time': getattr(sentence, 'end', 0),
            'text': getattr(sentence, 'text', ''),
            'translation': getattr(sentence, 'translation', ''),
            'duration': getattr(sentence, 'duration', 0),
            'speaker_id': getattr(sentence, 'speaker_id', 0),
            'speaker_similarity': getattr(sentence, 'speaker_similarity', 0),
            'speaker_embedding': (
                getattr(sentence, 'speaker_embedding', []).tolist() 
                if hasattr(getattr(sentence, 'speaker_embedding', []), 'tolist') 
                else getattr(sentence, 'speaker_embedding', [])
            )
        }
    
    @handle_errors(logger)
    async def save_sentences(self, sentences: List[Dict[str, Any]], output_path: Path, task_id: str) -> None:
        async with self._lock:
            try:
                formatted_sentences = [self._format_sentence(s) for s in sentences]
                output_path.parent.mkdir(parents=True, exist_ok=True)
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(formatted_sentences, f, ensure_ascii=False, indent=2)
                
                self.logger.debug(f"已保存 {len(sentences)} 个句子到 {output_path}")
            except Exception as e:
                self.logger.error(f"保存句子信息失败: {e}")
                raise

================
File: utils/serialization.py
================
import msgpack
import numpy as np
import logging
from typing import Any, Dict, List, Optional, Union, Tuple
import base64
import json

logger = logging.getLogger(__name__)

class NumpyHandler:
    """处理 numpy 数组的序列化和反序列化"""
    
    @staticmethod
    def encode(obj):
        if isinstance(obj, np.ndarray):
            return {
                "__numpy__": True,
                "data": base64.b64encode(obj.tobytes()).decode('ascii'),
                "dtype": str(obj.dtype),
                "shape": obj.shape
            }
        return obj
    
    @staticmethod
    def decode(obj):
        if isinstance(obj, dict) and obj.get("__numpy__", False):
            data = base64.b64decode(obj["data"])
            dtype = np.dtype(obj["dtype"])
            shape = obj["shape"]
            return np.frombuffer(data, dtype=dtype).reshape(shape)
        return obj

class SentenceSerializer:
    """句子对象的序列化和反序列化工具类"""
    
    @staticmethod
    def serialize(obj: Any) -> bytes:
        """
        将对象序列化为 msgpack 格式的二进制数据
        
        Args:
            obj: 要序列化的对象
            
        Returns:
            bytes: 序列化后的二进制数据
        """
        try:
            # 处理特殊类型（如 numpy 数组）
            def default(obj):
                # 处理 numpy 标量类型
                if isinstance(obj, (np.integer, np.int_, np.int8, np.int16, np.int32, np.int64)):
                    return int(obj)
                if isinstance(obj, (np.float_, np.float16, np.float32, np.float64)):
                    return float(obj)
                if isinstance(obj, (np.bool_)):
                    return bool(obj)
                if isinstance(obj, (np.ndarray,)):
                    return NumpyHandler.encode(obj)
                
                # 处理 Sentence 对象 - 将其转换为字典
                if hasattr(obj, '__dict__'):
                    return {
                        "__sentence__": True,
                        "data": {k: v for k, v in obj.__dict__.items() if not k.startswith('_')}
                    }
                
                # 其他类型的处理
                try:
                    return str(obj)
                except:
                    logger.warning(f"无法序列化的类型: {type(obj)}")
                    return None
            
            # 序列化为 msgpack 格式
            return msgpack.packb(obj, default=default, use_bin_type=True)
        except Exception as e:
            logger.error(f"序列化失败: {e}")
            # 失败时返回一个空字典的序列化结果
            return msgpack.packb({}, use_bin_type=True)
    
    @staticmethod
    def deserialize(data: bytes) -> Any:
        """
        将 msgpack 格式的二进制数据反序列化为对象
        
        Args:
            data: 要反序列化的二进制数据
            
        Returns:
            Any: 反序列化后的对象
        """
        try:
            # 定义 object_hook 函数处理特殊类型
            def object_hook(obj):
                if isinstance(obj, dict) and obj.get("__sentence__", False):
                    return type('Sentence', (), obj["data"])
                return NumpyHandler.decode(obj)
            
            # 反序列化
            return msgpack.unpackb(data, object_hook=object_hook, raw=False)
        except Exception as e:
            logger.error(f"反序列化失败: {e}")
            return {}

    @staticmethod
    def serialize_to_redis(obj: Any) -> str:
        """
        将对象序列化为适合存储在 Redis 中的字符串
        
        Args:
            obj: 要序列化的对象
            
        Returns:
            str: 序列化后的字符串
        """
        # 先序列化为 msgpack 二进制，再编码为 base64 字符串
        binary_data = SentenceSerializer.serialize(obj)
        return base64.b64encode(binary_data).decode('ascii')
    
    @staticmethod
    def deserialize_from_redis(data: str) -> Any:
        """
        从 Redis 中获取的字符串反序列化为对象
        
        Args:
            data: 要反序列化的字符串
            
        Returns:
            Any: 反序列化后的对象
        """
        # 先解码 base64 字符串为二进制，再反序列化
        binary_data = base64.b64decode(data.encode('ascii'))
        return SentenceSerializer.deserialize(binary_data)

================
File: utils/task_state.py
================
# ---------------------------------
# backend/utils/task_state.py (完整可复制版本)
# ---------------------------------
from dataclasses import dataclass, field
from typing import Any, Dict
from utils.task_storage import TaskPaths

@dataclass
class TaskState:
    """
    每个任务的独立状态：包括处理进度、分段信息等
    """
    task_id: str
    task_paths: TaskPaths
    target_language: str = "zh"
    generate_subtitle: bool = False

    # 已处理到的句子计数
    sentence_counter: int = 0

    # 时间戳记录
    current_time: float = 0

    # 第几个 HLS 批次 (混音后输出)
    batch_counter: int = 0

    # 每个分段对应的媒体文件信息
    segment_media_files: Dict[int, Dict[str, Any]] = field(default_factory=dict)

    # 记录 mixing_worker 产出的每个 segment_xxx.mp4
    merged_segments: list = field(default_factory=list)

    # 记录总分段数
    total_segments: int = 0

    # 错误记录
    errors: list = field(default_factory=list)

    def to_dict(self) -> dict:
        """将 TaskState 转换为字典，用于 Redis 存储"""
        return {
            "task_id": self.task_id,
            "task_paths": {
                "task_dir": str(self.task_paths.task_dir),
                "processing_dir": str(self.task_paths.processing_dir),
                "output_dir": str(self.task_paths.output_dir),
                "segments_dir": str(self.task_paths.segments_dir),
                "audio_dir": str(self.task_paths.audio_dir),
                "subtitle_dir": str(self.task_paths.subtitle_dir),
            },
            "target_language": self.target_language,
            "generate_subtitle": self.generate_subtitle,
            "sentence_counter": self.sentence_counter,
            "current_time": self.current_time,
            "batch_counter": self.batch_counter,
            "segment_media_files": self.segment_media_files,
            "merged_segments": self.merged_segments,
            "total_segments": self.total_segments,
            "errors": self.errors,
        }

    @classmethod
    def from_dict(cls, data: dict) -> 'TaskState':
        """从字典创建 TaskState 实例，用于从 Redis 恢复"""
        task_paths = TaskPaths(
            task_dir=data["task_paths"]["task_dir"],
            processing_dir=data["task_paths"]["processing_dir"],
            output_dir=data["task_paths"]["output_dir"],
            segments_dir=data["task_paths"]["segments_dir"],
            audio_dir=data["task_paths"]["audio_dir"],
            subtitle_dir=data["task_paths"]["subtitle_dir"],
        )
        
        task_state = cls(
            task_id=data["task_id"],
            task_paths=task_paths,
            target_language=data["target_language"],
            generate_subtitle=data["generate_subtitle"],
        )
        
        task_state.sentence_counter = data["sentence_counter"]
        task_state.current_time = data["current_time"]
        task_state.batch_counter = data["batch_counter"]
        task_state.segment_media_files = data["segment_media_files"]
        task_state.merged_segments = data["merged_segments"]
        task_state.total_segments = data["total_segments"]
        task_state.errors = data["errors"]
        
        return task_state
        
    def all_segments_processed(self) -> bool:
        """检查是否所有分段都已处理完成"""
        if self.total_segments == 0:
            return False
        return len(self.merged_segments) >= self.total_segments

================
File: utils/task_storage.py
================
import shutil
import logging
import os
from pathlib import Path
from config import Config
from typing import Optional, Union

logger = logging.getLogger(__name__)

class TaskPaths:
    def __init__(
        self, 
        task_dir: Union[str, Path],
        processing_dir: Optional[Union[str, Path]] = None,
        output_dir: Optional[Union[str, Path]] = None,
        segments_dir: Optional[Union[str, Path]] = None,
        audio_dir: Optional[Union[str, Path]] = None,
        subtitle_dir: Optional[Union[str, Path]] = None,
        config: Optional[Config] = None,
        task_id: Optional[str] = None
    ):
        """
        初始化任务路径
        可以通过两种方式初始化：
        1. 提供 config 和 task_id，自动生成所有路径
        2. 直接提供各个路径，用于从 Redis 恢复
        """
        # 将所有路径转换为 Path 对象
        self.task_dir = Path(task_dir)
        
        # 如果提供了 config 和 task_id，使用配置生成路径
        if config and task_id:
            self.config = config
            self.task_id = task_id

            self.task_dir = config.TASKS_DIR / task_id
            self.input_dir = self.task_dir / "input"
            self.processing_dir = self.task_dir / "processing"
            self.output_dir = self.task_dir / "output"

            self.segments_dir = config.PUBLIC_DIR / "segments" / task_id
            self.playlist_path = config.PUBLIC_DIR / "playlists" / f"playlist_{task_id}.m3u8"

            self.media_dir = self.processing_dir / "media"
            self.processing_segments_dir = self.processing_dir / "segments"
            self.audio_dir = self.processing_dir / "audio"
            self.subtitle_dir = self.processing_dir / "subtitle"
        else:
            # 直接使用提供的路径
            self.processing_dir = Path(processing_dir) if processing_dir else self.task_dir / "processing"
            self.output_dir = Path(output_dir) if output_dir else self.task_dir / "output"
            self.segments_dir = Path(segments_dir) if segments_dir else self.task_dir / "segments"
            self.audio_dir = Path(audio_dir) if audio_dir else self.processing_dir / "audio"
            self.subtitle_dir = Path(subtitle_dir) if subtitle_dir else self.processing_dir / "subtitle"
            
            # 其他可能需要的目录
            self.input_dir = self.task_dir / "input"
            self.media_dir = self.processing_dir / "media"
            self.processing_segments_dir = self.processing_dir / "segments"
            
            # 尝试从 task_dir 提取 task_id
            if not task_id:
                self.task_id = self.task_dir.name
            else:
                self.task_id = task_id

    def create_directories(self):
        """创建所有必要的目录"""
        dirs = [
            self.task_dir,
            self.input_dir,
            self.processing_dir,
            self.output_dir,
            self.segments_dir,
            self.media_dir,
            self.processing_segments_dir,
            self.audio_dir,
            self.subtitle_dir
        ]
        for d in dirs:
            d.mkdir(parents=True, exist_ok=True)
            logger.debug(f"[TaskPaths] 创建目录: {d}")

    async def cleanup(self, keep_output: bool = False):
        """清理任务目录"""
        try:
            if keep_output:
                logger.info(f"[TaskPaths] 保留输出目录, 即将清理输入/processing/segments")
                dirs_to_clean = [self.input_dir, self.processing_dir, self.segments_dir]
                for d in dirs_to_clean:
                    if d.exists():
                        shutil.rmtree(d)
                        logger.debug(f"[TaskPaths] 已清理: {d}")
            else:
                logger.info(f"[TaskPaths] 全量清理任务目录: {self.task_dir}")
                if self.task_dir.exists():
                    shutil.rmtree(str(self.task_dir))
                if hasattr(self, 'segments_dir') and self.segments_dir.exists():
                    shutil.rmtree(str(self.segments_dir))
                if hasattr(self, 'playlist_path') and self.playlist_path.exists():
                    os.remove(str(self.playlist_path))
        except Exception as e:
            logger.error(f"[TaskPaths] 清理失败: {e}")

================
File: utils/temp_file_manager.py
================
import logging
from pathlib import Path
from typing import Set

logger = logging.getLogger(__name__)

class TempFileManager:
    """临时文件管理器"""
    def __init__(self, base_dir: Path):
        self.base_dir = base_dir
        self.temp_files: Set[Path] = set()
    
    def add_file(self, file_path: Path) -> None:
        self.temp_files.add(Path(file_path))
    
    async def cleanup(self) -> None:
        for file_path in self.temp_files:
            try:
                if file_path.exists():
                    file_path.unlink()
                    logger.debug(f"已删除临时文件: {file_path}")
            except Exception as e:
                logger.warning(f"清理临时文件失败: {file_path}, 错误: {e}")
        self.temp_files.clear()

================
File: utils/worker_decorators.py
================
"""
工作者装饰器模块 - 包含所有与工作者相关的装饰器
"""
import functools
import json
import asyncio
import time
import aioredis
from pathlib import Path
from typing import Callable, Any, Optional, AsyncGenerator, TypeVar, Union, Literal, Dict
from utils.redis_utils import load_task_state, save_task_state, get_redis_connection
from utils.serialization import SentenceSerializer
from utils.log_config import get_logger

logger = get_logger(__name__)
T = TypeVar('T')
WorkerResult = Union[T, AsyncGenerator[T, None]]
WorkerMode = Literal['base', 'stream']
SerializationMode = Literal['json', 'msgpack']

def handle_errors(custom_logger = None) -> Callable:
    """错误处理装饰器。可应用于需要统一捕获日志的异步函数。"""
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        async def wrapper(*args, **kwargs) -> Any:
            # 如果当前对象有 logger 属性则使用，否则用传入的或全局 logger
            actual_logger = custom_logger if custom_logger else (getattr(args[0], 'logger', logger) if args else logger)
            start_time = time.time()
            try:
                result = await func(*args, **kwargs)
                elapsed = time.time() - start_time
                actual_logger.info(f"{func.__name__} 正常结束，耗时 {elapsed:.2f}s")
                return result
            except Exception as e:
                elapsed = time.time() - start_time
                actual_logger.error(f"{func.__name__} 执行出错，耗时 {elapsed:.2f}s, 错误: {e}", exc_info=True)
                raise
        return wrapper
    return decorator

def worker_decorator(
    input_queue_attr: str,
    next_queue_attr: Optional[str] = None,
    worker_name: Optional[str] = None,
    mode: WorkerMode = 'base'
) -> Callable:
    """通用 Worker 装饰器 (内存队列版本)"""
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        async def wrapper(self, task_state, *args, **kwargs):
            worker_display_name = worker_name or func.__name__
            wlogger = getattr(self, 'logger', logger)

            input_queue = getattr(task_state, input_queue_attr)
            next_queue = getattr(task_state, next_queue_attr) if next_queue_attr else None

            wlogger.info(f"[{worker_display_name}] 启动 (TaskID={task_state.task_id}). "
                         f"输入队列: {input_queue_attr}, 下游队列: {next_queue_attr if next_queue_attr else '无'}")

            processed_count = 0
            try:
                while True:
                    try:
                        queue_size_before = input_queue.qsize()
                        item = await input_queue.get()
                        if item is None:
                            if next_queue:
                                await next_queue.put(None)
                            wlogger.info(f"[{worker_display_name}] 收到停止信号。已处理 {processed_count} 个item。")
                            break

                        wlogger.debug(f"[{worker_display_name}] 从 {input_queue_attr} 取出一个item. 队列剩余: {queue_size_before}")

                        start_time = time.time()
                        if mode == 'stream':
                            async for result in func(self, item, task_state, *args, **kwargs):
                                if result is not None and next_queue:
                                    await next_queue.put(result)
                        else:
                            result = await func(self, item, task_state, *args, **kwargs)
                            if result is not None and next_queue:
                                await next_queue.put(result)

                        processed_count += 1
                        elapsed = time.time() - start_time
                        wlogger.debug(f"[{worker_display_name}] item处理完成，耗时 {elapsed:.2f}s. "
                                      f"TaskID={task_state.task_id}, 已处理计数: {processed_count}")

                    except asyncio.CancelledError:
                        wlogger.warning(f"[{worker_display_name}] 被取消 (TaskID={task_state.task_id}). "
                                        f"已处理 {processed_count} 个item")
                        if next_queue:
                            await next_queue.put(None)
                        break
                    except Exception as e:
                        wlogger.error(f"[{worker_display_name}] 发生异常: {e} (TaskID={task_state.task_id}). "
                                      f"已处理 {processed_count} 个item", exc_info=True)
                        if next_queue:
                            await next_queue.put(None)
                        break
            finally:
                wlogger.info(f"[{worker_display_name}] 结束 (TaskID={task_state.task_id}). 共处理 {processed_count} 个item.")

        return wrapper
    return decorator

def redis_worker_decorator(
    input_queue: str,
    next_queue: Optional[str] = None,
    worker_name: Optional[str] = None,
    mode: WorkerMode = 'base',
    serialization_mode: SerializationMode = 'msgpack'
) -> Callable:
    """Redis 队列 Worker 装饰器
    
    Args:
        input_queue: 输入队列名称
        next_queue: 下游队列名称（可选）
        worker_name: Worker 显示名称（可选）
        mode: Worker 模式，'base' 或 'stream'
        serialization_mode: 序列化模式，'json' 或 'msgpack'
    """
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        async def wrapper(self, *args, **kwargs):
            worker_display_name = worker_name or func.__name__
            wlogger = getattr(self, 'logger', logger)

            redis = await get_redis_connection()
            wlogger.info(f"[{worker_display_name}] 启动. 输入队列: {input_queue}, 下游队列: {next_queue if next_queue else '无'}, "
                        f"序列化模式: {serialization_mode}")

            processed_count = 0
            try:
                while True:
                    try:
                        # 从 Redis 队列获取任务
                        item_raw = await redis.blpop(input_queue, timeout=0)
                        if not item_raw:
                            continue
                        
                        # 根据序列化模式反序列化数据
                        if serialization_mode == 'json':
                            item_data = json.loads(item_raw[1].decode('utf-8'))
                        else:  # msgpack
                            # 检查是否为 JSON 格式的控制消息
                            try:
                                item_str = item_raw[1].decode('utf-8')
                                if item_str.startswith('{') and item_str.endswith('}'):
                                    item_data = json.loads(item_str)
                                    if 'stop_signal' in item_data:
                                        # 控制消息使用 JSON 格式
                                        pass
                                    else:
                                        # 尝试从 JSON 中提取 msgpack 数据
                                        if 'data' in item_data and isinstance(item_data['data'], str):
                                            item_data['data'] = SentenceSerializer.deserialize_from_redis(item_data['data'])
                                else:
                                    # 直接使用 msgpack 反序列化
                                    item_data = SentenceSerializer.deserialize(item_raw[1])
                            except UnicodeDecodeError:
                                # 二进制数据，直接使用 msgpack 反序列化
                                item_data = SentenceSerializer.deserialize(item_raw[1])
                            except Exception as e:
                                wlogger.error(f"[{worker_display_name}] 反序列化失败: {e}")
                                continue
                        
                        if item_data.get('stop_signal'):
                            if next_queue:
                                # 停止信号使用 JSON 格式
                                await redis.rpush(next_queue, json.dumps({'stop_signal': True}))
                            wlogger.info(f"[{worker_display_name}] 收到停止信号。已处理 {processed_count} 个item。")
                            break

                        task_id = item_data.get('task_id')
                        if not task_id:
                            wlogger.warning(f"[{worker_display_name}] 收到无效任务，缺少 task_id")
                            continue

                        # 获取任务路径
                        task_dir = Path(item_data.get('task_dir', f"/tmp/tasks/{task_id}"))
                        
                        # 加载任务状态
                        try:
                            task_state = await load_task_state(task_id, task_dir)
                            if not task_state:
                                wlogger.warning(f"[{worker_display_name}] 无法加载任务状态，跳过任务 {task_id}")
                                continue
                        except Exception as e:
                            wlogger.error(f"[{worker_display_name}] 加载任务状态失败: {e}, 跳过任务 {task_id}")
                            continue

                        wlogger.info(f"[{worker_display_name}] 从 {input_queue} 取出一个item. TaskID={task_id}")
                        
                        # 打印反序列化后的第一个句子对象内容（如果存在）
                        try:
                            data_to_log = item_data.get('data', item_data) if isinstance(item_data, dict) else item_data
                            wlogger.debug(f"[{worker_display_name}] 解析后的输入数据类型: {type(data_to_log)}")
                            
                            if isinstance(data_to_log, list) and len(data_to_log) > 0:
                                first_item = data_to_log[0]
                                wlogger.debug(f"[{worker_display_name}] 第一个元素类型: {type(first_item)}")
                                
                                if hasattr(first_item, '__dict__'):
                                    # 如果是对象，打印其属性
                                    item_attrs = {k: str(v) for k, v in first_item.__dict__.items() 
                                                if k not in ['audio_data', 'waveform'] and not k.startswith('_')}
                                    wlogger.debug(f"[{worker_display_name}] 第一个句子对象属性: {item_attrs}")
                                elif isinstance(first_item, dict):
                                    # 如果是字典，直接打印（排除大型二进制数据）
                                    filtered_dict = {k: str(v) for k, v in first_item.items() 
                                                  if k not in ['audio_data', 'waveform'] and not isinstance(v, bytes)}
                                    wlogger.debug(f"[{worker_display_name}] 第一个句子对象: {filtered_dict}")
                                else:
                                    wlogger.debug(f"[{worker_display_name}] 第一个句子对象类型: {type(first_item)}, 值: {str(first_item)}")
                            elif isinstance(data_to_log, dict):
                                # 直接是单个字典对象
                                filtered_dict = {k: str(v) for k, v in data_to_log.items() 
                                              if k not in ['audio_data', 'waveform'] and not isinstance(v, bytes)}
                                wlogger.debug(f"[{worker_display_name}] 句子对象: {filtered_dict}")
                        except Exception as e:
                            wlogger.debug(f"[{worker_display_name}] 打印句子对象失败: {str(e)}")
                            import traceback
                            wlogger.debug(f"[{worker_display_name}] 错误详情: {traceback.format_exc()}")

                        start_time = time.time()
                        if mode == 'stream':
                            async for result in func(self, item_data, task_state, *args, **kwargs):
                                if result is not None and next_queue:
                                    # 根据序列化模式序列化数据
                                    if serialization_mode == 'json':
                                        await redis.rpush(next_queue, json.dumps({
                                            'task_id': task_id,
                                            'task_dir': str(task_dir),
                                            'data': result
                                        }))
                                    else:  # msgpack
                                        # 将结果序列化为 msgpack 格式
                                        serialized_result = SentenceSerializer.serialize_to_redis(result)
                                        await redis.rpush(next_queue, json.dumps({
                                            'task_id': task_id,
                                            'task_dir': str(task_dir),
                                            'data': serialized_result
                                        }))
                        else:
                            result = await func(self, item_data, task_state, *args, **kwargs)
                            if result is not None and next_queue:
                                # 根据序列化模式序列化数据
                                if serialization_mode == 'json':
                                    await redis.rpush(next_queue, json.dumps({
                                        'task_id': task_id,
                                        'task_dir': str(task_dir),
                                        'data': result
                                    }))
                                else:  # msgpack
                                    # 将结果序列化为 msgpack 格式
                                    serialized_result = SentenceSerializer.serialize_to_redis(result)
                                    await redis.rpush(next_queue, json.dumps({
                                        'task_id': task_id,
                                        'task_dir': str(task_dir),
                                        'data': serialized_result
                                    }))

                        # 保存更新后的任务状态
                        await save_task_state(task_state)

                        processed_count += 1
                        elapsed = time.time() - start_time
                        wlogger.info(f"[{worker_display_name}] item处理完成，耗时 {elapsed:.2f}s. "
                                    f"TaskID={task_id}, 已处理计数: {processed_count}")

                    except asyncio.CancelledError:
                        wlogger.warning(f"[{worker_display_name}] 被取消. 已处理 {processed_count} 个item")
                        if next_queue:
                            await redis.rpush(next_queue, json.dumps({'stop_signal': True}))
                        break
                    except Exception as e:
                        wlogger.error(f"[{worker_display_name}] 发生异常: {e}. 已处理 {processed_count} 个item", exc_info=True)
                        if next_queue:
                            await redis.rpush(next_queue, json.dumps({'stop_signal': True}))
                        break
            finally:
                wlogger.info(f"[{worker_display_name}] 结束. 共处理 {processed_count} 个item.")
                redis.close()
                await redis.wait_closed()

        return wrapper
    return decorator

================
File: workers/asr_worker/auto_sense.py
================
import logging
import os
import importlib.util
import sys
import torch
import numpy as np
from tqdm import tqdm
from pathlib import Path
import time
from funasr.register import tables
from funasr.auto.auto_model import AutoModel as BaseAutoModel
from funasr.auto.auto_model import prepare_data_iterator
from funasr.utils.misc import deep_update
from funasr.models.campplus.utils import sv_chunk, postprocess
from funasr.models.campplus.cluster_backend import ClusterBackend
from .sentence_tools import get_sentences
from funasr.utils.vad_utils import slice_padding_audio_samples
from funasr.utils.load_utils import load_audio_text_image_video

# [MODIFIED] 新增以下导入，用于在 async 函数中包装同步调用
from utils import concurrency
from functools import partial

class SenseAutoModel(BaseAutoModel):
    def __init__(self, config, **kwargs):
        super().__init__(**kwargs)
        self.logger = logging.getLogger(__name__)
        self.config = config
        
        if self.spk_model is not None:
            self.cb_model = ClusterBackend().to(kwargs["device"])
            spk_mode = kwargs.get("spk_mode", "punc_segment")
            if spk_mode not in ["default", "vad_segment", "punc_segment"]:
                self.logger.error("spk_mode 应该是 'default', 'vad_segment' 或 'punc_segment' 之一。")
            self.spk_mode = spk_mode

    def inference_with_vad(self, input, input_len=None, **cfg):
        kwargs = self.kwargs
        self.tokenizer = kwargs.get("tokenizer")
        deep_update(self.vad_kwargs, cfg)
        
        res = self.inference(input, input_len=input_len, model=self.vad_model, kwargs=self.vad_kwargs, **cfg)
        model = self.model
        deep_update(kwargs, cfg)
        kwargs["batch_size"] = max(int(kwargs.get("batch_size_s", 300)) * 1000, 1)
        batch_size_threshold_ms = int(kwargs.get("batch_size_threshold_s", 60)) * 1000

        key_list, data_list = prepare_data_iterator(input, input_len=input_len, data_type=kwargs.get("data_type", None))
        results_ret_list = []

        pbar_total = tqdm(total=len(res), dynamic_ncols=True, disable=kwargs.get("disable_pbar", False))

        for i, item in enumerate(res):
            key, vadsegments = item["key"], item["value"]
            input_i = data_list[i]
            fs = kwargs["frontend"].fs if hasattr(kwargs["frontend"], "fs") else 16000
            speech = load_audio_text_image_video(input_i, fs=fs, audio_fs=kwargs.get("fs", 16000))
            speech_lengths = len(speech)
            self.logger.debug(f"音频长度: {speech_lengths} 样本")

            if speech_lengths < 400:
                self.logger.warning(f"音频太短（{speech_lengths} 样本），可能导致处理错误")

            sorted_data = sorted([(seg, idx) for idx, seg in enumerate(vadsegments)], key=lambda x: x[0][1] - x[0][0])
            if not sorted_data:
                self.logger.info(f"解码, utt: {key}, 空语音")
                continue

            results_sorted = []
            all_segments = []
            beg_idx, end_idx = 0, 1
            max_len_in_batch = 0

            for j in range(len(sorted_data)):
                sample_length = sorted_data[j][0][1] - sorted_data[j][0][0]
                potential_batch_length = max(max_len_in_batch, sample_length) * (j + 1 - beg_idx)

                if (j < len(sorted_data) - 1 and 
                    sample_length < batch_size_threshold_ms and 
                    potential_batch_length < kwargs["batch_size"]):
                    max_len_in_batch = max(max_len_in_batch, sample_length)
                    end_idx += 1
                    continue

                speech_j, _ = slice_padding_audio_samples(speech, speech_lengths, sorted_data[beg_idx:end_idx])
                results = self.inference(speech_j, input_len=None, model=model, kwargs=kwargs, **cfg)

                if self.spk_model is not None:
                    for _b, speech_segment in enumerate(speech_j):
                        vad_segment = sorted_data[beg_idx:end_idx][_b][0]
                        segments = sv_chunk([[vad_segment[0] / 1000.0, vad_segment[1] / 1000.0, np.array(speech_segment)]])
                        all_segments.extend(segments)
                        speech_b = [seg[2] for seg in segments]
                        spk_res = self.inference(speech_b, input_len=None, model=self.spk_model, kwargs=kwargs, **cfg)
                        results[_b]["spk_embedding"] = spk_res[0]["spk_embedding"]
                beg_idx, end_idx = end_idx, end_idx + 1
                max_len_in_batch = sample_length
                results_sorted.extend(results)

            if len(results_sorted) != len(sorted_data):
                self.logger.info(f"解码，utt: {key}，空结果")
                continue

            restored_data = [0] * len(sorted_data)
            for j, (_, idx) in enumerate(sorted_data):
                restored_data[idx] = results_sorted[j]

            result = self.combine_results(restored_data, vadsegments)

            if self.spk_model is not None and kwargs.get("return_spk_res", True):
                all_segments.sort(key=lambda x: x[0])
                spk_embedding = result["spk_embedding"]
                labels = self.cb_model(spk_embedding.cpu(), oracle_num=kwargs.get("preset_spk_num", None))
                sv_output = postprocess(all_segments, None, labels, spk_embedding.cpu())

                if "timestamp" not in result:
                    self.logger.error(f"speaker diarization 依赖于时间戳对于 utt: {key}")
                    sentence_list = []
                else:
                    sentence_list = get_sentences(
                        tokens=result["token"],
                        timestamps=result["timestamp"],
                        tokenizer=self.tokenizer,
                        speech=speech,
                        sd_time_list=sv_output,
                        sample_rate=fs,
                        config=self.config
                    )
                    results_ret_list = sentence_list
            else:
                sentence_list = []
            pbar_total.update(1)

        pbar_total.close()
        return results_ret_list

    def combine_results(self, restored_data, vadsegments):
        result = {}
        for j, data in enumerate(restored_data):
            for k, v in data.items():
                if k.startswith("timestamp"):
                    if k not in result:
                        result[k] = []
                    for t in v:
                        t[0] += vadsegments[j][0]
                        t[1] += vadsegments[j][0]
                    result[k].extend(v)
                elif k == "spk_embedding":
                    if k not in result:
                        result[k] = v
                    else:
                        result[k] = torch.cat([result[k], v], dim=0)
                elif "token" in k:
                    if k not in result:
                        result[k] = v
                    else:
                        result[k].extend(v)
                else:
                    if k not in result:
                        result[k] = v
                    else:
                        result[k] += v
        return result

    # [MODIFIED] 统一使用 concurrency.run_sync 来执行 self.generate
    async def generate_async(self, input, input_len=None, **cfg):
        func = partial(self.generate, input, input_len, **cfg)
        return await concurrency.run_sync(func)

================
File: workers/asr_worker/sentence_tools.py
================
import os
import torch
import torchaudio
import numpy as np
from typing import List, Tuple, Dict
from dataclasses import dataclass, field
from pathlib import Path
from config import Config

Token = int
Timestamp = Tuple[float, float]
SpeakerSegment = Tuple[float, float, int]

@dataclass
class Sentence:
    raw_text: str
    start: float
    end: float
    speaker_id: int
    trans_text: str = field(default="")
    sentence_id: int = field(default=-1)
    audio_path: str = field(default="")  # 存储音频文件路径
    target_duration: float = field(default=None)
    duration: float = field(default=0.0)
    diff: float = field(default=0.0)
    silence_duration: float = field(default=0.0)
    speed: float = field(default=1.0)
    is_first: bool = field(default=False)
    is_last: bool = field(default=False)
    model_input: Dict = field(default_factory=dict)
    generated_audio: np.ndarray = field(default=None)
    adjusted_start: float = field(default=0.0)
    adjusted_duration: float = field(default=0.0)
    segment_index: int = field(default=-1)
    segment_start: float = field(default=0.0)
    task_id: str = field(default="")

def tokens_timestamp_sentence(tokens: List[Token], timestamps: List[Timestamp], speaker_segments: List[SpeakerSegment], tokenizer, config: Config) -> List[Tuple[List[Token], List[Timestamp], int]]:
    sentences = []
    current_tokens = []
    current_timestamps = []
    token_index = 0

    for segment in speaker_segments:
        seg_start_ms = int(segment[0] * 1000)
        seg_end_ms = int(segment[1] * 1000)
        speaker_id = segment[2]

        while token_index < len(tokens):
            token = tokens[token_index]
            token_start, token_end = timestamps[token_index]

            if token_start >= seg_end_ms:
                break
            if token_end <= seg_start_ms:
                token_index += 1
                continue

            current_tokens.append(token)
            current_timestamps.append(timestamps[token_index])
            token_index += 1

            if token in config.STRONG_END_TOKENS and len(current_tokens) <= config.MIN_SENTENCE_LENGTH:
                if sentences:
                    previous_end_time = sentences[-1][1][-1][1]
                    current_start_time = current_timestamps[0][0]
                    time_gap = current_start_time - previous_end_time

                    if time_gap > config.SHORT_SENTENCE_MERGE_THRESHOLD_MS:
                        continue

                    sentences[-1] = (
                        sentences[-1][0] + current_tokens[:],
                        sentences[-1][1] + current_timestamps[:],
                        sentences[-1][2]
                    )
                    current_tokens.clear()
                    current_timestamps.clear()
                continue

            if (token in config.STRONG_END_TOKENS or len(current_tokens) > config.MAX_TOKENS_PER_SENTENCE):
                sentences.append((current_tokens[:], current_timestamps[:], speaker_id))
                current_tokens.clear()
                current_timestamps.clear()

        if current_tokens:
            if len(current_tokens) >= config.MIN_SENTENCE_LENGTH or not sentences:
                sentences.append((current_tokens[:], current_timestamps[:], speaker_id))
                current_tokens.clear()
                current_timestamps.clear()
            else:
                continue

    if current_tokens:
        if len(current_tokens) >= config.MIN_SENTENCE_LENGTH or not sentences:
            sentences.append((current_tokens[:], current_timestamps[:], speaker_id))
            current_tokens.clear()
            current_timestamps.clear()
        else:
            sentences[-1] = (
                sentences[-1][0] + current_tokens[:],
                sentences[-1][1] + current_timestamps[:],
                sentences[-1][2]
            )
            current_tokens.clear()
            current_timestamps.clear()

    return sentences

def merge_sentences(raw_sentences: List[Tuple[List[Token], List[Timestamp], int]], 
                   tokenizer,
                   input_duration: float,
                   config: Config) -> List[Sentence]:
    merged_sentences = []
    current = None
    current_tokens_count = 0

    for tokens, timestamps, speaker_id in raw_sentences:
        time_gap = timestamps[0][0] - current.end if current else float('inf')
        
        if (current and 
            current.speaker_id == speaker_id and 
            current_tokens_count + len(tokens) <= config.MAX_TOKENS_PER_SENTENCE and
            time_gap <= config.MAX_GAP_MS):
            current.raw_text += tokenizer.decode(tokens)
            current.end = timestamps[-1][1]
            current_tokens_count += len(tokens)
        else:
            if current:
                current.target_duration = timestamps[0][0] - current.start
                merged_sentences.append(current)
            
            text = tokenizer.decode(tokens)
            current = Sentence(
                raw_text=text, 
                start=timestamps[0][0], 
                end=timestamps[-1][1], 
                speaker_id=speaker_id,
            )
            current_tokens_count = len(tokens)

    if current:
        current.target_duration = input_duration - current.start
        merged_sentences.append(current)

    if merged_sentences:
        merged_sentences[0].is_first = True
        merged_sentences[-1].is_last = True

    return merged_sentences

def extract_audio(sentences: List[Sentence], speech: torch.Tensor, sr: int, config: Config) -> List[Sentence]:
    target_samples = int(config.SPEAKER_AUDIO_TARGET_DURATION * sr)
    speech = speech.unsqueeze(0) if speech.dim() == 1 else speech

    speaker_segments: Dict[int, List[Tuple[int, int, int]]] = {}
    for idx, s in enumerate(sentences):
        start_sample = int(s.start * sr / 1000)
        end_sample = int(s.end * sr / 1000)
        speaker_segments.setdefault(s.speaker_id, []).append((start_sample, end_sample, idx))

    output_dir = Path(config.TASKS_DIR) / sentences[0].task_id / 'speakers'
    output_dir.mkdir(parents=True, exist_ok=True)

    # 检查每个说话人的音频文件是否已存在
    for speaker_id in speaker_segments.keys():
        output_path = output_dir / f'speaker_{speaker_id}.wav'
        
        # 如果文件已存在，直接设置路径并跳过处理
        if output_path.exists():
            for sentence in sentences:
                if sentence.speaker_id == speaker_id:
                    sentence.audio_path = str(output_path)
            continue
            
        # 文件不存在，需要处理并保存
        segments = speaker_segments[speaker_id]
        segments.sort(key=lambda x: x[1] - x[0], reverse=True)
        longest_start, longest_end, _ = segments[0]

        ignore_samples = int(0.5 * sr)
        adjusted_start = longest_start + ignore_samples
        available_length_adjusted = longest_end - adjusted_start

        if available_length_adjusted > 0:
            audio_length = min(target_samples, available_length_adjusted)
            speaker_audio = speech[:, adjusted_start:adjusted_start + audio_length]
        else:
            available_length_original = longest_end - longest_start
            audio_length = min(target_samples, available_length_original)
            speaker_audio = speech[:, longest_start:longest_start + audio_length]

        torchaudio.save(str(output_path), speaker_audio, sr)

        for sentence in sentences:
            if sentence.speaker_id == speaker_id:
                sentence.audio_path = str(output_path)

    return sentences

def get_sentences(tokens: List[Token],
                  timestamps: List[Timestamp],
                  speech: torch.Tensor,
                  tokenizer,
                  sd_time_list: List[SpeakerSegment],
                  sample_rate: int = 16000,
                  config: Config = None) -> List[Sentence]:
    if config is None:
        config = Config()

    input_duration = (speech.shape[-1] / sample_rate) * 1000

    raw_sentences = tokens_timestamp_sentence(tokens, timestamps, sd_time_list, tokenizer, config)
    merged_sentences = merge_sentences(raw_sentences, tokenizer, input_duration, config)
    sentences_with_audio = extract_audio(merged_sentences, speech, sample_rate, config)

    return sentences_with_audio

================
File: workers/asr_worker/worker.py
================
from typing import Dict, Any
import asyncio
from utils.task_state import TaskState
from utils.worker_decorators import redis_worker_decorator
from utils.log_config import get_logger
from .auto_sense import SenseAutoModel

logger = get_logger(__name__)

class ASRWorker:
    """语音识别 Worker"""

    def __init__(self, config):
        """初始化 ASRWorker"""
        self.config = config
        self.logger = logger
        
        # 直接初始化 ASR 模型
        self.sense_model = SenseAutoModel(
            config=config,
            **config.ASR_MODEL_KWARGS
        )

    @redis_worker_decorator(
        input_queue='asr_queue',
        next_queue='translation_queue',
        worker_name='ASR Worker',
        serialization_mode='msgpack'
    )
    async def run(self, item: Dict[str, Any], task_state: TaskState) -> Dict[str, Any]:
        """处理音频识别"""
        self.logger.debug(f"[ASR Worker] 收到任务, TaskID={task_state.task_id}")
        
        # 获取音频文件路径
        segment_info = item.get('data', item) if isinstance(item, dict) else item
        
        self.logger.debug(
            f"[ASR Worker] 开始处理分段 {segment_info['segment_index']} -> TaskID={task_state.task_id}"
        )
        
        # 执行语音识别
        try:
            # 调用 ASR 模型（异步接口）
            sentences = await self.sense_model.generate_async(
                input=segment_info['vocals_path'],
                cache={},
                language="auto",
                use_itn=True,
                batch_size_s=60,
                merge_vad=False
            )
            
            if not sentences:
                self.logger.warning(
                    f"[ASR Worker] 分段 {segment_info['segment_index']} 未识别到语音 -> TaskID={task_state.task_id}"
                )
                return None

            self.logger.info(
                f"[ASR Worker] 识别到 {len(sentences)} 条句子, seg={segment_info['segment_index']}, TaskID={task_state.task_id}"
            )
            
            for s in sentences:
                s.segment_index = segment_info['segment_index']
                s.segment_start = segment_info['start']
                s.task_id = task_state.task_id
                s.sentence_id = task_state.sentence_counter
                task_state.sentence_counter += 1
            
            # 打印第一个句子的属性，帮助调试
            if sentences and len(sentences) > 0:
                first_sentence = sentences[0]
                self.logger.debug(f"[ASR Worker] 返回的第一个句子类型: {type(first_sentence)}")
                self.logger.debug(f"[ASR Worker] 返回的第一个句子属性: {', '.join(dir(first_sentence))}")
                self.logger.debug(f"[ASR Worker] 返回的第一个句子结构: {first_sentence.__dict__}")
                
                # 添加尝试手动序列化的代码
                try:
                    from utils.serialization import SentenceSerializer
                    serialized = SentenceSerializer.serialize([first_sentence])
                    self.logger.debug(f"[ASR Worker] 第一个句子序列化测试成功，大小：{len(serialized)}字节")
                except Exception as e:
                    self.logger.error(f"[ASR Worker] 第一个句子序列化测试失败: {str(e)}")
                    import traceback
                    self.logger.error(f"[ASR Worker] 序列化错误详情: {traceback.format_exc()}")
            
            # 直接返回 Sentence 对象列表，不需要转换为字典
            return sentences
            
        except Exception as e:
            self.logger.error(
                f"[ASR Worker] 分段 {segment_info['segment_index']} 处理失败: {e} -> TaskID={task_state.task_id}",
                exc_info=True
            )
            task_state.errors.append({
                'segment_index': segment_info['segment_index'],
                'stage': 'asr',
                'error': str(e)
            })
            return None

async def start():
    """启动 Worker"""
    config_module = __import__('config')
    config = config_module.Config()
    worker = ASRWorker(config)
    await worker.run()

if __name__ == '__main__':
    asyncio.run(start())

================
File: workers/audio_gen_worker/audio_gener.py
================
# core/audio_gener.py

import logging
import asyncio
import numpy as np
from typing import List
from services.cosyvoice.client import CosyVoiceClient
from utils import concurrency

class AudioGenerator:
    def __init__(self, cosyvoice_client: CosyVoiceClient, sample_rate: int = 24000):
        self.cosyvoice_client = cosyvoice_client
        self.sample_rate = sample_rate
        self.logger = logging.getLogger(__name__)

    async def vocal_audio_maker(self, sentences: List):
        """
        并发生成音频
        """
        tasks = []
        for s in sentences:
            text_uuid = s.model_input.get('text_uuid')
            speaker_uuid = s.model_input.get('speaker_uuid')
                
            if not text_uuid or not speaker_uuid:
                self.logger.warning("缺少text_uuid或speaker_uuid，无法生成音频")
                continue

            tasks.append(self._generate_single_async(s))

        try:
            results = await asyncio.gather(*tasks)
            return results
        except Exception as e:
            self.logger.error(f"音频生成失败: {str(e)}")
            raise

    async def _generate_single_async(self, sentence):
        try:
            final_audio = await concurrency.run_sync(self._generate_audio_single, sentence)
            sentence.generated_audio = final_audio
            return sentence
        except Exception as e:
            self.logger.error(
                f"音频生成失败 (text_uuid: {sentence.model_input.get('text_uuid', 'unknown')}, "
                f"speaker_uuid: {sentence.model_input.get('speaker_uuid', 'unknown')}): {str(e)}"
            )
            sentence.generated_audio = None
            return sentence

    def _generate_audio_single(self, sentence):
        """
        使用text_uuid和speaker_uuid从服务端获取音频
        """
        text_uuid = sentence.model_input.get('text_uuid')
        speaker_uuid = sentence.model_input.get('speaker_uuid')
        speed = getattr(sentence, 'speed', 1.0) or 1.0
        is_first = sentence.is_first
        start = sentence.start
        silence_duration = sentence.silence_duration

        if not text_uuid or not speaker_uuid:
            self.logger.warning("缺少text_uuid或speaker_uuid，无法生成音频")
            return np.zeros(0, dtype=np.float32)

        audio_np, dur_sec = self.cosyvoice_client.token2wav(text_uuid, speaker_uuid, speed=speed)

        if is_first and start > 0:
            silence_samples = int(start * self.sample_rate / 1000)
            audio_np = np.concatenate([np.zeros(silence_samples, dtype=np.float32), audio_np])

        if silence_duration > 0:
            silence_samples = int(silence_duration * self.sample_rate / 1000)
            audio_np = np.concatenate([audio_np, np.zeros(silence_samples, dtype=np.float32)])

        self.logger.debug(
            f"音频生成完成 (text_uuid={text_uuid}, speaker_uuid={speaker_uuid}), "
            f"长度={len(audio_np)/self.sample_rate:.2f}s"
        )
        return audio_np

================
File: workers/audio_gen_worker/timestamp_adjuster.py
================
import logging
from typing import List, Optional

logger = logging.getLogger(__name__)

class TimestampAdjuster:
    """句子时间戳调整器"""
    
    def __init__(self, config):
        """初始化时间戳调整器
        
        Args:
            config: 配置对象，用于获取采样率
        """
        self.logger = logging.getLogger(__name__)
        self.sample_rate = config.SAMPLE_RATE
        
    def update_timestamps(self, sentences: List, start_time: Optional[float] = None) -> float:
        """更新句子的时间戳信息
        
        Args:
            sentences: 要更新的句子列表
            start_time: 起始时间（毫秒），如果为 None 则使用第一个句子的开始时间
            
        Returns:
            float: 最后一个句子结束的时间点（毫秒）
        """
        if not sentences:
            return start_time if start_time is not None else 0
            
        # 使用传入的起始时间或第一个句子的开始时间
        current_time = start_time if start_time is not None else sentences[0].start
        
        for sentence in sentences:
            # 计算实际音频长度（毫秒）
            if sentence.generated_audio is not None:
                actual_duration = (len(sentence.generated_audio) / self.sample_rate) * 1000
            else:
                actual_duration = 0
                self.logger.warning(f"句子 {sentence.sentence_id} 没有生成音频")
            
            # 更新时间戳
            sentence.adjusted_start = current_time
            sentence.adjusted_duration = actual_duration
            
            # 更新差异值
            sentence.diff = sentence.duration - actual_duration
            
            # 更新下一个句子的开始时间
            current_time += actual_duration
            
        return current_time
    
    def validate_timestamps(self, sentences: List) -> bool:
        """验证时间戳的连续性和有效性
        
        Args:
            sentences: 要验证的句子列表
            
        Returns:
            bool: 时间戳是否有效
        """
        if not sentences:
            return True
            
        for i in range(len(sentences) - 1):
            current = sentences[i]
            next_sentence = sentences[i + 1]
            
            # 验证时间连续性
            expected_next_start = current.adjusted_start + current.adjusted_duration
            if abs(next_sentence.adjusted_start - expected_next_start) > 1:  # 允许1毫秒的误差
                self.logger.error(
                    f"时间戳不连续 - 句子 {current.sentence_id} 结束时间: {expected_next_start:.2f}ms, "
                    f"句子 {next_sentence.sentence_id} 开始时间: {next_sentence.adjusted_start:.2f}ms"
                )
                return False
                
            # 验证时长有效性
            if current.adjusted_duration <= 0:
                self.logger.error(f"句子 {current.sentence_id} 的时长无效: {current.adjusted_duration:.2f}ms")
                return False
                
        return True

================
File: workers/audio_gen_worker/worker.py
================
import logging
import asyncio
from typing import List, Any
from utils.worker_decorators import redis_worker_decorator
from utils.task_state import TaskState
from .audio_gener import AudioGenerator
from .timestamp_adjuster import TimestampAdjuster
from services.cosyvoice.client import CosyVoiceClient


logger = logging.getLogger(__name__)

class AudioGenWorker:
    """
    音频生成 Worker：利用 AudioGenerator 对句子生成合成音频。
    """

    def __init__(self, config):
        """初始化 AudioGenWorker"""
        self.config = config
        self.logger = logger
        
        # 初始化 CosyVoiceClient
        cosyvoice_address = f"{config.COSYVOICE_SERVICE_HOST}:{config.COSYVOICE_SERVICE_PORT}"
        cosyvoice_client = CosyVoiceClient(address=cosyvoice_address)
        self.audio_generator = AudioGenerator(
            cosyvoice_client=cosyvoice_client,
            sample_rate=config.SAMPLE_RATE
        )
        self.timestamp_adjuster = TimestampAdjuster(config=config)

    @redis_worker_decorator(
        input_queue='audio_gen_queue',
        next_queue='mixing_queue',
        worker_name='音频生成 Worker',
        serialization_mode='msgpack'
    )
    async def run(self, item, task_state: TaskState):
        sentences_batch = item.get('data', item) if isinstance(item, dict) else item
        if not sentences_batch:
            return
        self.logger.debug(f"[音频生成 Worker] 收到 {len(sentences_batch)} 句子, TaskID={task_state.task_id}")

        await self.audio_generator.vocal_audio_maker(sentences_batch)
        task_state.current_time = self.timestamp_adjuster.update_timestamps(sentences_batch, start_time=task_state.current_time)
        valid = self.timestamp_adjuster.validate_timestamps(sentences_batch)
        if not valid:
            self.logger.warning(f"[音频生成 Worker] 检测到时间戳不连续, TaskID={task_state.task_id}")
        return sentences_batch

async def start():
    """启动 Worker"""
    config_module = __import__('config')
    config = config_module.Config()
    worker = AudioGenWorker(config)
    await worker.run()

if __name__ == '__main__':
    asyncio.run(start())

================
File: workers/duration_worker/duration_aligner.py
================
import logging

class DurationAligner:
    def __init__(self, model_in=None, simplifier=None, tts_token_gener=None, max_speed=1.1):
        """
        model_in：生成模型接口，用于更新文本特征  
        simplifier：简化处理接口（Translator）  
        tts_token_gener：TTS token 生成接口  
        max_speed：语速阈值，超过该速率的句子需要进行简化
        """
        self.model_in = model_in
        self.simplifier = simplifier
        self.tts_token_gener = tts_token_gener
        self.max_speed = max_speed
        self.logger = logging.getLogger(__name__)

    async def align_durations(self, sentences):
        """
        对整批句子进行时长对齐，并检查是否需要精简（语速过快）。
        """
        if not sentences:
            return

        # 第一次对齐
        self._align_batch(sentences)

        # 查找语速过快的句子（speed > max_speed）
        retry_sentences = []
        for s in sentences:
            if s.speed > self.max_speed:
                retry_sentences.append(s)
                    
        if retry_sentences:
            self.logger.info(f"{len(retry_sentences)} 个句子语速过快, 正在精简...")
            success = await self._retry_sentences_batch(retry_sentences)
            if success:
                # 若精简文本成功，再次对齐
                self._align_batch(sentences)
            else:
                self.logger.warning("精简过程失败, 保持原结果")

    def _align_batch(self, sentences):
        """
        同批次句子进行时长对齐。
        """
        if not sentences:
            return

        # 计算每句需要调整的时间差
        for s in sentences:
            s.diff = s.duration - s.target_duration

        total_diff_to_adjust = sum(s.diff for s in sentences)
        positive_diff_sum = sum(x.diff for x in sentences if x.diff > 0)
        negative_diff_sum_abs = sum(abs(x.diff) for x in sentences if x.diff < 0)
        current_time = sentences[0].start

        for s in sentences:
            s.adjusted_start = current_time
            diff = s.diff
            s.speed = 1.0
            s.silence_duration = 0.0
            s.adjusted_duration = s.duration

            if total_diff_to_adjust != 0:
                if total_diff_to_adjust > 0 and diff > 0:
                    if positive_diff_sum > 0:
                        proportion = diff / positive_diff_sum
                        adjustment = total_diff_to_adjust * proportion
                        s.adjusted_duration = s.duration - adjustment
                        s.speed = s.duration / max(s.adjusted_duration, 0.001)
                elif total_diff_to_adjust < 0 and diff < 0:
                    if negative_diff_sum_abs > 0:
                        proportion = abs(diff) / negative_diff_sum_abs
                        total_needed = abs(total_diff_to_adjust) * proportion
                        max_slowdown = s.duration * 0.07
                        slowdown = min(total_needed, max_slowdown)
                        s.adjusted_duration = s.duration + slowdown
                        s.speed = s.duration / max(s.adjusted_duration, 0.001)
                        s.silence_duration = total_needed - slowdown
                        if s.silence_duration > 0:
                            s.adjusted_duration += s.silence_duration

            s.diff = s.duration - s.adjusted_duration
            current_time += s.adjusted_duration

            self.logger.info(
                f"对齐后: {s.trans_text}, duration: {s.duration}, "
                f"target_duration: {s.target_duration}, diff: {s.diff}, "
                f"speed: {s.speed}, silence_duration: {s.silence_duration}"
            )

    async def _retry_sentences_batch(self, sentences):
        """
        对语速过快的句子执行精简 + 更新 TTS token。
        """
        try:
            # 1. 分批对语速过快的句子进行精简
            async for _ in self.simplifier.simplify_sentences(sentences, target_speed=self.max_speed):
                pass
            # 2. 批量更新文本特征（复用 speaker 与 uuid）
            async for batch in self.model_in.modelin_maker(
                sentences,
                reuse_speaker=True,
                batch_size=3
            ):
                # 3. 再生成 token（复用 uuid）
                updated_batch = await self.tts_token_gener.tts_token_maker(batch)
            return True
        except Exception as e:
            self.logger.error(f"_retry_sentences_batch 出错: {e}")
            return False

================
File: workers/duration_worker/worker.py
================
import logging
import asyncio
from typing import List, Any
from utils.worker_decorators import redis_worker_decorator
from utils.task_state import TaskState
from .duration_aligner import DurationAligner
from workers.modelin_worker.model_in import ModelIn
from workers.tts_worker.tts_token_gener import TTSTokenGenerator
from workers.translation_worker.translation.translator import Translator
from workers.translation_worker.translation.deepseek_client import DeepSeekClient
from workers.translation_worker.translation.gemini_client import GeminiClient
from services.cosyvoice.client import CosyVoiceClient

logger = logging.getLogger(__name__)

class DurationWorker:
    """
    时长对齐 Worker：调用 DurationAligner 对句子进行时长调整。
    """

    def __init__(self, config):
        """初始化 DurationWorker"""
        self.config = config
        self.logger = logger
        
        # 初始化 CosyVoiceClient
        cosyvoice_address = f"{config.COSYVOICE_SERVICE_HOST}:{config.COSYVOICE_SERVICE_PORT}"
        cosyvoice_client = CosyVoiceClient(address=cosyvoice_address)
        
        # 初始化依赖组件
        model_in = ModelIn(cosyvoice_client=cosyvoice_client, max_concurrent_tasks=config.MAX_PARALLEL_SEGMENTS)
        tts_token_generator = TTSTokenGenerator(cosyvoice_client=cosyvoice_client)
        
        # 初始化翻译客户端
        translation_model = config.TRANSLATION_MODEL.lower()
        if translation_model == "deepseek":
            client = DeepSeekClient(api_key=config.DEEPSEEK_API_KEY)
        elif translation_model == "gemini":
            client = GeminiClient(api_key=config.GEMINI_API_KEY)
        else:
            raise ValueError(f"不支持的翻译模型: {translation_model}")
        simplifier = Translator(translation_client=client)
        
        # 直接初始化 DurationAligner
        self.duration_aligner = DurationAligner(
            model_in=model_in,
            simplifier=simplifier,
            tts_token_gener=tts_token_generator,
            max_speed=1.2
        )

    @redis_worker_decorator(
        input_queue='duration_align_queue',
        next_queue='audio_gen_queue',
        worker_name='时长对齐 Worker',
        serialization_mode='msgpack'
    )
    async def run(self, item, task_state: TaskState):
        sentences_batch = item.get('data', item) if isinstance(item, dict) else item
        if not sentences_batch:
            return
        self.logger.debug(f"[时长对齐 Worker] 收到 {len(sentences_batch)} 句子, TaskID={task_state.task_id}")

        # 对齐时长
        await self.duration_aligner.align_durations(sentences_batch)
        return sentences_batch

async def start():
    """启动 Worker"""
    config_module = __import__('config')
    config = config_module.Config()
    worker = DurationWorker(config)
    await worker.run()

if __name__ == '__main__':
    asyncio.run(start())

================
File: workers/mixer_worker/media_mixer.py
================
# ---------------------------------------------------
# backend/workers/mixer_worker/media_mixer.py
# ---------------------------------------------------
import numpy as np
import logging
import soundfile as sf
import os
import asyncio
from contextlib import ExitStack
from tempfile import NamedTemporaryFile
from typing import List, Any

import pysubs2

from utils.worker_decorators import handle_errors
from utils.ffmpeg_utils import FFmpegTool
from config import Config
from utils.task_state import TaskState

logger = logging.getLogger(__name__)

class MediaMixer:
    """
    用于将多段句子的合成音频与原视频片段混合，并可生成带字幕的视频。
    支持:
      - 音频淡入淡出
      - 背景音乐混合
      - 基于 pysubs2 生成 .ass 字幕("YouTube风格")
      - 按语言自动决定单行最大长度
    """
    def __init__(self, config: Config):  # 移除 sample_rate 参数，从 config 获取
        self.config = config
        self.sample_rate = config.SAMPLE_RATE  # 从 config 获取全局采样率

        # 音量相关
        self.max_val = 1.0
        self.overlap = self.config.AUDIO_OVERLAP
        self.vocals_volume = self.config.VOCALS_VOLUME
        self.background_volume = self.config.BACKGROUND_VOLUME

        # 全局缓存, 可按需使用
        self.full_audio_buffer = np.array([], dtype=np.float32)

        self.ffmpeg_tool = FFmpegTool()

    @handle_errors(logger)
    async def mixed_media_maker(
        self,
        sentences: List[Any],
        task_state: TaskState,
        output_path: str,
        generate_subtitle: bool = False
    ) -> bool:
        """
        主入口: 处理一批句子的音频与视频，输出一段带音频的 MP4。
        根据 generate_subtitle 决定是否烧制字幕。

        Args:
            sentences: 本片段内的所有句子对象
            task_state: 任务状态，内部包含 target_language 等
            output_path: 生成的 MP4 文件路径
            generate_subtitle: 是否在最终视频里烧制字幕

        Returns:
            True / False 表示成功或失败
        """
        if not sentences:
            logger.warning("mixed_media_maker: 收到空的句子列表")
            return False

        segment_index = sentences[0].segment_index
        if segment_index is None:
            logger.error("找不到分段索引")
            return False

        segment_files = task_state.segment_media_files.get(str(segment_index))
        if not segment_files:
            logger.error(f"找不到分段 {segment_index} 对应的媒体文件信息")
            return False

        # =========== (1) 拼接所有句子的合成音频 =============
        full_audio = np.array([], dtype=np.float32)
        for sentence in sentences:
            audio_data = np.asarray(sentence.generated_audio, dtype=np.float32)
            # 如果已经有前面累积的音频，做淡入淡出衔接
            if len(full_audio) > 0:
                audio_data = self._apply_fade_effect(audio_data)
            full_audio = np.concatenate((full_audio, audio_data))
            if audio_data is None:
                logger.warning(
                    "句子音频生成失败: text=%r, UUID=%s",
                    sentence.raw_text or sentence.trans_text,
                    sentence.model_input.get("uuid", "unknown")
                )

        if len(full_audio) == 0:
            logger.error("mixed_media_maker: 没有有效的合成音频数据")
            return False

        # 计算当前片段的起始时间和时长(秒)
        start_time = 0.0
        if sentences[0]:
            if not sentences[0].is_first:
                start_time = (sentences[0].adjusted_start - sentences[0].segment_start * 1000) / 1000.0

        # 计算总时长
        duration = 0.0
        for s in sentences:
            duration += s.adjusted_duration
        duration /= 1000.0  # 转换为秒

        # =========== (2) 背景音乐混合 (可选) =============
        background_audio_path = segment_files['background']
        if background_audio_path is not None:
            full_audio = self._mix_with_background(
                bg_path=background_audio_path,
                start_time=start_time,
                duration=duration,
                audio_data=full_audio
            )
            full_audio = self._normalize_audio(full_audio)

        # (可按需储存到全局 mixer 缓存)
        self.full_audio_buffer = np.concatenate((self.full_audio_buffer, full_audio))

        # =========== (3) 如果有视频，就把音频合并到视频里 ============
        video_path = segment_files['video']
        if video_path:
            await self._add_video_segment(
                video_path=video_path,
                start_time=start_time,
                duration=duration,
                audio_data=full_audio,
                output_path=output_path,
                sentences=sentences,
                generate_subtitle=generate_subtitle,
                task_state=task_state  # 传入以获取 target_language
            )
            return True

        logger.warning("mixed_media_maker: 本片段无video_path可用")
        return False

    def _apply_fade_effect(self, audio_data: np.ndarray) -> np.ndarray:
        """在语音片段衔接处做 overlap 长度的淡入淡出衔接。"""
        if audio_data is None or len(audio_data) == 0:
            return np.array([], dtype=np.float32)

        cross_len = min(self.overlap, len(self.full_audio_buffer), len(audio_data))
        if cross_len <= 0:
            return audio_data

        fade_out = np.sqrt(np.linspace(1.0, 0.0, cross_len, dtype=np.float32))
        fade_in  = np.sqrt(np.linspace(0.0, 1.0, cross_len, dtype=np.float32))

        audio_data = audio_data.copy()
        overlap_region = self.full_audio_buffer[-cross_len:]

        audio_data[:cross_len] = overlap_region * fade_out + audio_data[:cross_len] * fade_in
        return audio_data

    def _mix_with_background(
        self,
        bg_path: str,
        start_time: float,
        duration: float,
        audio_data: np.ndarray
    ) -> np.ndarray:
        """
        从 bg_path 读取背景音乐，在 [start_time, start_time+duration] 区间截取，
        与 audio_data (人声) 混合。
        """
        background_audio, sr = sf.read(bg_path)
        background_audio = np.asarray(background_audio, dtype=np.float32)
        if sr != self.sample_rate:
            logger.warning(
                f"背景音采样率={sr} 与目标={self.sample_rate}不匹配, 未做重采样, 可能有问题."
            )

        target_length = int(duration * self.sample_rate)
        start_sample = int(start_time * self.sample_rate)
        end_sample   = start_sample + target_length

        if end_sample <= len(background_audio):
            bg_segment = background_audio[start_sample:end_sample]
        else:
            bg_segment = background_audio[start_sample:]

        result = np.zeros(target_length, dtype=np.float32)
        audio_len = min(len(audio_data), target_length)
        bg_len    = min(len(bg_segment), target_length)

        # 混合人声 & 背景
        if audio_len > 0:
            result[:audio_len] = audio_data[:audio_len] * self.vocals_volume
        if bg_len > 0:
            result[:bg_len] += bg_segment[:bg_len] * self.background_volume

        return result

    def _normalize_audio(self, audio_data: np.ndarray) -> np.ndarray:
        """对音频做简单归一化"""
        if len(audio_data) == 0:
            return audio_data
        max_val = np.max(np.abs(audio_data))
        if max_val > self.max_val:
            audio_data = audio_data * (self.max_val / max_val)
        return audio_data

    @handle_errors(logger)
    async def _add_video_segment(
        self,
        video_path: str,
        start_time: float,
        duration: float,
        audio_data: np.ndarray,
        output_path: str,
        sentences: List[Any],
        generate_subtitle: bool,
        task_state: TaskState
    ):
        """
        从原视频里截取 [start_time, start_time + duration] 的视频段(无声)，
        与合成音频合并。
        若 generate_subtitle=True, 则生成 .ass 字幕并在 ffmpeg 工具中进行"烧制"。
        """
        if not os.path.exists(video_path):
            raise FileNotFoundError(f"_add_video_segment: 视频文件不存在: {video_path}")
        if len(audio_data) == 0:
            raise ValueError("_add_video_segment: 无音频数据")
        if duration <= 0:
            raise ValueError("_add_video_segment: 无效时长 <=0")

        with ExitStack() as stack:
            temp_video = stack.enter_context(NamedTemporaryFile(suffix='.mp4'))
            temp_audio = stack.enter_context(NamedTemporaryFile(suffix='.wav'))

            end_time = start_time + duration

            # 1) 截取视频 (无音轨)
            await self.ffmpeg_tool.cut_video_track(
                input_path=video_path,
                output_path=temp_video.name,
                start=start_time,
                end=end_time
            )

            # 2) 写合成音频到临时文件
            await asyncio.to_thread(sf.write, temp_audio.name, audio_data, self.sample_rate)

            # 3) 如果需要字幕，则构建 .ass 并用 ffmpeg "烧"进去
            if generate_subtitle:
                temp_ass = stack.enter_context(NamedTemporaryFile(suffix='.ass'))
                await asyncio.to_thread(
                    self._generate_subtitles_for_segment,
                    sentences,
                    start_time * 1000,   # segment_start_ms
                    temp_ass.name,
                    task_state.target_language
                )

                # 生成带字幕的视频
                await self.ffmpeg_tool.cut_video_with_subtitles_and_audio(
                    input_video_path=temp_video.name,
                    input_audio_path=temp_audio.name,
                    subtitles_path=temp_ass.name,
                    output_path=output_path
                )
            else:
                # 不加字幕，仅合并音频
                await self.ffmpeg_tool.cut_video_with_audio(
                    input_video_path=temp_video.name,
                    input_audio_path=temp_audio.name,
                    output_path=output_path
                )

    def _generate_subtitles_for_segment(
        self,
        sentences: List[Any],
        segment_start_ms: float,
        output_sub_path: str,
        target_language: str = "en"
    ):
        """生成 ASS 字幕文件"""
        subs = pysubs2.SSAFile()

        for s in sentences:
            # 计算相对时间
            start_local = s.adjusted_start - segment_start_ms - s.segment_start * 1000
            sub_text = (s.trans_text or s.raw_text or "").strip()
            duration_ms = s.adjusted_duration
            lang = target_language

            if not sub_text:
                continue

            # 直接使用adjusted_duration作为duration_ms
            if duration_ms <= 0:
                continue

            # 拆分长句子 -> 多段 sequential
            blocks = self._split_long_text_to_sub_blocks(
                text=sub_text,
                start_ms=start_local,
                duration_ms=duration_ms,
                lang=lang
            )

            for block in blocks:
                evt = pysubs2.SSAEvent(
                    start=int(block["start"]),
                    end=int(block["end"]),
                    text=block["text"]
                )
                subs.append(evt)

        # 设置"类YouTube"的默认样式
        style = subs.styles.get("Default", pysubs2.SSAStyle())

        style.fontname = "Arial"             # 常见无衬线
        style.fontsize = 22
        style.bold = True
        style.italic = False
        style.underline = False

        # 颜色 (R, G, B, A=0 => 不透明)
        style.primarycolor = pysubs2.Color(255, 255, 255, 0)
        style.outlinecolor = pysubs2.Color(0, 0, 0, 100)  # 半透明黑
        style.borderstyle = 3  # 3 => 有背景块
        style.shadow = 0
        style.alignment = pysubs2.Alignment.BOTTOM_CENTER
        style.marginv = 20    # 离底部像素

        # 更新回 default
        subs.styles["Default"] = style

        # 写入文件
        subs.save(output_sub_path, format="ass")
        logger.debug(f"_generate_subtitles_for_segment: 已写入字幕 => {output_sub_path}")

    def _split_long_text_to_sub_blocks(
        self,
        text: str,
        start_ms: float,
        duration_ms: float,
        lang: str = "en"
    ) -> List[dict]:
        """将文本拆分成多块字幕"""
        recommended_max_chars = {
            "zh": 20,
            "ja": 20,
            "ko": 20,
            "en": 40
        }
        if lang not in recommended_max_chars:
            lang = "en"
        max_chars = recommended_max_chars[lang]

        if len(text) <= max_chars:
            return [{
                "start": start_ms,
                "end":   start_ms + duration_ms,
                "text":  text
            }]

        chunks = self._chunk_text_by_language(text, lang, max_chars)

        sub_blocks = []
        total_chars = sum(len(c) for c in chunks)
        current_start = start_ms

        for c in chunks:
            chunk_len = len(c)
            chunk_dur = duration_ms * (chunk_len / total_chars) if total_chars > 0 else 0
            block_start = current_start
            block_end   = current_start + chunk_dur

            sub_blocks.append({
                "start": block_start,
                "end":   block_end,
                "text":  c
            })
            current_start += chunk_dur

        if sub_blocks:
            sub_blocks[-1]["end"] = start_ms + duration_ms
        else:
            sub_blocks.append({
                "start": start_ms,
                "end":   start_ms + duration_ms,
                "text":  text
            })

        return sub_blocks

    def _chunk_text_by_language(self, text: str, lang: str, max_chars: int) -> List[str]:
        """根据语言拆分文本"""
        cjk_puncts = set("，,。.!！？?；;：:、…~— ")
        eng_puncts = set(".,!?;: ")

        if lang == "en":
            return self._chunk_english_text(text, max_chars, eng_puncts)
        else:
            return self._chunk_cjk_text(text, max_chars, cjk_puncts)

    def _chunk_english_text(self, text: str, max_chars: int, puncts: set) -> List[str]:
        """英文文本拆分"""
        words = text.split()
        chunks = []
        current_line = []

        for w in words:
            line_len = sum(len(x) for x in current_line) + len(current_line)
            if line_len + len(w) > max_chars:
                if current_line:
                    chunks.append(" ".join(current_line))
                    current_line = []
            current_line.append(w)

        if current_line:
            chunks.append(" ".join(current_line))

        return chunks

    def _chunk_cjk_text(self, text: str, max_chars: int, puncts: set) -> List[str]:
        """中日韩文本拆分"""
        chunks = []
        total_length = len(text)
        start_idx = 0

        while start_idx < total_length:
            end_idx = start_idx + max_chars
            
            if end_idx < total_length and text[end_idx] in puncts:
                end_idx += 1

            end_idx = min(end_idx, total_length)
            chunk = text[start_idx:end_idx]
            chunks.append(chunk)
            start_idx = end_idx

        return chunks

    async def reset(self):
        """重置 mixer 状态"""
        self.full_audio_buffer = np.array([], dtype=np.float32)
        logger.debug("MediaMixer 已重置 full_audio_buffer")

================
File: workers/mixer_worker/worker.py
================
import asyncio
import json
from typing import List, Any
from pathlib import Path
from utils.worker_decorators import redis_worker_decorator
from utils.task_state import TaskState
from utils.redis_utils import load_task_state
from utils.log_config import get_logger
from .media_mixer import MediaMixer
from services.hls import HLSClient
import aioredis

logger = get_logger(__name__)

class MixerWorker:
    """
    混音 Worker：调用 MediaMixer 将生成的音频与视频混合，生成最终输出段视频。
    """

    def __init__(self, config):
        """初始化 MixerWorker"""
        self.config = config
        self.logger = logger
        self.hls_service = None  # 异步初始化
        
        # 直接实例化 MediaMixer
        self.mixer = MediaMixer(config=config)

    async def initialize(self):
        """异步初始化 HLS 服务"""
        self.hls_service = await HLSClient.create(self.config)
        return self

    @redis_worker_decorator(
        input_queue='mixing_queue',
        worker_name='混音 Worker',
        serialization_mode='msgpack'
    )
    async def run(self, item, task_state: TaskState):
        sentences_batch = item.get('data', item) if isinstance(item, dict) else item
        if not sentences_batch:
            return
        self.logger.debug(f"[混音 Worker] 收到 {len(sentences_batch)} 句子, TaskID={task_state.task_id}")

        # 确保 HLS 服务已初始化
        if not self.hls_service:
            await self.initialize()

        # 处理输出路径
        output_path = task_state.task_paths.segments_dir / f"segment_{task_state.batch_counter}.mp4"

        # 混音处理
        success = await self.mixer.mixed_media_maker(
            sentences=sentences_batch,
            task_state=task_state,
            output_path=str(output_path),
            generate_subtitle=task_state.generate_subtitle
        )
        
        if success:
            # 使用 HLS 服务
            added = await self.hls_service.add_segment(
                task_state.task_id,
                output_path,
                task_state.batch_counter
            )
            if added:
                self.logger.info(f"[混音 Worker] 分段 {task_state.batch_counter} 已加入 HLS, TaskID={task_state.task_id}")
                task_state.merged_segments.append(str(output_path))
                
                # 检查是否所有分段都已处理完成
                if task_state.all_segments_processed():
                    await self._notify_task_completion(task_state)
            else:
                self.logger.error(f"[混音 Worker] 分段 {task_state.batch_counter} 添加到 HLS 流失败, TaskID={task_state.task_id}")

        task_state.batch_counter += 1
        return None
        
    async def _notify_task_completion(self, task_state: TaskState):
        """通知任务完成"""
        redis = await aioredis.create_redis_pool('redis://localhost')
        try:
            # 获取最终视频路径
            final_video_path = await self.hls_service.finalize_task(task_state.task_id)
            
            # 发送完成信号
            completion_key = f"task_completion:{task_state.task_id}"
            completion_data = {
                "status": "success",
                "final_video_path": str(final_video_path) if final_video_path else ""
            }
            await redis.rpush(completion_key, json.dumps(completion_data))
            
            self.logger.info(f"[混音 Worker] 任务 {task_state.task_id} 已完成，发送完成信号")
        except Exception as e:
            self.logger.error(f"[混音 Worker] 发送完成信号失败: {e}")
        finally:
            redis.close()
            await redis.wait_closed()

async def start():
    """启动 Worker"""
    config_module = __import__('config')
    config = config_module.Config()
    worker = await MixerWorker(config).initialize()
    await worker.run()

if __name__ == '__main__':
    asyncio.run(start())

================
File: workers/modelin_worker/model_in.py
================
import logging
import asyncio
import torch
import numpy as np
import librosa
from typing import List, Dict
import os
import uuid
import soundfile as sf

from services.cosyvoice.client import CosyVoiceClient

class ModelIn:
    def __init__(self, cosyvoice_client: CosyVoiceClient, max_concurrent_tasks: int = 4):
        self.cosyvoice_client = cosyvoice_client
        self.logger = logging.getLogger(__name__)
        self.semaphore = asyncio.Semaphore(max_concurrent_tasks)
        self.speaker_cache: Dict[str, asyncio.Task] = {}  # speaker_id -> asyncio.Task
        self.max_val = 0.8
        self.cosy_sample_rate = 24000

    def postprocess(self, speech, sr, top_db=60, hop_length=220, win_length=440):
        """
        对音频进行 trim、幅度归一化，并在结尾加 0.2s 静音
        """
        speech, _ = librosa.effects.trim(
            speech,
            top_db=top_db,
            frame_length=win_length,
            hop_length=hop_length
        )
        if np.abs(speech).max() > self.max_val:
            speech = speech / np.abs(speech).max() * self.max_val

        # 使用 sr 采样率计算静音尾音的长度
        # 因为我们期望输入的音频采样率为 sr
        pad_samples = int(sr * 0.2)  # 0.2秒的静音
        speech = np.concatenate([speech, np.zeros(pad_samples, dtype=speech.dtype)])
        return speech

    async def get_speaker_features(self, speaker_id: str, sentence) -> str:
        """
        异步提取说话人特征，返回说话人UUID
        """
        if speaker_id in self.speaker_cache:
            return await self.speaker_cache[speaker_id]

        async def extract_features() -> str:
            # 从文件加载音频
            if not hasattr(sentence, 'audio_path') or not sentence.audio_path:
                raise ValueError(f"缺少音频路径 (audio_path)，无法提取说话人特征: speaker_id={speaker_id}")
            audio_path = sentence.audio_path
                
            # 从文件路径加载音频
            audio_np, sr = sf.read(audio_path)
            
            # 处理音频，得到numpy array
            postprocessed_audio = self.postprocess(audio_np, sr)

            speaker_uuid = str(uuid.uuid4())
            success = await asyncio.to_thread(
                self.cosyvoice_client.extract_speaker_features,
                speaker_uuid,
                postprocessed_audio,  # 直接传递numpy array
                16000  # 使用 16000Hz 作为采样率参数
            )
            if not success:
                raise Exception("提取说话人特征失败")
            return speaker_uuid

        task = asyncio.create_task(extract_features())
        self.speaker_cache[speaker_id] = task
        return await task

    async def _process_one_sentence_async(self, sentence, reuse_speaker: bool):
        async with self.semaphore:
            # 获取 speaker_id
            speaker_id = getattr(sentence, 'speaker_id', None)
            
            # 处理文本UUID
            trans_text = sentence.trans_text or ""
                
            text_uuid = await asyncio.to_thread(
                self.cosyvoice_client.normalize_text,
                trans_text
            )
            
            # 更新 model_input
            sentence.model_input['text_uuid'] = text_uuid
            
            # 处理说话人UUID
            if speaker_id is not None and not reuse_speaker:
                speaker_uuid = await self.get_speaker_features(speaker_id, sentence)
                sentence.model_input['speaker_uuid'] = speaker_uuid
            elif 'speaker_uuid' not in sentence.model_input:
                raise ValueError("缺少说话人UUID且未提取新特征")

            return sentence

    async def modelin_maker(self, sentences: List, reuse_speaker: bool = False, batch_size: int = 3):
        """
        对一批 sentence 进行文本与说话人特征提取，按 batch_size 分批 yield 结果
        """
        if not sentences:
            self.logger.warning("modelin_maker: 收到空句子列表")
            return

        tasks = [
            asyncio.create_task(self._process_one_sentence_async(s, reuse_speaker))
            for s in sentences
        ]

        results_batch = []
        try:
            for i, task in enumerate(tasks, start=1):
                updated = await task
                if updated is not None:
                    results_batch.append(updated)
                if i % batch_size == 0:
                    yield results_batch
                    results_batch = []
            if results_batch:
                yield results_batch
        except Exception as e:
            self.logger.error(f"modelin_maker处理失败: {e}")
            raise
        finally:
            if not reuse_speaker:
                self.speaker_cache.clear()

================
File: workers/modelin_worker/worker.py
================
import logging
import asyncio
from typing import List, Any
from utils.worker_decorators import redis_worker_decorator
from utils.task_state import TaskState
from .model_in import ModelIn
from services.cosyvoice.client import CosyVoiceClient

logger = logging.getLogger(__name__)

class ModelInWorker:
    """
    模型输入 Worker：调用 ModelIn 对句子进行 speaker 特征更新、文本处理等。
    """

    def __init__(self, config):
        """初始化 ModelInWorker"""
        self.config = config
        self.logger = logger
        
        # 初始化 CosyVoiceClient
        cosyvoice_address = f"{config.COSYVOICE_SERVICE_HOST}:{config.COSYVOICE_SERVICE_PORT}"
        cosyvoice_client = CosyVoiceClient(address=cosyvoice_address)
        self.model_in = ModelIn(cosyvoice_client=cosyvoice_client)

    @redis_worker_decorator(
        input_queue='modelin_queue',
        next_queue='tts_token_queue',
        worker_name='模型输入 Worker',
        mode='stream',
        serialization_mode='msgpack'
    )
    async def run(self, item, task_state: TaskState):
        sentences_batch = item.get('data', item) if isinstance(item, dict) else item
        if not sentences_batch:
            return
        self.logger.debug(f"[模型输入 Worker] 收到 {len(sentences_batch)} 句子, TaskID={task_state.task_id}")

        async for processed_batch in self.model_in.modelin_maker(
            sentences_batch,
            reuse_speaker=False,
            batch_size=self.config.MODELIN_BATCH_SIZE
        ):
            yield processed_batch

async def start():
    """启动 Worker"""
    config_module = __import__('config')
    config = config_module.Config()
    worker = ModelInWorker(config)
    await worker.run()

if __name__ == '__main__':
    asyncio.run(start())

================
File: workers/segment_worker/audio_separator.py
================
from abc import ABC, abstractmethod
from typing import Tuple
import numpy as np

from models.ClearerVoice.clearvoice import ClearVoice

class AudioSeparator(ABC):
    """音频分离器接口"""
    @abstractmethod
    def separate_audio(self, input_path: str, **kwargs) -> Tuple[np.ndarray, np.ndarray]:
        pass

class ClearVoiceSeparator(AudioSeparator):
    """使用 ClearVoice 实现的音频分离器"""
    def __init__(self, model_name: str = 'MossFormer2_SE_48K'):
        self.model_name = model_name
        self.clearvoice = ClearVoice(
            task='speech_enhancement',
            model_names=[model_name]
        )
    
    def separate_audio(self, input_path: str) -> Tuple[np.ndarray, np.ndarray, int]:
        enhanced_audio, background_audio = self.clearvoice(
            input_path=input_path,
            online_write=False,
            extract_noise=True
        )
        
        if self.model_name.endswith('16K'):
            sr = 16000
        elif self.model_name.endswith('48K'):
            sr = 48000
        else:
            sr = 48000
        
        return enhanced_audio, background_audio, sr
    
    async def separate(self, input_path: str, vocals_output: str, background_output: str) -> None:
        """
        分离音频，并将人声和背景音保存到指定路径
        
        Args:
            input_path: 输入音频路径
            vocals_output: 人声输出路径
            background_output: 背景音输出路径
        """
        import soundfile as sf
        
        # 分离音频
        enhanced_audio, background_audio, sr = self.separate_audio(input_path)
        
        # 保存音频
        sf.write(vocals_output, enhanced_audio, sr)
        sf.write(background_output, background_audio, sr)

================
File: workers/segment_worker/video_segmenter.py
================
import logging
from typing import List, Tuple
from utils.ffmpeg_utils import FFmpegTool

logger = logging.getLogger(__name__)

class VideoSegmenter:
    """视频分段计算器，负责计算视频应该如何分段"""
    
    def __init__(self, config, ffmpeg_tool: FFmpegTool):
        self.config = config
        self.ffmpeg_tool = ffmpeg_tool
        self.logger = logger
        
    async def get_video_duration(self, video_path: str) -> float:
        """获取视频时长"""
        return await self.ffmpeg_tool.get_duration(video_path)
        
    async def get_audio_segments(self, duration: float) -> List[Tuple[float, float]]:
        """
        按配置分割时间片：
        segment_length: 每段的理想长度
        min_length: 最小允许的分段时长，若最后一个分段不足该时长则与前一段合并
        """
        segment_length = self.config.SEGMENT_MINUTES * 60
        min_length = self.config.MIN_SEGMENT_MINUTES * 60

        if duration <= min_length:
            return [(0, duration)]

        segments = []
        current_pos = 0.0

        while current_pos < duration:
            remaining_duration = duration - current_pos
            
            if remaining_duration <= segment_length:
                if remaining_duration < min_length and segments:
                    # 合并到上一段
                    start = segments[-1][0]
                    new_duration = duration - start
                    segments[-1] = (start, new_duration)
                else:
                    segments.append((current_pos, remaining_duration))
                break

            segments.append((current_pos, segment_length))
            current_pos += segment_length

        return segments

================
File: workers/segment_worker/worker.py
================
import asyncio
import numpy as np
import torch
import torchaudio
import soundfile as sf
from pathlib import Path
import json
import aioredis
from typing import Dict, Any, Optional
from utils.ffmpeg_utils import FFmpegTool
from utils.task_state import TaskState
from utils.worker_decorators import redis_worker_decorator
from utils import concurrency
from utils.log_config import get_logger
from .audio_separator import ClearVoiceSeparator
from .video_segmenter import VideoSegmenter

logger = get_logger(__name__)

class SegmentWorker:
    """
    视频分段处理 Worker：负责分段初始化以及媒体提取（音频、视频和后续音频分离）。
    """

    def __init__(self, config):
        """初始化 SegmentWorker 及其依赖"""
        self.config = config
        self.target_sr = 24000  # 从config获取
        
        # 初始化依赖
        self.audio_separator = ClearVoiceSeparator(model_name='MossFormer2_SE_48K')
        self.ffmpeg_tool = FFmpegTool()
        self.video_segmenter = VideoSegmenter(config=config, ffmpeg_tool=self.ffmpeg_tool)
        self.logger = logger

    @redis_worker_decorator(
        input_queue='segment_init_queue',
        next_queue='segment_queue',
        worker_name='分段初始化 Worker',
        mode='stream'
    )
    async def run_init(self, item, task_state: TaskState):
        """处理视频分段初始化任务"""
        try:
            video_path = item['video_path']
            duration = await self.video_segmenter.get_video_duration(video_path)
            if duration <= 0:
                raise ValueError(f"无效的视频时长: {duration}s")
            segments = await self.video_segmenter.get_audio_segments(duration)
            if not segments:
                raise ValueError("无法获取有效分段")
                
            # 更新总分段数
            task_state.total_segments = len(segments)
            
            self.logger.info(
                f"[分段初始化 Worker] 视频总长={duration:.2f}s, 分段数={len(segments)}, TaskID={task_state.task_id}"
            )
            for i, (start, seg_duration) in enumerate(segments):
                yield {
                    'index': i,
                    'start': start,
                    'duration': seg_duration,
                    'video_path': video_path  # 传递视频路径给下游
                }
        except Exception as e:
            self.logger.error(f"[分段初始化 Worker] 处理失败: {e} -> TaskID={task_state.task_id}", exc_info=True)
            task_state.errors.append({
                'stage': 'segment_initialization',
                'error': str(e)
            })

    @redis_worker_decorator(
        input_queue='segment_queue',
        next_queue='asr_queue',
        worker_name='分段提取 Worker'
    )
    async def run_extract(self, item, task_state: TaskState) -> Dict[str, Any]:
        """
        处理单个视频分段，执行：
          1. 并发提取音频与视频；
          2. 分离人声和背景音；
          3. 重采样与写文件；
          4. 清理临时文件，并返回提取信息。
        """
        try:
            data = item.get('data', item) if isinstance(item, dict) else item
            index = data['index']
            start = data['start']
            duration = data['duration']
            video_path = data['video_path']  # 从队列消息中获取视频路径

            self.logger.info(
                f"[分段提取 Worker] 开始处理分段 {index}, start={start:.2f}s, duration={duration:.2f}s -> TaskID={task_state.task_id}"
            )
            silent_video = str(task_state.task_paths.processing_dir / f"video_silent_{index}.mp4")
            full_audio = str(task_state.task_paths.processing_dir / f"audio_full_{index}.wav")
            vocals_audio = str(task_state.task_paths.processing_dir / f"vocals_{index}.wav")
            background_audio = str(task_state.task_paths.processing_dir / f"background_{index}.wav")

            # 并发提取音频与视频
            await asyncio.gather(
                self.ffmpeg_tool.extract_audio(video_path, full_audio, start, duration),
                self.ffmpeg_tool.extract_video(video_path, silent_video, start, duration)
            )

            # 分离人声和背景音
            await self.audio_separator.separate(
                input_path=full_audio,
                vocals_output=vocals_audio,
                background_output=background_audio
            )

            # 保存分段媒体文件信息
            segment_info = {
                'segment_index': index,
                'start': start,
                'duration': duration,
                'silent_video_path': silent_video,
                'full_audio_path': full_audio,
                'vocals_path': vocals_audio,
                'background_path': background_audio
            }
            task_state.segment_media_files[str(index)] = segment_info

            self.logger.info(
                f"[分段提取 Worker] 分段 {index} 处理完成 -> TaskID={task_state.task_id}"
            )
            return segment_info

        except Exception as e:
            self.logger.error(
                f"[分段提取 Worker] 分段 {index if 'index' in locals() else '?'} 处理失败: {e} -> TaskID={task_state.task_id}",
                exc_info=True
            )
            task_state.errors.append({
                'segment_index': index if 'index' in locals() else None,
                'stage': 'segment_extraction',
                'error': str(e)
            })
            return None

async def start():
    """启动 Worker"""
    config_module = __import__('config')
    config = config_module.Config()
    worker = SegmentWorker(config)
    
    # 启动两个并行任务
    await asyncio.gather(
        worker.run_init(),
        worker.run_extract()
    )

if __name__ == '__main__':
    asyncio.run(start())

================
File: workers/translation_worker/translation/__init__.py
================
# 空文件，用于标识 translation 目录为一个 Python 包

================
File: workers/translation_worker/translation/deepseek_client.py
================
# =========================== deepseek_client.py ===========================
import json
import logging
from openai import OpenAI
from typing import Dict
from json_repair import loads

# [MODIFIED] 引入统一线程管理
from utils import concurrency

logger = logging.getLogger(__name__)

class DeepSeekClient:
    def __init__(self, api_key: str):
        """初始化 DeepSeek 客户端"""
        if not api_key:
            raise ValueError("DeepSeek API key must be provided")
            
        self.client = OpenAI(
            api_key=api_key,
            base_url="https://api.deepseek.com"
        )
        logger.info("DeepSeek 客户端初始化成功")

    async def translate(
        self,
        system_prompt: str,
        user_prompt: str
    ) -> Dict[str, str]:
        """
        直接调用 DeepSeek API，要求返回 JSON 格式的内容。
        """
        try:
            response = await concurrency.run_sync(
                self.client.chat.completions.create,
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=1.3
            )
            result = response.choices[0].message.content
            logger.info(f"DeepSeek 原文请求内容:\n{user_prompt}")
            logger.info(f"DeepSeek 原始返回内容 (长度: {len(result)}):\n{result!r}")
            
            if not result or not result.strip():
                logger.error("DeepSeek 返回了空响应")
                raise ValueError("Empty response from DeepSeek")
                
            # 尝试修复和解析 JSON
            try:
                parsed_result = loads(result)
                logger.debug("DeepSeek 请求成功，JSON 解析完成")
                return parsed_result
            except Exception as json_error:
                logger.error(f"JSON 解析失败，原始内容: {result!r}")
                logger.error(f"JSON 解析错误详情: {str(json_error)}")
                raise
            
        except Exception as e:
            logger.error(f"DeepSeek 请求失败: {str(e)}")
            if "503" in str(e):
                logger.error("连接错误：无法连接到 DeepSeek API，可能是代理或网络问题")
            raise

================
File: workers/translation_worker/translation/gemini_client.py
================
import logging
from typing import Dict

import google.generativeai as genai
from google.generativeai.types import GenerationConfig

from json_repair import loads

# 引入统一线程管理，与 deepseek_client 用法一致
from utils import concurrency

logger = logging.getLogger(__name__)

class GeminiClient:
    def __init__(self, api_key: str):
        """初始化 Gemini 客户端"""
        if not api_key:
            raise ValueError("Gemini API key must be provided")
        # 配置 Gemini
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('gemini-1.5-flash')  # 或 'gemini-2.0-flash-exp'
        logger.info("Gemini 客户端初始化成功")
    
    async def translate(
        self,
        system_prompt: str,
        user_prompt: str
    ) -> Dict[str, str]:
        """
        直接调用 Gemini API，要求返回 JSON 格式的内容。
        """
        try:
            response = await concurrency.run_sync(
                self.model.generate_content,
                [system_prompt, user_prompt],
                generation_config=GenerationConfig(temperature=0.3)
            )
            logger.info(f"Gemini 原文请求内容:\n{user_prompt}")
            result_text = response.text
            logger.info(f"Gemini 原始返回内容 (长度: {len(result_text)}):\n{result_text!r}")
            
            if not result_text or not result_text.strip():
                logger.error("Gemini 返回了空响应")
                raise ValueError("Empty response from Gemini")
                
            # 尝试修复和解析 JSON
            try:
                parsed_result = loads(result_text)
                logger.debug("Gemini 请求成功，JSON 解析完成")
                return parsed_result
            except Exception as json_error:
                logger.error(f"JSON 解析失败，原始内容: {result_text!r}")
                logger.error(f"JSON 解析错误详情: {str(json_error)}")
                raise
            
        except Exception as e:
            logger.error(f"Gemini 请求失败: {str(e)}")
            raise

================
File: workers/translation_worker/translation/glm4_client.py
================
import json
import logging
from zhipuai import ZhipuAI
from .prompt import GLM4_TRANSLATION_PROMPT, GLM4_SYSTEM_PROMPT

logger = logging.getLogger(__name__)

class GLM4Client:
    def __init__(self, api_key: str):
        """初始化 GLM-4 客户端"""
        if not api_key:
            raise ValueError("API key must be provided")
        self.client = ZhipuAI(api_key=api_key)
        logger.info("GLM-4 客户端初始化成功")

    async def translate(self, texts: dict) -> str:
        """调用 GLM-4 模型进行翻译，返回 JSON 字符串"""
        prompt = GLM4_TRANSLATION_PROMPT.format(json_content=json.dumps(texts, ensure_ascii=False, indent=2))
        try:
            logger.debug(f"需要翻译的JSON: {json.dumps(texts, ensure_ascii=False, indent=2)}")
            response = self.client.chat.completions.create(
                model="glm-4-flash",
                messages=[
                    {"role": "system", "content": GLM4_SYSTEM_PROMPT},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                top_p=0.8
            )
            content = response.choices[0].message.content
            logger.debug(f"翻译结果: {content}")
            return content
        except Exception as e:
            logger.error(f"GLM-4 翻译请求失败: {str(e)}")
            raise

================
File: workers/translation_worker/translation/prompt.py
================
# 支持的语言映射
LANGUAGE_MAP = {
    "zh": "中文",
    "en": "英文",
    "ja": "日文",
    "ko": "韩文"
}

TRANSLATION_USER_PROMPT = """
**角色设定：**
- **经历：** 游学四方、博览群书、翻译官、外交官
- **性格：**严谨、好奇、坦率、睿智、求真、惜墨如金
- **技能：**精通{target_language}、博古通今、斟字酌句、精确传达
- **表达方式：** 精炼、简洁、最优化、避免冗余
**执行规则：**
1.无论何时，只翻译JSON格式中的value，*严格保持原 JSON 结构与各层级字段key数量完全一致*
2.value中出现的数字，翻译成*{target_language}数字*，而非阿拉伯数字。
3.仔细阅读原文，结合上下文，深思熟虑，并将你的思考过程和分析放入 "thinking" 字段中。
4.确保译文语气、风格、表达的意思与原文保持一致。
5.将翻译后的JSON文本放入 "output" 字段中。

最终请返回一个JSON对象，结构如下：
{{
    "thinking": "...",
    "output": {{ ... 翻译后的JSON ... }}
}}

原文JSON如下：
{json_content}
"""

TRANSLATION_SYSTEM_PROMPT = """你将扮演久经历练的翻译官，致力于将提供的JSON格式原文翻译成地道的{target_language}。"""

SIMPLIFICATION_USER_PROMPT = """
**角色设定：**
- **性格：**严谨克制、精确表达、追求简约
- **技能：**咬文嚼字、斟字酌句、去芜存菁
- **表达方式：** 精炼、清晰、最优化、避免冗余
**执行规则：**
1.在"thinking" 字段中提醒自己：
-原始文本是什么语言，你的任务不是翻译，而是精简，*切勿改变语言*。
-原始文本中出现的数字，保留当前语言的数字形式，而非阿拉伯数字。
-无论何时，只精简JSON格式中的value，*保持key不变，不要进行合并*。
2.首先对value内容进行深度分析，进行3种不同层次的精简：
- "slight": 轻微精简，去除冗余和重复，忠实保持原意。
- "moderate": 中度精简，进一步去除冗余，用精简的表达取代复杂的表达，保持原意不变。
- "extreme": 极度精简，保持句子的基本结构和意思不变。

最终请返回一个JSON对象，结构如下：
{{
    "thinking": "...",
    "slight": {{ ... 轻微精简后的JSON ... }},
    "moderate": {{ ... 中度精简后的JSON ... }},
    "extreme": {{ ... 极度精简后的JSON ... }}
}}

原文JSON如下：
{json_content}
"""

SIMPLIFICATION_SYSTEM_PROMPT = """你将扮演克制严谨的语言专家，致力于将提供的JSON格式文本进行精简，不要对文本进行翻译。"""

================
File: workers/translation_worker/translation/translator.py
================
import asyncio
import logging
from typing import Dict, List, AsyncGenerator, Protocol, Optional, TypeVar
from dataclasses import dataclass
from .prompt import (
    TRANSLATION_SYSTEM_PROMPT,
    TRANSLATION_USER_PROMPT,
    SIMPLIFICATION_SYSTEM_PROMPT,
    SIMPLIFICATION_USER_PROMPT,
    LANGUAGE_MAP
)

logger = logging.getLogger(__name__)

class TranslationClient(Protocol):
    async def translate(
        self,
        texts: Dict[str, str],
        system_prompt: str,
        user_prompt: str
    ) -> Dict[str, str]:
        ...

@dataclass
class BatchConfig:
    """批处理配置"""
    initial_size: int = 100
    min_size: int = 1
    required_successes: int = 2
    retry_delay: float = 0.1

T = TypeVar('T')

class Translator:
    def __init__(self, translation_client: TranslationClient):
        self.translation_client = translation_client
        self.logger = logging.getLogger(__name__)

    async def translate(self, texts: Dict[str, str], target_language: str = "zh") -> Dict[str, str]:
        """执行翻译并返回包含 "thinking" 与 "output" 字段的 JSON"""
        try:
            system_prompt = TRANSLATION_SYSTEM_PROMPT.format(
                target_language=LANGUAGE_MAP.get(target_language, target_language)
            )
            user_prompt = TRANSLATION_USER_PROMPT.format(
                target_language=LANGUAGE_MAP.get(target_language, target_language),
                json_content=texts
            )
            return await self.translation_client.translate(
                system_prompt=system_prompt,
                user_prompt=user_prompt
            )
        except Exception as e:
            self.logger.error(f"翻译失败: {str(e)}")
            raise

    async def simplify(self, texts: Dict[str, str]) -> Dict[str, str]:
        """执行简化并返回包含 "thinking"、"slight"、"moderate"、"extreme" 字段的 JSON"""
        try:
            system_prompt = SIMPLIFICATION_SYSTEM_PROMPT
            user_prompt = SIMPLIFICATION_USER_PROMPT.format(json_content=texts)
            return await self.translation_client.translate(
                system_prompt=system_prompt,
                user_prompt=user_prompt
            )
        except Exception as e:
            self.logger.error(f"简化失败: {str(e)}")
            raise

    async def _process_batch(
        self,
        items: List[T],
        process_func: callable,
        config: BatchConfig,
        error_handler: Optional[callable] = None,
        reduce_batch_on_error: bool = True
    ) -> AsyncGenerator[List[T], None]:
        if not items:
            return

        i = 0
        batch_size = config.initial_size
        success_count = 0
        
        while i < len(items):
            # 提前初始化 batch 变量，确保异常处理中可以安全访问
            batch = []
            try:
                batch = items[i:i+batch_size]
                if not batch:
                    break

                results = await process_func(batch)
                if results:
                    success_count += 1
                    yield results
                    i += len(batch)
                    if reduce_batch_on_error and batch_size < config.initial_size and success_count >= config.required_successes:
                        self.logger.debug(f"连续成功{success_count}次，恢复到初始批次大小: {config.initial_size}")
                        batch_size = config.initial_size
                        success_count = 0

                    if i < len(items):
                        await asyncio.sleep(config.retry_delay)

            except Exception as e:
                self.logger.error(f"批处理失败: {str(e)}")
                if reduce_batch_on_error and batch_size > config.min_size:
                    batch_size = max(batch_size // 2, config.min_size)
                    success_count = 0
                    self.logger.debug(f"出错后减小批次大小到: {batch_size}")
                    continue
                else:
                    # 确保 batch 有值，即使发生了异常
                    if error_handler and batch:
                        yield error_handler(batch)
                    i += len(batch) if batch else 1  # 如果 batch 为空，则至少前进一个

    async def _process_translation_batch(self, batch: List, target_language: str = "zh") -> List:
        """处理翻译批次"""
        if not batch:
            return None
            
        # 构建翻译请求
        texts = {}
        for i, sentence in enumerate(batch):
            texts[str(i)] = sentence.raw_text
                
        self.logger.debug(f"翻译批次: {len(texts)}条文本")
        translated = await self.translate(texts, target_language)
        if "output" not in translated:
            self.logger.error("翻译结果中缺少 output 字段")
            return None
        translated_texts = translated["output"]
        if len(translated_texts) == len(texts):
            for j, sentence in enumerate(batch):
                sentence.trans_text = translated_texts[str(j)]
            return batch
        return None

    def handle_error(self, batch: List) -> List:
        for sentence in batch:
            sentence.trans_text = sentence.raw_text
        return batch

    async def translate_sentences(
        self,
        sentences: List,
        batch_size: int = 100,
        target_language: str = "zh"
    ) -> AsyncGenerator[List, None]:
        """
        批量翻译处理，将每个句子的原始文本翻译后赋值给 sentence.trans_text。
        """
        if not sentences:
            self.logger.warning("收到空的句子列表")
            return

        config = BatchConfig(initial_size=batch_size)

        async for batch_result in self._process_batch(
            sentences,
            self._process_translation_batch,
            config,
            error_handler=self.handle_error,
            reduce_batch_on_error=True
        ):
            yield batch_result

    async def simplify_sentences(
        self,
        sentences: List,
        batch_size: int = 4,
        target_speed: float = 1.1  # 目标语速设定为 max_speed，此处默认值 1.1
    ) -> AsyncGenerator[List, None]:
        """
        批量精简处理，对于语速过快的句子（由 DurationAligner 筛选），
        根据原文本与各精简版本的长度比较，计算理想文本长度后选择最佳候选版本。

        理想文本长度计算公式：
            ideal_length = len(old_text) * (target_speed / s.speed)
        当 target_speed = max_speed 时，可确保精简后的文本达到预期的语速要求。
        """
        if not sentences:
            self.logger.warning("收到空的句子列表")
            return

        config = BatchConfig(initial_size=batch_size, min_size=1, required_successes=2)

        async def process_batch(batch: List) -> Optional[List]:
            texts = {str(i): s.trans_text for i, s in enumerate(batch)}
            self.logger.debug(f"简化批次: {len(texts)}条文本")
            batch_result = await self.simplify(texts)
            
            if "thinking" not in batch_result or not any(key in batch_result for key in ["slight", "moderate", "extreme"]):
                self.logger.error("简化结果格式不正确，缺少必要字段")
                return None
                
            for i, s in enumerate(batch):
                old_text = s.trans_text
                str_i = str(i)
                
                # 确保每个句子的简化结果都存在
                if not any(str_i in batch_result.get(key, {}) for key in ["slight", "moderate", "extreme"]):
                    self.logger.error(f"句子 {i} 的简化结果不完整")
                    continue

                # 根据原文本长度和当前语速计算理想文本长度
                ideal_length = len(old_text) * (target_speed / s.speed) if s.speed > 0 else len(old_text)
                
                acceptable_candidates = {}
                non_acceptable_candidates = {}
                
                for key in ["slight", "moderate", "extreme"]:
                    if key in batch_result and str_i in batch_result[key]:
                        candidate_text = batch_result[key][str_i]
                        if candidate_text:
                            candidate_length = len(candidate_text)
                            if candidate_length <= ideal_length:
                                acceptable_candidates[key] = candidate_text
                            else:
                                non_acceptable_candidates[key] = candidate_text
                
                if acceptable_candidates:
                    # 在满足候选长度不超过理想长度的版本中，选择文本最长的版本
                    chosen_key, chosen_text = max(acceptable_candidates.items(), key=lambda item: len(item[1]))
                elif non_acceptable_candidates:
                    # 若所有候选均超过理想长度，则选择与理想长度差值最小的版本
                    chosen_key, chosen_text = min(non_acceptable_candidates.items(), key=lambda item: abs(len(item[1]) - ideal_length))
                else:
                    chosen_text = old_text

                s.trans_text = chosen_text
                self.logger.info(
                    f"精简: {old_text} -> {chosen_text} (理想长度: {ideal_length}, s.speed: {s.speed})"
                )
            return batch

        def handle_error(batch: List) -> List:
            # 出错时原样返回
            return batch

        async for batch_result in self._process_batch(
            sentences,
            process_batch,
            config,
            error_handler=handle_error,
            reduce_batch_on_error=False
        ):
            yield batch_result

================
File: workers/translation_worker/worker.py
================
import logging
import asyncio
from typing import List, Any
from utils.worker_decorators import redis_worker_decorator
from utils.task_state import TaskState
from .translation.translator import Translator
from .translation.deepseek_client import DeepSeekClient
from .translation.gemini_client import GeminiClient

logger = logging.getLogger(__name__)

class TranslationWorker:
    """
    翻译 Worker：调用 Translator 对句子进行批量翻译。
    """

    def __init__(self, config):
        """初始化 TranslationWorker"""
        self.config = config
        self.logger = logger
        
        # 根据配置选择翻译客户端
        translation_model = config.TRANSLATION_MODEL.lower()
        if translation_model == "deepseek":
            client = DeepSeekClient(api_key=config.DEEPSEEK_API_KEY)
        elif translation_model == "gemini":
            client = GeminiClient(api_key=config.GEMINI_API_KEY)
        else:
            raise ValueError(f"不支持的翻译模型: {translation_model}")
        
        # 直接初始化 Translator
        self.translator = Translator(translation_client=client)

    @redis_worker_decorator(
        input_queue='translation_queue',
        next_queue='modelin_queue',
        worker_name='翻译 Worker',
        mode='stream',
        serialization_mode='msgpack'
    )
    async def run(self, item, task_state: TaskState):
        # 检查是否有嵌套的数据结构
        sentences_list = item.get('data', item) if isinstance(item, dict) else item
        
        if not sentences_list:
            return
        self.logger.debug(f"[翻译 Worker] 收到 {len(sentences_list)} 句子, TaskID={task_state.task_id}")

        async for translated_batch in self.translator.translate_sentences(
            sentences_list,
            batch_size=self.config.TRANSLATION_BATCH_SIZE,
            target_language=task_state.target_language
        ):
            yield translated_batch

async def start():
    """启动 Worker"""
    config_module = __import__('config')
    config = config_module.Config()
    worker = TranslationWorker(config)
    await worker.run()

if __name__ == '__main__':
    asyncio.run(start())

================
File: workers/tts_worker/tts_token_gener.py
================
# core/tts_token_gener.py

import logging
import asyncio
from typing import List
from services.cosyvoice.client import CosyVoiceClient
from utils import concurrency

class TTSTokenGenerator:
    def __init__(self, cosyvoice_client: CosyVoiceClient):
        self.cosyvoice_client = cosyvoice_client
        self.logger = logging.getLogger(__name__)

    async def tts_token_maker(self, sentences: List):
        """
        并发生成TTS tokens，并存储预估时长
        """
        if not sentences:
            return []

        tasks = []
        for s in sentences:
            text_uuid = s.model_input.get('text_uuid')
            speaker_uuid = s.model_input.get('speaker_uuid')
                
            if not text_uuid or not speaker_uuid:
                self.logger.warning("缺少text_uuid或speaker_uuid，无法生成TTS tokens")
                continue

            tasks.append(asyncio.create_task(self._generate_tts_single_async(s, text_uuid, speaker_uuid)))

        processed = await asyncio.gather(*tasks)
        return processed

    async def _generate_tts_single_async(self, sentence, text_uuid: str, speaker_uuid: str):
        return await concurrency.run_sync(
            self._generate_tts_single, sentence, text_uuid, speaker_uuid
        )

    def _generate_tts_single(self, sentence, text_uuid: str, speaker_uuid: str):
        duration_ms, success = self.cosyvoice_client.generate_tts_tokens(text_uuid, speaker_uuid)
        if not success:
            self.logger.error(f"生成TTS tokens失败 (text_uuid={text_uuid}, speaker_uuid={speaker_uuid})")
            return sentence

        sentence.duration = duration_ms
            
        self.logger.debug(f"[TTS Token] (text_uuid={text_uuid}, speaker_uuid={speaker_uuid}) 生成完毕 => 估计时长 {duration_ms}ms")
        return sentence

================
File: workers/tts_worker/worker.py
================
import logging
import asyncio
from typing import List, Any
from utils.worker_decorators import redis_worker_decorator
from utils.task_state import TaskState
from .tts_token_gener import TTSTokenGenerator
from services.cosyvoice.client import CosyVoiceClient

logger = logging.getLogger(__name__)

class TTSTokenWorker:
    """
    TTS Token 生成 Worker：调用 TTSTokenGenerator 为句子生成 TTS token。
    """

    def __init__(self, config):
        """初始化 TTSTokenWorker"""
        self.config = config
        self.logger = logger
        
        # 初始化 CosyVoiceClient
        cosyvoice_address = f"{config.COSYVOICE_SERVICE_HOST}:{config.COSYVOICE_SERVICE_PORT}"
        cosyvoice_client = CosyVoiceClient(address=cosyvoice_address)
        self.tts_token_generator = TTSTokenGenerator(cosyvoice_client=cosyvoice_client)

    @redis_worker_decorator(
        input_queue='tts_token_queue',
        next_queue='duration_align_queue',
        worker_name='TTS Token生成 Worker',
        serialization_mode='msgpack'
    )
    async def run(self, item, task_state: TaskState):
        sentences_batch = item.get('data', item) if isinstance(item, dict) else item
        if not sentences_batch:
            return
        self.logger.debug(f"[TTS Token生成 Worker] 收到 {len(sentences_batch)} 句子, TaskID={task_state.task_id}")

        # 生成 TTS token
        await self.tts_token_generator.tts_token_maker(sentences_batch)
        
        # 计算目标时长
        for s in sentences_batch:
            s.target_duration = s.end - s.start
            
        return sentences_batch

async def start():
    """启动 Worker"""
    config_module = __import__('config')
    config = config_module.Config()
    worker = TTSTokenWorker(config)
    await worker.run()

if __name__ == '__main__':
    asyncio.run(start())

================
File: __init__.py
================
# 空文件即可，标识这是一个 Python 包

================
File: .cursorignore
================
# 忽略模型文件夹，因为包含大量模型文件和第三方代码
models/

================
File: .env.example
================
# ================================
# .env.example
# ================================

# 【可选】修改 FLASK_ENV，默认使用 development 方便调试
FLASK_ENV=development

# 翻译模型选择 (可选值: deepseek, glm4, gemini)
TRANSLATION_MODEL=deepseek

# API Keys (请替换为实际的密钥)
ZHIPUAI_API_KEY=your_zhipuai_key_here
GEMINI_API_KEY=your_gemini_key_here
DEEPSEEK_API_KEY=your_deepseek_key_here

================
File: api.py
================
# ------------------------------
# backend/api.py  (完整可复制版本)
# ------------------------------
import sys
from pathlib import Path
import logging
import uuid
import asyncio
from typing import Dict

import uvicorn
from fastapi import FastAPI, File, UploadFile, HTTPException, Request, Form
from fastapi.responses import JSONResponse, FileResponse
from fastapi.templating import Jinja2Templates
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
import aiofiles

from config import Config
config = Config()
config.init_directories()

sys.path.extend(config.SYSTEM_PATHS)

logging.basicConfig(
    level=logging.INFO,
    format="%(levelname)s | %(asctime)s | %(name)s | L%(lineno)d | %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)
logger = logging.getLogger(__name__)

from video_translator import ViTranslator
from utils.task_storage import TaskPaths
from fastapi import BackgroundTasks

app = FastAPI(debug=True)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

current_dir = Path(__file__).parent
templates = Jinja2Templates(directory=str(current_dir / "templates"))

# 初始化服务
vi_translator = None

@app.on_event("startup")
async def startup_event():
    global vi_translator
    vi_translator = await ViTranslator(config=config).initialize()

task_results: Dict[str, dict] = {}

@app.get("/")
async def index(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

@app.post("/upload")
async def upload_video(
    video: UploadFile = File(...),
    target_language: str = Form("zh"),
    generate_subtitle: bool = Form(False),
):
    try:
        if not video:
            raise HTTPException(status_code=400, detail="没有文件上传")
        
        if not video.content_type.startswith('video/'):
            raise HTTPException(status_code=400, detail="只支持视频文件")
            
        if target_language not in ["zh", "en", "ja", "ko"]:
            raise HTTPException(status_code=400, detail=f"不支持的目标语言: {target_language}")
        
        task_id = str(uuid.uuid4())
        task_dir = config.TASKS_DIR / task_id
        task_paths = TaskPaths(task_dir=task_dir, config=config, task_id=task_id)
        task_paths.create_directories()
        
        video_path = task_paths.input_dir / f"original_{video.filename}"
        try:
            async with aiofiles.open(video_path, "wb") as f:
                content = await video.read()
                await f.write(content)
        except Exception as e:
            logger.error(f"保存文件失败: {str(e)}")
            raise HTTPException(status_code=500, detail="文件保存失败")

        # 创建新的事件循环来处理视频转换
        loop = asyncio.get_event_loop()
        task = loop.create_task(vi_translator.trans_video(
            video_path=str(video_path),
            task_id=task_id,
            task_paths=task_paths,
            target_language=target_language,
            generate_subtitle=generate_subtitle,
        ))
        
        task_results[task_id] = {
            "status": "processing",
            "message": "视频处理中",
            "progress": 0
        }
        
        async def on_task_complete(t):
            try:
                result = await t
                if result.get('status') == 'success':
                    task_results[task_id].update({
                        "status": "success",
                        "message": "处理完成",
                        "progress": 100
                    })
                else:
                    task_results[task_id].update({
                        "status": "error",
                        "message": result.get('message', '处理失败'),
                        "progress": 0
                    })
            except Exception as e:
                logger.error(f"任务处理失败: {str(e)}")
                task_results[task_id].update({
                    "status": "error",
                    "message": str(e),
                    "progress": 0
                })
        
        task.add_done_callback(lambda t: asyncio.create_task(on_task_complete(t)))
        
        return {
            'status': 'processing',
            'task_id': task_id,
            'message': '视频上传成功，开始处理'
        }
    except HTTPException as e:
        raise e
    except Exception as e:
        logger.error(f"上传处理失败: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/task/{task_id}")
async def get_task_status(task_id: str):
    result = task_results.get(task_id)
    if not result:
        return {
            "status": "error",
            "message": "任务不存在",
            "progress": 0
        }
    return result

# 挂载静态文件服务
app.mount("/playlists", StaticFiles(directory=str(config.PUBLIC_DIR / "playlists")), name="playlists")
app.mount("/segments", StaticFiles(directory=str(config.PUBLIC_DIR / "segments")), name="segments")

@app.get("/download/{task_id}")
async def download_translated_video(task_id: str):
    final_video_path = config.TASKS_DIR / task_id / "output" / f"final_{task_id}.mp4"
    if not final_video_path.exists():
        raise HTTPException(status_code=404, detail="最终视频文件尚未生成或已被删除")
    return FileResponse(
        str(final_video_path),
        media_type='video/mp4',
        filename=f"final_{task_id}.mp4",
    )

if __name__ == "__main__":
    uvicorn.run(
        app,
        host=config.SERVER_HOST,
        port=config.SERVER_PORT,
        log_level="info"
    )

================
File: config.py
================
import os
from pathlib import Path
from dotenv import load_dotenv
import torch

current_dir = Path(__file__).parent
env_path = current_dir / '.env'
load_dotenv(env_path)

project_dir = current_dir.parent
storage_dir = project_dir / 'storage'

class Config:
    SERVER_HOST = "0.0.0.0"
    SERVER_PORT = 8000
    LOG_LEVEL = "DEBUG"

    BASE_DIR = storage_dir
    TASKS_DIR = BASE_DIR / "tasks"
    PUBLIC_DIR = BASE_DIR / "public"

    BATCH_SIZE = 6
    TARGET_SPEAKER_AUDIO_DURATION = 8
    VAD_SR = 16000
    VOCALS_VOLUME = 0.7
    BACKGROUND_VOLUME = 0.3
    AUDIO_OVERLAP = 1024
    NORMALIZATION_THRESHOLD = 0.9

    SEGMENT_MINUTES = 5
    MIN_SEGMENT_MINUTES = 3

    TRANSLATION_MODEL = os.getenv("TRANSLATION_MODEL", "deepseek")
    ZHIPUAI_API_KEY = os.getenv("ZHIPUAI_API_KEY", "")
    GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "")
    DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY", "")

    SYSTEM_PATHS = [
        str(current_dir / 'models' / 'CosyVoice'),
        str(current_dir / 'models' / 'ClearVoice'),
        str(current_dir / 'models' / 'CosyVoice' / 'third_party' / 'Matcha-TTS')
    ]

    MODEL_DIR = project_dir / "models"

    # =========== 全局音频配置 ===========
    SAMPLE_RATE = 24000  # 全局采样率，从 CosyVoice 模型加载后会被更新

    # =========== ASR 模型相关配置 ===========
    ASR_MODEL_DIR = MODEL_DIR / "SenseVoice"
    ASR_MODEL_NAME = "iic/SenseVoiceSmall"
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    
    # ASR 模型的其他参数
    ASR_MODEL_KWARGS = {
        "model": "iic/SenseVoiceSmall",
        "remote_code": "./models/SenseVoice/model.py",
        "vad_model": "iic/speech_fsmn_vad_zh-cn-16k-common-pytorch",
        "vad_kwargs": {"max_single_segment_time": 30000},
        "spk_model": "cam++",
        "trust_remote_code": True,
        "disable_update": True,
        "device": "cuda" if torch.cuda.is_available() else "cpu"
    }

    @property
    def MODEL_PATH(self) -> Path:
        return Path(self.MODEL_DIR)

    @property
    def BASE_PATH(self) -> Path:
        return self.BASE_DIR

    @property
    def TASKS_PATH(self) -> Path:
        return self.TASKS_DIR

    @property
    def PUBLIC_PATH(self) -> Path:
        return self.PUBLIC_DIR

    @classmethod
    def init_directories(cls):
        directories = [
            cls.BASE_DIR,
            cls.TASKS_DIR,
            cls.PUBLIC_DIR,
            cls.PUBLIC_DIR / "playlists",
            cls.PUBLIC_DIR / "segments"
        ]
        for dir_path in directories:
            dir_path.mkdir(parents=True, exist_ok=True)
            os.chmod(str(dir_path), 0o755)

    MAX_GAP_MS = 2000
    SHORT_SENTENCE_MERGE_THRESHOLD_MS = 1000
    MAX_TOKENS_PER_SENTENCE = 80
    MIN_SENTENCE_LENGTH = 4
    SENTENCE_END_TOKENS = {9686, 9688, 9676, 9705, 9728, 9729, 20046, 24883, 24879}
    STRONG_END_TOKENS = {9688, 9676, 9705, 9729, 20046, 24883}
    WEAK_END_TOKENS = {9686, 9728, 24879}
    SPEAKER_AUDIO_TARGET_DURATION = 8.0
    TRANSLATION_BATCH_SIZE = 50
    MODELIN_BATCH_SIZE = 3
    # 控制同时处理多少个视频分段
    MAX_PARALLEL_SEGMENTS = 2

    # 存储服务配置
    STORAGE_TYPE = "local"  # 'local' 或 's3'
    
    # HLS 配置
    HLS_SEGMENT_DURATION = 10  # 分片时长(秒)
    HLS_LIST_SIZE = 6         # 播放列表保留的分片数
    HLS_SEGMENT_FORMAT = "ts"  # 分片格式
    HLS_TIME = 10             # 每个分片的目标时长
    HLS_FLAGS = "independent_segments"  # HLS 标志

    # HLS gRPC服务配置
    HLS_GRPC_HOST = "0.0.0.0"
    HLS_GRPC_PORT = 50051
    
    # HLS服务地址(供客户端使用)
    HLS_SERVICE_HOST = os.getenv("HLS_SERVICE_HOST", "localhost")
    HLS_SERVICE_PORT = int(os.getenv("HLS_SERVICE_PORT", "50051"))

    # CosyVoice 服务配置
    COSYVOICE_SERVICE_HOST = os.getenv("COSYVOICE_SERVICE_HOST", "localhost")
    COSYVOICE_SERVICE_PORT = int(os.getenv("COSYVOICE_SERVICE_PORT", "50052"))

================
File: postcss.config.json
================
{ "plugins": { "postcss-preset-env": {}, "autoprefixer": {} } }

================
File: run_cosyvoice_service.py
================
import logging
import sys
import os
from pathlib import Path
import argparse

current_dir = Path(__file__).parent

def setup_system_paths():
    system_paths = [
        str(current_dir / 'models' / 'CosyVoice'),
        str(current_dir / 'models' / 'CosyVoice' / 'third_party' / 'Matcha-TTS')
    ]
    for path in system_paths:
        if path not in sys.path and os.path.exists(path):
            sys.path.append(path)
            logging.info(f"Added {path} to system path")

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    parser = argparse.ArgumentParser()
    parser.add_argument('--model_dir', type=str, default='models/CosyVoice/pretrained_models/CosyVoice2-0.5B', help='base directory containing model folders')
    parser.add_argument('--host', type=str, default='0.0.0.0', help='service host')
    parser.add_argument('--port', type=int, default=50052, help='service port')
    args = parser.parse_args()
    
    # 设置系统路径
    setup_system_paths()

    # 必须在导入任何使用顶层 "cosyvoice" 模块之前注册别名
    import models.CosyVoice.cosyvoice
    sys.modules["cosyvoice"] = models.CosyVoice.cosyvoice

    # 现在再导入服务模块，确保其内部使用 "cosyvoice" 时能正确找到
    from services.cosyvoice.service import serve
    serve(args)

================
File: run_hls_service.py
================
import asyncio
import logging
from config import Config
from services.hls import serve

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

async def main():
    config = Config()
    server = serve(config)
    
    try:
        logger.info(f"启动HLS gRPC服务 - {config.HLS_GRPC_HOST}:{config.HLS_GRPC_PORT}")
        await server.start()
        await server.wait_for_termination()
    except KeyboardInterrupt:
        logger.info("正在关闭服务器...")
        await server.stop(0)
        
if __name__ == "__main__":
    asyncio.run(main())

================
File: video_translator.py
================
import asyncio
import json
import logging
from pathlib import Path
from typing import Dict, Any, Optional

import aioredis

from config import Config
from services.hls.client import HLSClient
from utils.task_storage import TaskPaths
from utils.task_state import TaskState
from utils.redis_utils import save_task_state, push_to_queue
from utils.ffmpeg_utils import FFmpegTool

logger = logging.getLogger(__name__)

class ViTranslator:
    """视频翻译器"""
    def __init__(self, config: Config):
        self.config = config
        self.hls_client = None  # 异步初始化
        self.redis = None  # 异步初始化
        self.logger = logger

    async def initialize(self):
        """异步初始化"""
        self.redis = await aioredis.create_redis_pool("redis://localhost")
        return self

    async def trans_video(
        self,
        video_path: str,
        task_id: str,
        task_paths: TaskPaths,
        target_language: str = "zh",
        generate_subtitle: bool = False,
    ):
        """
        翻译视频
        
        Args:
            video_path: 视频路径
            task_id: 任务ID
            task_paths: 任务路径
            target_language: 目标语言
            generate_subtitle: 是否生成字幕
        
        Returns:
            最终视频路径
        """
        try:
            # 初始化 HLS 客户端
            if not self.hls_client:
                self.hls_client = await HLSClient.create(self.config)
            
            # 初始化任务
            try:
                if not await self.hls_client.init_task(task_id):
                    logger.error(f"HLS任务初始化失败")
                    raise RuntimeError("HLS任务初始化失败")
            except Exception as e:
                logger.error(f"HLS任务初始化失败: {e}", exc_info=True)
                raise RuntimeError("HLS任务初始化失败")
            
            # 创建并保存任务状态
            task_state = TaskState(
                task_id=task_id,
                task_paths=task_paths,
                target_language=target_language,
                generate_subtitle=generate_subtitle
            )
            await save_task_state(task_state)
            
            # 推送初始化任务
            segment_init_data = {
                "task_id": task_id,
                "task_dir": str(task_paths.task_dir),
                "video_path": video_path,
            }
            await self.redis.rpush("segment_init_queue", json.dumps(segment_init_data))
            
            # 等待任务完成
            try:
                final_video_path = await self._wait_task_completion(task_id)
                if final_video_path:
                    return final_video_path
                else:
                    logger.error(f"任务 {task_id} 处理失败或超时")
                    raise RuntimeError("视频处理失败或超时")
            except Exception as e:
                logger.error(f"等待任务完成时出错: {e}", exc_info=True)
                raise RuntimeError(f"处理失败: {str(e)}")
        except Exception as e:
            # 清理任务
            if self.hls_client:
                try:
                    await self.hls_client.cleanup_task(task_id)
                except Exception as cleanup_error:
                    logger.error(f"清理任务失败: {cleanup_error}")
            
            # 重新抛出异常
            raise RuntimeError(f"处理失败: {str(e)}")
        finally:
            if self.hls_client:
                await self.hls_client.close()

    async def _wait_task_completion(self, task_id: str):
        """等待任务完成并返回最终视频路径"""
        try:
            # 等待完成信号
            completion_key = f"task_completion:{task_id}"
            
            # 检查是否已经有结果
            existing_result = await self.redis.lrange(completion_key, 0, -1)
            if existing_result:
                # 如果已经有结果，直接返回
                result_data = json.loads(existing_result[0].decode('utf-8'))
                if result_data.get("status") == "success":
                    return Path(result_data.get("final_video_path", ""))
                return None
            
            # 设置更短的超时时间，避免长时间阻塞
            timeout = 300  # 5分钟超时
            try:
                result = await asyncio.wait_for(
                    self.redis.blpop(completion_key),
                    timeout=timeout
                )
                
                if result:
                    # 解析结果
                    result_data = json.loads(result[1].decode('utf-8'))
                    if result_data.get("status") == "success":
                        return Path(result_data.get("final_video_path", ""))
                return None
            except asyncio.TimeoutError:
                logger.warning(f"等待任务 {task_id} 完成超时（{timeout}秒）")
                return None
        except Exception as e:
            logger.error(f"等待任务完成时出错: {e}", exc_info=True)
            return None

if __name__ == "__main__":
    # 示例用法
    async def main():
        config = Config()
        translator = await ViTranslator(config).initialize()
        # 示例调用
        result = await translator.trans_video(
            video_path="/path/to/video.mp4",
            task_id="test_task",
            task_paths=TaskPaths(
                task_dir=Path("/tmp/tasks/test_task"),
                processing_dir=Path("/tmp/tasks/test_task/processing"),
                output_dir=Path("/tmp/tasks/test_task/output")
            )
        )
        print(result)

    # asyncio.run(main())

================
File: worker_launcher.py
================
#!/usr/bin/env python3
"""
Worker 启动器：用于启动所有 worker 进程
"""
import os
import sys
import asyncio
import logging
import signal
import argparse
from pathlib import Path
from config import Config
from workers.segment_worker.worker import SegmentWorker
from workers.asr_worker.worker import ASRWorker
from workers.translation_worker.worker import TranslationWorker
from workers.modelin_worker.worker import ModelInWorker
from workers.tts_worker.worker import TTSTokenWorker
from workers.duration_worker.worker import DurationWorker
from workers.audio_gen_worker.worker import AudioGenWorker
from workers.mixer_worker.worker import MixerWorker

# 导入日志配置模块
from utils.log_config import configure_logging

# 初始化日志系统
log_dir = configure_logging()
logger = logging.getLogger(__name__)

# 定义所有工作者类及其显示名称
WORKERS = {
    'segment': (SegmentWorker, "分段 Worker"),
    'asr': (ASRWorker, "ASR Worker"),
    'translation': (TranslationWorker, "翻译 Worker"),
    'modelin': (ModelInWorker, "模型输入 Worker"),
    'tts': (TTSTokenWorker, "TTS Token生成 Worker"),
    'duration': (DurationWorker, "时长对齐 Worker"),
    'audio': (AudioGenWorker, "音频生成 Worker"),
    'mixer': (MixerWorker, "混音 Worker"),
}

async def start_worker(worker_class, config, worker_name):
    """启动单个 worker"""
    try:
        logger.info(f"启动 {worker_name}...")
        worker = worker_class(config)
        
        # 对于需要特殊初始化的 worker
        if hasattr(worker, 'initialize') and callable(worker.initialize):
            worker = await worker.initialize()
        
        # 针对不同类型的worker调用不同的方法
        if worker_class == SegmentWorker:
            logger.info(f"{worker_name} 开始处理队列...")
            await asyncio.gather(
                worker.run_init(),
                worker.run_extract()
            )
        elif hasattr(worker, 'run') and callable(worker.run):
            logger.info(f"{worker_name} 开始处理队列...")
            await worker.run()
        else:
            logger.error(f"{worker_name} 没有可调用的run方法")
            
        # 创建一个永不结束的任务，让 worker 保持运行
        await asyncio.Future()
    except Exception as e:
        logger.error(f"{worker_name} 启动失败: {e}", exc_info=True)

async def start_all_workers(config):
    """启动所有 worker"""
    # 创建所有 worker 任务
    tasks = []
    for worker_id, (worker_class, worker_name) in WORKERS.items():
        task = asyncio.create_task(start_worker(worker_class, config, worker_name))
        tasks.append(task)
    
    # 等待所有 worker 完成
    await asyncio.gather(*tasks)

def handle_signal(sig, frame):
    """处理信号"""
    logger.info(f"收到信号 {sig}，准备关闭所有 worker...")
    for task in asyncio.all_tasks():
        task.cancel()
    
    # 强制退出
    sys.exit(0)

def main():
    """主函数"""
    parser = argparse.ArgumentParser(description="启动 Redis 队列 Worker")
    parser.add_argument('--worker', type=str, 
                        help=f'指定要启动的 worker: {", ".join(WORKERS.keys())}, all')
    args = parser.parse_args()
    
    # 注册信号处理器
    signal.signal(signal.SIGINT, handle_signal)
    signal.signal(signal.SIGTERM, handle_signal)
    
    config = Config()
    
    if args.worker in WORKERS:
        worker_class, worker_name = WORKERS[args.worker]
        asyncio.run(start_worker(worker_class, config, worker_name))
    elif args.worker == 'all' or args.worker is None:
        # 启动所有 worker
        asyncio.run(start_all_workers(config))
    else:
        logger.error("无效的 worker 名称")

if __name__ == "__main__":
    main()



================================================================
End of Codebase
================================================================
