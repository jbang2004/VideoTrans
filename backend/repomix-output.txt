This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-01-23T14:44:33.851Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repomix, visit: https://github.com/yamadashy/repomix

================================================================
Repository Structure
================================================================
core/
  timeadjust/
    duration_aligner.py
    timestamp_adjuster.py
  translation/
    __init__.py
    deepseek_client.py
    gemini_client.py
    glm4_client.py
    prompt.py
    translator.py
  __init__.py
  audio_gener.py
  audio_separator.py
  auto_sense.py
  hls_manager.py
  media_mixer.py
  model_in.py
  sentence_tools.py
  tts_token_gener.py
utils/
  decorators.py
  media_utils.py
  sentence_logger.py
  task_state.py
  task_storage.py
  temp_file_manager.py
__init__.py
.cursorignore
.env.example
api.py
config.py
pipeline_scheduler.py
postcss.config.json
requirements.txt
video_translator.py

================================================================
Repository Files
================================================================

================
File: core/timeadjust/duration_aligner.py
================
import logging

class DurationAligner:
    def __init__(self, model_in=None, simplifier=None, tts_token_gener=None, max_speed=1.1):
        self.model_in = model_in
        self.simplifier = simplifier
        self.tts_token_gener = tts_token_gener
        self.max_speed = max_speed
        self.logger = logging.getLogger(__name__)

    async def align_durations(self, sentences):
        if not sentences:
            return

        self._align_batch(sentences)

        # 查找超速句子
        retry_sentences = [s for s in sentences if s.speed > self.max_speed]
        if retry_sentences:
            self.logger.info(f"{len(retry_sentences)} 个句子语速过快, 正在精简...")
            success = await self._retry_sentences_batch(retry_sentences)
            if success:
                self._align_batch(sentences)
            else:
                self.logger.warning("精简过程失败, 保持原结果")

    def _align_batch(self, sentences):
        if not sentences:
            return
        for s in sentences:
            s.diff = s.duration - s.target_duration

        total_diff_to_adjust = sum(s.diff for s in sentences)
        current_time = sentences[0].start

        for s in sentences:
            s.adjusted_start = current_time
            diff = s.diff

            if total_diff_to_adjust == 0:
                s.adjusted_duration = s.duration
                s.diff = 0
            elif total_diff_to_adjust > 0:
                # 压缩
                positive_diff_sum = sum(x.diff for x in sentences if x.diff > 0)
                if positive_diff_sum > 0 and diff > 0:
                    proportion = diff / positive_diff_sum
                    adjustment = total_diff_to_adjust * proportion
                    s.adjusted_duration = s.duration - adjustment
                    s.diff = s.duration - s.adjusted_duration
                    s.speed = (s.duration / s.adjusted_duration) if s.adjusted_duration else 1.0
                else:
                    s.adjusted_duration = s.duration
                    s.diff = 0
            else:
                # 扩展
                negative_diff_sum_abs = sum(abs(x.diff) for x in sentences if x.diff < 0)
                if negative_diff_sum_abs > 0 and diff < 0:
                    proportion = abs(diff) / negative_diff_sum_abs
                    total_needed = abs(total_diff_to_adjust) * proportion

                    max_slowdown = s.duration * 0.1
                    slowdown = min(total_needed, max_slowdown)

                    s.adjusted_duration = s.duration + slowdown
                    s.speed = (s.duration / s.adjusted_duration) if s.adjusted_duration else 1.0

                    s.silence_duration = total_needed - slowdown
                    if s.silence_duration > 0:
                        s.adjusted_duration += s.silence_duration
                else:
                    s.adjusted_duration = s.duration
                    s.speed = 1.0
                    s.silence_duration = 0
                s.diff = s.duration - s.adjusted_duration

            current_time += s.adjusted_duration

    async def _retry_sentences_batch(self, sentences):
        try:
            # 1. 精简文本
            texts_to_simplify = {str(i): s.trans_text for i, s in enumerate(sentences)}
            simplified_texts = await self.simplifier.simplify(texts_to_simplify)

            # 2. 更新句子的文本
            for i, s in enumerate(sentences):
                new_text = simplified_texts.get(str(i))
                if new_text:
                    self.logger.info(f"精简: {s.trans_text} -> {new_text}")
                    s.trans_text = new_text

            # 3. 批量更新文本特征(复用 speaker+uuid)
            async for batch in self.model_in.modelin_maker(
                sentences,
                reuse_speaker=True,
                reuse_uuid=True,
                batch_size=3
            ):
                # 4. 生成 token (同样可以复用 uuid)
                updated_batch = await self.tts_token_gener.tts_token_maker(
                    batch, reuse_uuid=True
                )

                # Do something with updated_batch...
            return True

        except Exception as e:
            self.logger.error(f"_retry_sentences_batch 出错: {e}")
            return False

================
File: core/timeadjust/timestamp_adjuster.py
================
import logging
from typing import List, Optional
from dataclasses import dataclass

class TimestampAdjuster:
    """句子时间戳调整器"""
    
    def __init__(self, sample_rate: int):
        self.logger = logging.getLogger(__name__)
        self.sample_rate = sample_rate
        
    def update_timestamps(self, sentences: List, start_time: float = None) -> float:
        """更新句子的时间戳信息
        
        Args:
            sentences: 要更新的句子列表
            start_time: 起始时间（毫秒），如果为 None 则使用第一个句子的开始时间
            
        Returns:
            float: 最后一个句子结束的时间点（毫秒）
        """
        if not sentences:
            return start_time if start_time is not None else 0
            
        # 使用传入的起始时间或第一个句子的开始时间
        current_time = start_time if start_time is not None else sentences[0].start
        
        for i, sentence in enumerate(sentences):
            # 计算实际音频长度（毫秒）
            if sentence.generated_audio is not None:
                actual_duration = (len(sentence.generated_audio) / self.sample_rate) * 1000
            else:
                actual_duration = 0
                self.logger.warning(f"句子 {sentence.sentence_id} 没有生成音频")
            
            # 更新时间戳
            sentence.adjusted_start = current_time
            sentence.adjusted_duration = actual_duration
            
            # 更新差异值
            sentence.diff = sentence.duration - actual_duration
            
            # 更新下一个句子的开始时间
            current_time += actual_duration
            
        return current_time
        
    def validate_timestamps(self, sentences: List) -> bool:
        """验证时间戳的连续性和有效性
        
        Args:
            sentences: 要验证的句子列表
            
        Returns:
            bool: 时间戳是否有效
        """
        if not sentences:
            return True
            
        for i in range(len(sentences) - 1):
            current = sentences[i]
            next_sentence = sentences[i + 1]
            
            # 验证时间连续性
            expected_next_start = current.adjusted_start + current.adjusted_duration
            if abs(next_sentence.adjusted_start - expected_next_start) > 1:  # 允许1毫秒的误差
                self.logger.error(
                    f"时间戳不连续 - 句子 {current.sentence_id} 结束时间: {expected_next_start:.2f}ms, "
                    f"句子 {next_sentence.sentence_id} 开始时间: {next_sentence.adjusted_start:.2f}ms"
                )
                return False
                
            # 验证时长有效性
            if current.adjusted_duration <= 0:
                self.logger.error(f"句子 {current.sentence_id} 的时长无效: {current.adjusted_duration:.2f}ms")
                return False
                
        return True

================
File: core/translation/__init__.py
================
# 空文件，用于标识 translation 目录为一个 Python 包

================
File: core/translation/deepseek_client.py
================
import json
import logging
import re
from openai import OpenAI
from typing import Dict
from json_repair import loads

logger = logging.getLogger(__name__)

class DeepSeekClient:
    def __init__(self, api_key: str):
        """初始化 DeepSeek 客户端"""
        if not api_key:
            raise ValueError("DeepSeek API key must be provided")
            
        self.client = OpenAI(
            api_key=api_key,
            base_url="https://api.deepseek.com"
        )
        logger.info("DeepSeek 客户端初始化成功")

    def _extract_output_content(self, text: str) -> str:
        """从响应中提取 <OUTPUT> 标签中的内容
        
        Args:
            text: 包含 <OUTPUT> 标签的文本
            
        Returns:
            <OUTPUT> 标签中的内容
        """
        pattern = r"<OUTPUT>(.*?)</OUTPUT>"
        match = re.search(pattern, text, re.DOTALL)
        if match:
            return match.group(1).strip()
        logger.warning("未找到 <OUTPUT> 标签，返回原始内容")
        return text

    async def translate(
        self,
        texts: Dict[str, str],
        system_prompt: str,
        user_prompt: str
    ) -> Dict[str, str]:
        """调用 DeepSeek 模型进行处理
        
        Args:
            texts: 要处理的文本字典
            system_prompt: 系统提示词
            user_prompt: 用户提示词
            
        Returns:
            处理后的文本字典
        """
        try:
            response = self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
            )
            
            result = response.choices[0].message.content
            logger.info(f"DeepSeek 请求结果: {result}")
            
            # 提取 <OUTPUT> 标签中的内容
            output_content = self._extract_output_content(result)
            logger.info(f"提取的 OUTPUT 内容: {output_content}")
            
            parsed_result = loads(output_content)
            logger.info("DeepSeek 请求成功")
            return parsed_result
            
        except Exception as e:
            logger.error(f"DeepSeek 请求失败: {str(e)}")
            if "503" in str(e):
                logger.error("连接错误：无法连接到 DeepSeek API，可能是代理或网络问题")
            raise

================
File: core/translation/gemini_client.py
================
import json
import logging
import google.generativeai as genai
from typing import Dict
import os

from .prompt import TRANSLATION_PROMPT, SYSTEM_PROMPT, LANGUAGE_MAP, EXAMPLE_OUTPUTS

logger = logging.getLogger(__name__)

class GeminiClient:
    def __init__(self, api_key: str):
        """初始化 Gemini 客户端"""
        if not api_key:
            raise ValueError("Gemini API key must be provided")
            
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('gemini-1.5-flash')
        logger.info("Gemini 客户端初始化成功")

    async def translate(self, texts: Dict[str, str], target_language: str = "zh") -> str:
        """调用 Gemini 模型进行翻译，返回 JSON 字符串
        
        Args:
            texts: 要翻译的文本字典
            target_language: 目标语言代码 (zh/en/ja/ko)
        """
        try:
            if target_language not in LANGUAGE_MAP:
                raise ValueError(f"不支持的目标语言: {target_language}")
                
            prompt = TRANSLATION_PROMPT.format(
                json_content=json.dumps(texts, ensure_ascii=False, indent=2),
                target_language=LANGUAGE_MAP[target_language],
                example_output=EXAMPLE_OUTPUTS[target_language]
            )
            
            response = self.model.generate_content(
                [SYSTEM_PROMPT.format(target_language=LANGUAGE_MAP[target_language]), prompt],
                generation_config=genai.types.GenerationConfig(temperature=0.3)
            )
            logger.info(f"Gemini 翻译请求成功，目标语言: {LANGUAGE_MAP[target_language]}")
            return response.text
        except Exception as e:
            logger.error(f"Gemini 翻译请求失败: {str(e)}")
            if "503" in str(e):
                logger.error("连接错误：无法连接到 Gemini API，可能是代理或网络问题")
            raise

================
File: core/translation/glm4_client.py
================
import json
import logging
from zhipuai import ZhipuAI
from .prompt import GLM4_TRANSLATION_PROMPT, GLM4_SYSTEM_PROMPT

logger = logging.getLogger(__name__)

class GLM4Client:
    def __init__(self, api_key: str):
        """初始化 GLM-4 客户端"""
        if not api_key:
            raise ValueError("API key must be provided")
        self.client = ZhipuAI(api_key=api_key)

    async def translate(self, texts: dict) -> str:
        """调用 GLM-4 模型进行翻译，返回 JSON 字符串"""
        prompt = GLM4_TRANSLATION_PROMPT.format(json_content=json.dumps(texts, ensure_ascii=False, indent=2))
        try:
            # 打印需要翻译的JSON
            logger.info(f"需要翻译的JSON: {json.dumps(texts, ensure_ascii=False, indent=2)}")
            # 发送API请求
            response = self.client.chat.completions.create(
                model="glm-4-flash",
                messages=[
                    {"role": "system", "content": GLM4_SYSTEM_PROMPT},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                top_p=0.8
            )
            content = response.choices[0].message.content
            logger.info(f"翻译结果: {content}")
            return content
        except Exception as e:
            logger.error(f"GLM-4 翻译请求失败: {str(e)}")
            raise

================
File: core/translation/prompt.py
================
# 支持的语言映射
LANGUAGE_MAP = {
    "zh": "Chinese",
    "en": "English",
    "ja": "Japanese",
    "ko": "Korean"
}

TRANSLATION_USER_PROMPT = """
**角色设定：**
- **经历：** 游学四方、博览群书、翻译官、外交官
- **性格：**严谨、好奇、坦率、睿智、求真、惜墨如金
- **技能：**精通{target_language}、博古通今、斟字酌句、精确传达
- **表达方式：** 精炼、简洁、最优化、避免冗余
**执行规则：**
1.无论何时，只翻译JSON格式中的value，保持key不变。
2.value中出现的数字，翻译成*{target_language}数字*，而非阿拉伯数字。
3.对提供的原文内容深思熟虑，总结上下文，将你的总结和思考放入<Thinking></Thinking>中。
4.确保译文精炼、简洁，与原文意思保持一致。
5.把实际的输出JSON译文放在<OUTPUT></OUTPUT>中。
以下是JSON格式原文：
{json_content}
"""

TRANSLATION_SYSTEM_PROMPT = """你将扮演久经历练的翻译官，致力于将提供的JSON格式原文，翻译成地道的{target_language}，打破语言界限，促进两国交流。"""

SIMPLIFICATION_USER_PROMPT = """
**角色设定：**
- **性格：**严谨克制、精确表达、追求简约
- **技能：**咬文嚼字、斟字酌句、去芜存菁
- **表达方式：** 精炼、清晰、最优化、避免冗余
**执行规则：**
1.无论何时，只精简JSON格式中的value，*保持key不变，不要进行合并*。
2.value中出现的数字，保留当前语言的数字形式，而非阿拉伯数字。
3.首先对value内容进行深度分析，进行3种不同层次的精简：
-轻微精简：去除重复和冗余词汇，保持原意不变，放入<Slight JSON></Slight JSON>标签中。
-中度精简：进一步简化句子结构，去除不必要的修饰词，放入<Moderate JSON>></Moderate JSON>标签中。
-极度精简：仅保留核心信息，去除所有修饰和冗余，放入<Extreme JSON>></Extreme JSON>标签中。
4.选择轻微精简，并修正表达，把实际的输出JSON放在<OUTPUT></OUTPUT>中。
以下是JSON格式原文：
{json_content}
"""

SIMPLIFICATION_SYSTEM_PROMPT = """你将扮演克制严谨的语言专家，致力于将提供的JSON格式原文进行恰当的精简。"""

================
File: core/translation/translator.py
================
import asyncio
import logging
from typing import Dict, List, AsyncGenerator, Protocol
from json_repair import loads
from .prompt import (
    TRANSLATION_SYSTEM_PROMPT,
    TRANSLATION_USER_PROMPT,
    SIMPLIFICATION_SYSTEM_PROMPT,
    SIMPLIFICATION_USER_PROMPT,
    LANGUAGE_MAP
)

logger = logging.getLogger(__name__)

class TranslationClient(Protocol):
    async def translate(
        self,
        texts: Dict[str, str],
        system_prompt: str,
        user_prompt: str
    ) -> Dict[str, str]:
        ...

class Translator:
    def __init__(self, translation_client: TranslationClient):
        self.translation_client = translation_client
        self.logger = logging.getLogger(__name__)

    async def translate(self, texts: Dict[str, str], target_language: str = "zh") -> Dict[str, str]:
        """执行翻译并处理结果"""
        try:
            # 准备 prompt
            system_prompt = TRANSLATION_SYSTEM_PROMPT.format(
                target_language=LANGUAGE_MAP.get(target_language, target_language)
            )
            user_prompt = TRANSLATION_USER_PROMPT.format(
                target_language=LANGUAGE_MAP.get(target_language, target_language),
                json_content=texts
            )
            
            # 调用翻译
            result = await self.translation_client.translate(
                texts=texts,
                system_prompt=system_prompt,
                user_prompt=user_prompt
            )
            return result
            
        except Exception as e:
            self.logger.error(f"翻译失败: {str(e)}")
            raise

    async def simplify(self, texts: Dict[str, str], batch_size: int = 4) -> Dict[str, str]:
        """执行文本简化并处理结果，支持批量处理和错误恢复
        
        Args:
            texts: 要简化的文本字典
            batch_size: 初始批次大小
        Returns:
            简化后的文本字典
        """
        if not texts:
            return {}

        result = {}
        keys = list(texts.keys())
        i = 0
        success_count = 0
        current_batch_size = batch_size

        while i < len(keys):
            try:
                # 获取当前批次的文本
                batch_keys = keys[i:i+current_batch_size]
                batch_texts = {k: texts[k] for k in batch_keys}
                
                self.logger.info(f"简化批次: {len(batch_texts)}条文本, 大小: {current_batch_size}, 位置: {i}")

                # 准备 prompt
                system_prompt = SIMPLIFICATION_SYSTEM_PROMPT
                user_prompt = SIMPLIFICATION_USER_PROMPT.format(
                    json_content=batch_texts
                )
                
                # 调用简化
                batch_result = await self.translation_client.translate(
                    texts=batch_texts,
                    system_prompt=system_prompt,
                    user_prompt=user_prompt
                )
                
                # 检查结果完整性
                if len(batch_result) == len(batch_texts):
                    success_count += 1
                    result.update(batch_result)
                    i += len(batch_texts)
                    self.logger.info(f"简化成功: {len(batch_texts)}条文本, 连续成功: {success_count}次")
                    
                    # 在连续成功足够次数后恢复批次大小
                    if current_batch_size < batch_size and success_count >= 2:
                        self.logger.info(f"连续成功{success_count}次，恢复到初始批次大小: {batch_size}")
                        current_batch_size = batch_size
                        success_count = 0
                else:
                    raise ValueError(f"简化结果不完整 (输入: {len(batch_texts)}, 输出: {len(batch_result)})")

                # 避免API限流
                if i < len(keys):
                    await asyncio.sleep(0.1)

            except Exception as e:
                self.logger.error(f"简化失败: {str(e)}")
                if current_batch_size > 1:
                    current_batch_size = max(current_batch_size // 2, 1)
                    success_count = 0
                    self.logger.info(f"出错后减小批次大小到: {current_batch_size}")
                    continue
                else:
                    # 单条简化失败，保持原文
                    for k in batch_keys:
                        result[k] = texts[k]
                    i += len(batch_keys)

        return result

    async def translate_sentences(
        self,
        sentences: List,
        batch_size: int = 100,
        target_language: str = "zh"
    ) -> AsyncGenerator[List, None]:
        """批量翻译处理"""
        if not sentences:
            self.logger.warning("收到空的句子列表")
            return

        i = 0
        initial_size = batch_size  # 保存初始批次大小
        success_count = 0  # 连续成功计数
        required_successes = 2  # 需要连续成功几次才恢复大批次

        while i < len(sentences):
            # 在连续成功足够次数后才尝试恢复到初始批次大小
            if batch_size < initial_size and success_count >= required_successes:
                self.logger.info(f"连续成功{success_count}次，恢复到初始批次大小: {initial_size}")
                batch_size = initial_size
                success_count = 0

            success = False
            pos = i  # 保存当前位置

            while not success and batch_size >= 1:
                try:
                    # 获取当前批次的句子
                    batch = sentences[pos:pos+batch_size]
                    if not batch:
                        break

                    texts = {str(j): s.raw_text for j, s in enumerate(batch)}
                    self.logger.info(f"翻译批次: {len(texts)}条文本, 大小: {batch_size}, 位置: {pos}")

                    # 翻译并检查结果完整性
                    translated = await self.translate(texts, target_language)
                    
                    if len(translated) == len(texts):
                        success = True
                        success_count += 1
                        self.logger.info(f"翻译成功: {len(batch)}条文本, 连续成功: {success_count}次")
                        
                        # 处理翻译结果
                        results = []
                        for j, sentence in enumerate(batch):
                            sentence.trans_text = translated[str(j)]
                            results.append(sentence)

                        yield results
                        i += len(batch)
                    else:
                        # 结果不完整，减小批次大小重试
                        batch_size = max(batch_size // 2, 1)
                        success_count = 0
                        self.logger.warning(f"翻译不完整 (输入: {len(texts)}, 输出: {len(translated)}), 减小到: {batch_size}")
                        continue

                    # 避免API限流
                    if i < len(sentences):
                        await asyncio.sleep(0.1)

                except Exception as e:
                    self.logger.error(f"翻译失败: {str(e)}")
                    if batch_size > 1:
                        batch_size = max(batch_size // 2, 1)
                        success_count = 0
                        self.logger.info(f"出错后减小批次大小到: {batch_size}")
                        continue
                    else:
                        # 单句翻译失败，使用原文
                        results = []
                        for sentence in batch:
                            sentence.trans_text = sentence.raw_text
                            results.append(sentence)
                        yield results
                        i += 1

================
File: core/__init__.py
================
# 空文件即可，标识这是一个 Python 包

================
File: core/audio_gener.py
================
import logging
import asyncio
from concurrent.futures import ThreadPoolExecutor
import torch
import numpy as np
import os
import inspect  # 导入 inspect 模块

class AudioGenerator:
    def __init__(self, cosyvoice_model, sample_rate: int = None, max_workers=None):
        """
        Args:
            cosyvoice_model: CosyVoice模型
            sample_rate: 采样率，如果为None则使用cosyvoice_model的采样率
            max_workers: 并行处理的最大工作线程数，默认为None（将根据CPU核心数自动设置）
        """
        self.cosyvoice_model = cosyvoice_model.model
        self.sample_rate = sample_rate or cosyvoice_model.sample_rate
        # 获取CPU核心数
        cpu_count = os.cpu_count()
        # 如果未指定max_workers，则使用CPU核心数
        self.max_workers = min(max_workers or cpu_count, cpu_count)
        self.executor = None  # 延迟初始化executor
        self.logger = logging.getLogger(__name__)

    async def vocal_audio_maker(self, batch_sentences):
        """异步批量生成音频"""
        try:
            # 延迟初始化executor
            if self.executor is None:
                self.executor = ThreadPoolExecutor(max_workers=self.max_workers)
                self.logger.debug(f"创建音频生成线程池，max_workers={self.max_workers}")
            
            tasks = [
                self._generate_single_async(sentence)
                for sentence in batch_sentences
            ]
            await asyncio.gather(*tasks)
            
        except Exception as e:
            self.logger.error(f"音频生成失败: {str(e)}")
            raise

    async def _generate_single_async(self, sentence):
        """异步生成单个音频"""
        loop = asyncio.get_event_loop()
        try:
            audio_np = await loop.run_in_executor(
                self.executor, 
                self._generate_audio_single, 
                sentence
            )
            sentence.generated_audio = audio_np
            
        except Exception as e:
            self.logger.error(f"音频生成失败 (UUID: {sentence.model_input['uuid']}): {str(e)}")
            sentence.generated_audio = None  # 确保失败时设置为 None

    def _generate_audio_single(self, sentence):
        """生成单个音频"""
        model_input = sentence.model_input
        self.logger.info(f"开始生成音频 (主UUID: {model_input['uuid']})")
        
        try:
            # 检查是否有分段token
            if not model_input.get('segment_speech_tokens') or not model_input.get('segment_uuids'):
                self.logger.info(f"空的语音标记，创建空音频 (UUID: {model_input['uuid']})")
                return np.zeros(0)

            segment_audio_list = []
            
            # 为每段生成音频
            for i, (tokens, segment_uuid) in enumerate(zip(model_input['segment_speech_tokens'], 
                                                         model_input['segment_uuids'])):
                if not tokens:
                    continue
                    
                token2wav_kwargs = {
                    'token': torch.tensor(tokens).unsqueeze(dim=0),
                    'token_offset': 0,
                    'finalize': True,
                    'prompt_token': model_input.get('flow_prompt_speech_token', torch.zeros(1, 0, dtype=torch.int32)),
                    'prompt_feat': model_input.get('prompt_speech_feat', torch.zeros(1, 0, 80)),
                    'embedding': model_input.get('flow_embedding', torch.zeros(0)),
                    'uuid': segment_uuid,
                    'speed': sentence.speed if sentence.speed else 1.0
                }

                segment_output = self.cosyvoice_model.token2wav(**token2wav_kwargs)
                
                # 转换为numpy并确保是单声道
                segment_audio = segment_output.cpu().numpy()
                if segment_audio.ndim > 1:
                    segment_audio = segment_audio.mean(axis=0)
                    
                segment_audio_list.append(segment_audio)
                
                self.logger.debug(f"段落 {i+1}/{len(model_input['segment_uuids'])} 生成完成，"
                                f"时长: {len(segment_audio)/self.sample_rate:.2f}秒")

            # 合并所有音频段
            if not segment_audio_list:
                return np.zeros(0)
                
            final_audio = np.concatenate(segment_audio_list)

            # 处理静音
            if sentence.is_first and sentence.start > 0:
                silence_samples = int(sentence.start * self.sample_rate / 1000)  # 毫秒转样本数
                final_audio = np.concatenate([np.zeros(silence_samples), final_audio])

            if sentence.silence_duration > 0:
                silence_samples = int(sentence.silence_duration * self.sample_rate / 1000)  # 毫秒转样本数
                final_audio = np.concatenate([final_audio, np.zeros(silence_samples)])

            self.logger.info(f"音频生成完成 (主UUID: {model_input['uuid']}, "
                           f"段落数: {len(segment_audio_list)}, "
                           f"总时长: {len(final_audio)/self.sample_rate:.2f}秒)")
            
            return final_audio

        except Exception as e:
            self.logger.error(f"音频生成失败 (UUID: {model_input['uuid']}): {str(e)}")
            raise

================
File: core/audio_separator.py
================
from abc import ABC, abstractmethod
from typing import Tuple
import numpy as np

from models.ClearerVoice.clearvoice import ClearVoice

class AudioSeparator(ABC):
    """音频分离器接口"""
    @abstractmethod
    def separate_audio(self, input_path: str, **kwargs) -> Tuple[np.ndarray, np.ndarray]:
        """
        分离音频为语音和背景音
        Args:
            input_path: 输入音频路径
            **kwargs: 额外参数
        Returns:
            Tuple[语音数组, 背景音数组]
        """
        pass

class ClearVoiceSeparator(AudioSeparator):
    """使用 ClearVoice 实现的音频分离器"""
    def __init__(self, model_name: str = 'MossFormer2_SE_48K'):
        self.model_name = model_name
        self.clearvoice = ClearVoice(
            task='speech_enhancement',
            model_names=[model_name]
        )
    
    def separate_audio(self, input_path: str) -> Tuple[np.ndarray, np.ndarray, int]:
        """
        分离音频为语音和背景音
        Returns:
            Tuple[增强语音数据, 背景音数据, 采样率]
        """
        # 使用 ClearVoice 处理音频
        enhanced_audio, background_audio = self.clearvoice(
            input_path=input_path,
            online_write=False,
            extract_noise=True
        )
        
        # 根据模型名称确定采样率
        if self.model_name.endswith('16K'):
            sr = 16000
        elif self.model_name.endswith('48K'):
            sr = 48000
        else:
            # 默认采样率
            sr = 48000
        
        return enhanced_audio, background_audio, sr

================
File: core/auto_sense.py
================
import logging
import os
import importlib.util
import sys
import torch
import numpy as np
from tqdm import tqdm
from pathlib import Path
import time
from funasr.register import tables
from funasr.auto.auto_model import AutoModel as BaseAutoModel
from funasr.auto.auto_model import prepare_data_iterator
from funasr.utils.misc import deep_update
from funasr.models.campplus.utils import sv_chunk, postprocess
from funasr.models.campplus.cluster_backend import ClusterBackend
from .sentence_tools import get_sentences
from funasr.utils.vad_utils import slice_padding_audio_samples, merge_vad
from funasr.utils.load_utils import load_audio_text_image_video

class SenseAutoModel(BaseAutoModel):
    def __init__(self, config, **kwargs):
               
        super().__init__(**kwargs)
        self.config = config
        
        if self.spk_model is not None:
            self.cb_model = ClusterBackend().to(kwargs["device"])
            spk_mode = kwargs.get("spk_mode", "punc_segment")
            if spk_mode not in ["default", "vad_segment", "punc_segment"]:
                logging.error("spk_mode 应该是 'default', 'vad_segment' 或 'punc_segment' 之一。")
            self.spk_mode = spk_mode

    def inference_with_vad(self, input, input_len=None, **cfg):
        kwargs = self.kwargs
        self.tokenizer = kwargs.get("tokenizer")
        deep_update(self.vad_kwargs, cfg)
        
        res = self.inference(input, input_len=input_len, model=self.vad_model, kwargs=self.vad_kwargs, **cfg)

        model = self.model
        deep_update(kwargs, cfg)
        kwargs["batch_size"] = max(int(kwargs.get("batch_size_s", 300)) * 1000, 1)
        batch_size_threshold_ms = int(kwargs.get("batch_size_threshold_s", 60)) * 1000

        key_list, data_list = prepare_data_iterator(input, input_len=input_len, data_type=kwargs.get("data_type", None))
        results_ret_list = []

        pbar_total = tqdm(total=len(res), dynamic_ncols=True, disable=kwargs.get("disable_pbar", False))

        for i, item in enumerate(res):
            key, vadsegments = item["key"], item["value"]
            input_i = data_list[i]
            fs = kwargs["frontend"].fs if hasattr(kwargs["frontend"], "fs") else 16000
            speech = load_audio_text_image_video(input_i, fs=fs, audio_fs=kwargs.get("fs", 16000))
            speech_lengths = len(speech)
            print(f"##音频长度: {speech_lengths}")  # 添加此行
            if speech_lengths < 400:
                print(f"警告：音频太短（{speech_lengths} 样本），可能导致处理错误")

            sorted_data = sorted([(seg, idx) for idx, seg in enumerate(vadsegments)], key=lambda x: x[0][1] - x[0][0])

            if not sorted_data:
                logging.info(f"解码, utt: {key}, 空语音")
                continue

            results_sorted = []
            all_segments = []
            beg_idx, end_idx, max_len_in_batch = 0, 1, 0

            for j in range(len(sorted_data)):
                sample_length = sorted_data[j][0][1] - sorted_data[j][0][0]
                potential_batch_length = max(max_len_in_batch, sample_length) * (j + 1 - beg_idx)

                if (j < len(sorted_data) - 1 and 
                    sample_length < batch_size_threshold_ms and 
                    potential_batch_length < kwargs["batch_size"]):
                    max_len_in_batch = max(max_len_in_batch, sample_length)
                    end_idx += 1
                    continue

                speech_j, _ = slice_padding_audio_samples(speech, speech_lengths, sorted_data[beg_idx:end_idx])
                results = self.inference(speech_j, input_len=None, model=model, kwargs=kwargs, **cfg)

                if self.spk_model is not None:
                    for _b, speech_segment in enumerate(speech_j):
                        vad_segment = sorted_data[beg_idx:end_idx][_b][0]
                        segments = sv_chunk([[vad_segment[0] / 1000.0, vad_segment[1] / 1000.0, np.array(speech_segment)]])
                        all_segments.extend(segments)
                        speech_b = [seg[2] for seg in segments]
                        spk_res = self.inference(speech_b, input_len=None, model=self.spk_model, kwargs=kwargs, **cfg)
                        results[_b]["spk_embedding"] = spk_res[0]["spk_embedding"]
                beg_idx, end_idx = end_idx, end_idx + 1
                max_len_in_batch = sample_length
                results_sorted.extend(results)

            if len(results_sorted) != len(sorted_data):
                logging.info(f"解码，utt: {key}，空结果")
                continue

            restored_data = [0] * len(sorted_data)
            for j, (_, idx) in enumerate(sorted_data):
                restored_data[idx] = results_sorted[j]

            result = self.combine_results(restored_data, vadsegments)

            if self.spk_model is not None and kwargs.get("return_spk_res", True):
                all_segments.sort(key=lambda x: x[0])
                spk_embedding = result["spk_embedding"]
                labels = self.cb_model(spk_embedding.cpu(), oracle_num=kwargs.get("preset_spk_num", None))
                sv_output = postprocess(all_segments, None, labels, spk_embedding.cpu())

                if "timestamp" not in result:
                    logging.error(f"speaker diarization 依赖于时间戳对于 utt: {key}")
                    sentence_list = []
                else:
                    sentence_list = get_sentences(
                        tokens=result["token"],
                        timestamps=result["timestamp"],
                        tokenizer=self.tokenizer,
                        speech=speech,
                        sd_time_list=sv_output,
                        sample_rate=fs,
                        config=self.config
                    )
                    results_ret_list = sentence_list  # 直接添加 sentence_list 到结果列表
            else:
                sentence_list = []
            pbar_total.update(1)

        pbar_total.close()
        return results_ret_list

    def combine_results(self, restored_data, vadsegments):
        result = {}
        for j, data in enumerate(restored_data):
            for k, v in data.items():
                if k.startswith("timestamp"):
                    if k not in result:
                        result[k] = []
                    for t in v:
                        t[0] += vadsegments[j][0]
                        t[1] += vadsegments[j][0]
                    result[k].extend(v)
                elif k == "spk_embedding":
                    if k not in result:
                        result[k] = v
                    else:
                        result[k] = torch.cat([result[k], v], dim=0)
                elif "token" in k:
                    if k not in result:
                        result[k] = v
                    else:
                        result[k].extend(v)
                else:
                    if k not in result:
                        result[k] = v
                    else:
                        result[k] += v
        return result

================
File: core/hls_manager.py
================
import logging
import asyncio
from pathlib import Path
import m3u8
from typing import List, Union
from utils.decorators import handle_errors
from utils.task_storage import TaskPaths
import os.path
import shutil

logger = logging.getLogger(__name__)

class HLSManager:
    """处理 HLS 流媒体相关的功能"""
    def __init__(self, config, task_id: str, task_paths: TaskPaths):
        self.config = config
        self.task_id = task_id
        self.task_paths = task_paths
        self.logger = logging.getLogger(__name__)
        
        # 设置路径 - 确保是 Path 对象
        self.playlist_path = Path(task_paths.playlist_path)
        self.segments_dir = Path(task_paths.segments_dir)  # 确保是 Path 对象
        self.sequence_number = 0
        self._lock = asyncio.Lock()
        
        # 初始化主播放列表
        self.playlist = m3u8.M3U8()
        self.playlist.version = 3
        self.playlist.target_duration = 20
        self.playlist.media_sequence = 0
        self.playlist.playlist_type = 'VOD'
        self.playlist.is_endlist = False
        
        # 新增属性
        self.has_segments = False
        
        # 立即保存初始播放列表
        self._save_playlist()
    
    def _save_playlist(self) -> None:
        """保存播放列表到文件"""
        try:
            # 确保片段路径以斜杠开头
            for segment in self.playlist.segments:
                if segment.uri is not None and not segment.uri.startswith('/'):
                    segment.uri = '/' + segment.uri
            
            with open(self.playlist_path, 'w', encoding='utf-8') as f:
                f.write(self.playlist.dumps())
            logger.info(f"播放列表已更新")
        except Exception as e:
            logger.error(f"保存播放列表失败: {e}")
            raise
    
    @handle_errors(None)
    async def add_segment(self, video_path: Union[str, Path], part_index: int) -> None:
        """添加新的视频片段到播放列表"""
        async with self._lock:
            try:
                # 确保目录存在
                self.segments_dir.mkdir(parents=True, exist_ok=True)
                
                # 生成文件名和路径
                segment_filename = f'segment_{self.sequence_number:04d}_%03d.ts'
                # 直接在目标目录生成ts文件
                segment_pattern = str(self.segments_dir / segment_filename)
                temp_playlist_path = self.task_paths.processing_dir / f'temp_{part_index}.m3u8'
                
                cmd = [
                    'ffmpeg', '-y',
                    '-i', str(video_path),
                    '-c', 'copy',
                    '-f', 'hls',
                    '-hls_time', '10',
                    '-hls_list_size', '0',
                    '-hls_segment_type', 'mpegts',
                    '-hls_segment_filename', segment_pattern,
                    str(temp_playlist_path)
                ]
                
                process = await asyncio.create_subprocess_exec(
                    *cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                stdout, stderr = await process.communicate()
                
                if process.returncode != 0:
                    self.logger.error(f"FFmpeg 错误: {stderr.decode()}")
                    raise RuntimeError(f"FFmpeg 错误: {stderr.decode()}")

                # 加载并处理临时播放列表
                temp_m3u8 = m3u8.load(str(temp_playlist_path))
                
                # 添加不连续标记
                discontinuity_segment = m3u8.Segment(discontinuity=True)
                self.playlist.add_segment(discontinuity_segment)
                
                # 添加片段到播放列表
                for segment in temp_m3u8.segments:
                    segment.uri = f"segments/{self.task_id}/{Path(segment.uri).name}"
                    self.playlist.segments.append(segment)
                
                self.sequence_number += len(temp_m3u8.segments)
                self.has_segments = True
                self._save_playlist()
                
            finally:
                if os.path.exists(str(temp_playlist_path)):
                    try:
                        os.unlink(str(temp_playlist_path))
                    except Exception as e:
                        self.logger.warning(f"清理临时文件失败: {e}")
    
    async def finalize_playlist(self) -> None:
        """标记播放列表为完成状态"""
        # 只有当播放列表中有片段时才标记为结束
        if self.has_segments:
            self.playlist.is_endlist = True
            self._save_playlist()
            self.logger.info("播放列表已保存，并标记为完成状态")
        else:
            self.logger.warning("播放列表为空，不标记为结束状态")

================
File: core/media_mixer.py
================
import numpy as np
import logging
import soundfile as sf
import os
import shutil
import asyncio
from contextlib import ExitStack
from tempfile import NamedTemporaryFile
from pathlib import Path
from typing import Optional, List
from utils.decorators import handle_errors

logger = logging.getLogger(__name__)

class MediaMixer:
    def __init__(self, config, sample_rate: int):
        self.config = config
        self.sample_rate = sample_rate
        self.max_val = 1.0
        self.overlap = self.config.AUDIO_OVERLAP
        self.vocals_volume = self.config.VOCALS_VOLUME
        self.background_volume = self.config.BACKGROUND_VOLUME
        self.full_audio_buffer = np.array([], dtype=np.float32)

    @handle_errors(logger)
    async def mixed_media_maker(self, sentences, task_state=None, output_path=None):
        """
        处理一批句子的音频和视频
        :param sentences: 要处理的句子列表
        :param task_state: 任务状态对象
        :param output_path: 输出路径
        """
        if not sentences:
            logger.warning("接收到空的句子列表")
            return False

        full_audio = np.array([], dtype=np.float32)

        # 获取当前分段的索引和媒体文件
        segment_index = sentences[0].segment_index
        segment_files = task_state.segment_media_files.get(segment_index)
        if not segment_files:
            logger.error(f"找不到分段 {segment_index} 的媒体文件")
            return False

        # 构建音频数据
        for sentence in sentences:
            if sentence.generated_audio is not None:
                audio_data = np.asarray(sentence.generated_audio, dtype=np.float32)
                if len(full_audio) > 0:
                    audio_data = self._apply_fade_effect(audio_data)
                full_audio = np.concatenate((full_audio, audio_data))
            else:
                logger.warning(
                    f"句子音频生成失败: '{sentence.raw_text[:30]}...', "
                    f"UUID: {sentence.model_input.get('uuid', 'unknown')}"
                )

        if len(full_audio) == 0:
            logger.error("没有有效的音频数据")
            return False

        # 计算当前批次的时间信息
        start_time = 0.0 if sentences[0].is_first else (sentences[0].adjusted_start - sentences[0].segment_start * 1000) / 1000.0
        duration = sum(s.adjusted_duration for s in sentences) / 1000.0  # 转换为秒

        # 混合背景音频
        background_audio_path = segment_files['background']
        if background_audio_path is not None:
            full_audio = self._mix_with_background(background_audio_path, start_time, duration, full_audio)
            full_audio = self._normalize_audio(full_audio)

        self.full_audio_buffer = np.concatenate((self.full_audio_buffer, full_audio))

        # 处理视频
        video_path = segment_files['video']
        if video_path:
            await self._add_video_segment(video_path, start_time, duration, full_audio, output_path)
            return True

        return False

    def _apply_fade_effect(self, audio_data: np.ndarray) -> np.ndarray:
        """应用淡入淡出效果，自然处理重叠"""
        if audio_data is None or len(audio_data) == 0:
            return np.array([], dtype=np.float32)
        
        if len(audio_data) > self.overlap * 2:
            audio_data = audio_data.copy()
            fade_in = np.linspace(0, 1, self.overlap)
            fade_out = np.linspace(1, 0, self.overlap)
            
            # 应用淡入淡出效果
            audio_data[:self.overlap] *= fade_in
            audio_data[-self.overlap:] *= fade_out
            
            # 当连接到前一个音频时，淡入部分会自然地与前一个音频的淡出部分混合
            if len(self.full_audio_buffer) > 0:
                overlap_region = self.full_audio_buffer[-self.overlap:]
                audio_data[:self.overlap] = np.add(
                    overlap_region,
                    audio_data[:self.overlap],
                    dtype=np.float32
                )
        return audio_data

    def _mix_with_background(self, background_audio_path: str, start_time: float, duration: float, audio_data: np.ndarray) -> np.ndarray:
        """混合背景音频与语音
        
        Args:
            background_audio_path: 背景音频文件路径
            start_time: 开始时间（秒）
            duration: 持续时间（秒）
            audio_data: 语音音频数据
            
        Returns:
            np.ndarray: 混合后的音频数据
        """
        # 读取背景音频
        background_audio, _ = sf.read(background_audio_path)
        background_audio = np.asarray(background_audio, dtype=np.float32)
        
        # 计算目标长度（采样点数）
        target_length = int(duration * self.sample_rate)
        
        # 截取指定时间范围的背景音频
        start_sample = int(start_time * self.sample_rate)
        end_sample = start_sample + target_length
        background_segment = background_audio[start_sample:end_sample]
        
        # 确保音频数据长度一致
        result = np.zeros(target_length, dtype=np.float32)
        audio_length = min(len(audio_data), target_length)
        background_length = min(len(background_segment), target_length)
        
        # 混合音频
        if audio_length > 0:
            result[:audio_length] = audio_data[:audio_length] * self.vocals_volume
        if background_length > 0:
            result[:background_length] += background_segment[:background_length] * self.background_volume
        
        return result

    def _normalize_audio(self, audio_data: np.ndarray) -> np.ndarray:
        """音频归一化处理"""
        if audio_data is None or len(audio_data) == 0:
            return np.array([], dtype=np.float32)
        
        max_val = np.abs(audio_data).max()
        if max_val > self.max_val:
            audio_data = audio_data * (self.max_val / max_val)
        return audio_data

    @handle_errors(logger)
    async def _add_video_segment(self, video_path: str, start_time: float, duration: float, audio_data: np.ndarray, output_path: str) -> None:
        """添加视频片段"""
        if not os.path.exists(video_path):
            logger.error("视频文件不存在")
            raise FileNotFoundError("视频文件不存在")
        
        if audio_data is None or len(audio_data) == 0:
            logger.error("无有效音频数据")
            raise ValueError("无有效音频数据")
        
        if duration <= 0:
            logger.error("无效的持续时间")
            raise ValueError("无效的持续时间")

        with ExitStack() as stack:
            temp_video = stack.enter_context(NamedTemporaryFile(suffix='.mp4'))
            temp_audio = stack.enter_context(NamedTemporaryFile(suffix='.wav'))

            end_time = start_time + duration
            
            # 提取视频片段
            cmd = [
                'ffmpeg', '-y',
                '-i', video_path,
                '-ss', str(start_time),
                '-to', str(end_time),
                '-c:v', 'libx264',
                '-preset', 'superfast',
                '-an',
                '-vsync', 'vfr',
                temp_video.name
            ]
            await self._run_ffmpeg_command(cmd)

            # 保存音频
            await asyncio.to_thread(sf.write, temp_audio.name, audio_data, self.sample_rate)

            # 合并视频和新音频
            cmd = [
                'ffmpeg', '-y',
                '-i', temp_video.name,
                '-i', temp_audio.name,
                '-c:v', 'copy',
                '-c:a', 'aac',
                output_path
            ]
            await self._run_ffmpeg_command(cmd)

    @handle_errors(logger)
    async def _run_ffmpeg_command(self, command: List[str]) -> None:
        """异步执行 FFmpeg 命令"""
        process = await asyncio.create_subprocess_exec(
            *command,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        stdout, stderr = await process.communicate()
        
        if process.returncode != 0:
            raise RuntimeError(f"FFmpeg 命令执行失败: {stderr.decode()}")

    async def reset(self):
        """重置混音器状态"""
        self.full_audio_buffer = np.array([], dtype=np.float32)
        logger.debug("已重置 mixer 状态")

================
File: core/model_in.py
================
import os
import logging
import torch
import numpy as np
import librosa
from typing import List, Optional
import asyncio
from concurrent.futures import ThreadPoolExecutor

class ModelIn:
    def __init__(self, cosy_model,
                 max_workers: Optional[int] = None,
                 max_concurrent_tasks: int = 4):
        """
        Args:
            cosy_model: 带有 frontend 等属性的cosyvoice对象
            max_workers: 线程池线程数，不指定时默认为 CPU 核心数
            max_concurrent_tasks: 同时并发处理的句子数上限 (Semaphore)
        """
        self.cosy_frontend = cosy_model.frontend
        self.cosy_sample_rate = cosy_model.sample_rate
        self.logger = logging.getLogger(__name__)

        self.speaker_cache = {}
        self.max_val = 0.8  # 最大音量阈值

        cpu_count = os.cpu_count() or 1
        self.max_workers = min(max_workers or cpu_count, cpu_count)
        self.executor = ThreadPoolExecutor(max_workers=self.max_workers)
        self.semaphore = asyncio.Semaphore(max_concurrent_tasks)

        self.logger.info(
            f"ModelIn initialized with max_workers={self.max_workers}, "
            f"max_concurrent_tasks={max_concurrent_tasks}"
        )

    def postprocess(self, speech, top_db=60, hop_length=220, win_length=440):
        """
        音频后处理: 去除多余静音, 音量限制, 拼接短静音等
        """
        speech, _ = librosa.effects.trim(
            speech, top_db=top_db,
            frame_length=win_length,
            hop_length=hop_length
        )
        if speech.abs().max() > self.max_val:
            speech = speech / speech.abs().max() * self.max_val
        
        # 拼接 0.2s 静音
        speech = torch.concat([speech, torch.zeros(1, int(self.cosy_sample_rate * 0.2))], dim=1)
        return speech

    def _update_text_features_sync(self, sentence):
        """
        同步方法: 更新文本特征到 sentence.model_input
        """
        try:
            tts_text = sentence.trans_text
            normalized_segments = self.cosy_frontend.text_normalize(tts_text, split=True)

            segment_tokens = []
            segment_token_lens = []

            for seg in normalized_segments:
                txt, txt_len = self.cosy_frontend._extract_text_token(seg)
                segment_tokens.append(txt)
                segment_token_lens.append(txt_len)

            sentence.model_input['text'] = segment_tokens
            sentence.model_input['text_len'] = segment_token_lens
            sentence.model_input['normalized_text_segments'] = normalized_segments

            self.logger.debug(f"成功更新文本特征: {normalized_segments}")
            return sentence
        except Exception as e:
            self.logger.error(f"更新文本特征失败: {str(e)}")
            raise

    def _process_sentence_sync(self, sentence, reuse_speaker=False, reuse_uuid=False):
        """
        同步核心方法, 通过布尔参数控制:
          1) reuse_speaker=False => 如果 speaker_id 不在缓存中, 则执行 postprocess + frontend_cross_lingual
          2) reuse_uuid=False    => 重置 model_input['uuid'] = "", 适合“首次处理”
             reuse_uuid=True     => 保留/复用原来的 UUID

        整个流程:
          - 如果不复用 speaker, 检查 speaker_cache, 做音频处理 + 前端特征
          - 根据 reuse_uuid 决定是否重置 sentence.model_input['uuid']
          - 更新文本特征
        """
        speaker_id = sentence.speaker_id

        # 1) Speaker处理
        if not reuse_speaker:
            if speaker_id not in self.speaker_cache:
                # 首次处理或无缓存时
                processed_audio = self.postprocess(sentence.audio)
                self.speaker_cache[speaker_id] = self.cosy_frontend.frontend_cross_lingual(
                    "",  # 空字符串占位
                    processed_audio,
                    self.cosy_sample_rate
                )
            # 将speaker缓存复制到model_input
            speaker_features = self.speaker_cache[speaker_id].copy()
            sentence.model_input = speaker_features

        # 2) UUID处理
        if not reuse_uuid:
            # 不复用 => 重置
            sentence.model_input['uuid'] = ""

        # 3) 更新文本特征
        self._update_text_features_sync(sentence)
        return sentence

    async def _process_sentence_async(self, sentence, reuse_speaker=False, reuse_uuid=False):
        """
        将同步逻辑封装到线程池中执行
        """
        loop = asyncio.get_event_loop()
        async with self.semaphore:
            return await loop.run_in_executor(
                self.executor,
                self._process_sentence_sync,
                sentence,
                reuse_speaker,
                reuse_uuid
            )

    async def modelin_maker(self,
                            sentences,
                            reuse_speaker=False,
                            reuse_uuid=False,
                            batch_size=3):
        """
        统一的对外接口，用于处理句子（首次 or 仅更新文本特征）:
          - reuse_speaker=False, reuse_uuid=False 表示“首次处理”(含 speaker)
          - reuse_speaker=True,  reuse_uuid=True  表示“仅更新文本特征”

        其他组合也可自由使用:
          - reuse_speaker=True,  reuse_uuid=False => 复用 speaker, 但重新分配 UUID
          - reuse_speaker=False, reuse_uuid=True  => 不复用 speaker(重做前端), 但保留 UUID

        常见用法示例:
          1) 首次处理:
             async for batch in self.modelin_maker(sentences, batch_size=3):
                 yield batch
          2) 只更新文本特征(重试场景):
             async for batch in self.modelin_maker(sentences, reuse_speaker=True, reuse_uuid=True, batch_size=3):
                 ...
        """
        if not sentences:
            self.logger.warning("modelin_maker: 收到空的句子列表")
            return

        tasks = [
            asyncio.create_task(
                self._process_sentence_async(s, reuse_speaker, reuse_uuid)
            )
            for s in sentences
        ]

        try:
            results = []
            for i, task in enumerate(tasks, start=1):
                updated_sentence = await task
                results.append(updated_sentence)

                if i % batch_size == 0:
                    yield results
                    results = []

            if results:
                yield results

        except Exception as e:
            self.logger.error(f"modelin_maker处理失败: {str(e)}")
            raise

        finally:
            # 如果不复用 speaker, 说明是“首次完整处理”，用完后清缓存
            if not reuse_speaker:
                self.speaker_cache.clear()
                self.logger.debug("modelin_maker: 已清理 speaker_cache")

================
File: core/sentence_tools.py
================
# speech_processing.py
import os
import torch
import torchaudio
import numpy as np
from typing import List, Tuple, Dict, Any
from dataclasses import dataclass, field
from pathlib import Path
from config import Config

# 定义数据结构
Token = int
Timestamp = Tuple[float, float]
SpeakerSegment = Tuple[float, float, int]

@dataclass
class Sentence:
    """
    句子数据结构
    
    Attributes:
        raw_text: 原始句子文本
        trans_text: 翻译后的文本
        start: 开始时间（毫秒）
        end: 结束时间（毫秒）
        speaker_id: 说话人ID
        sentence_id: 句子ID（全局唯一递增）
        audio: 音频数据
        target_duration: 目标音频长度（毫秒）
        duration: 实际音频长度（毫秒）
        diff: 时长差异（毫秒）
        silence_duration: 静音时长（毫秒）
        speed: 播放速度
        speaker_changed: 说话人是否改变
        is_first: 是否为第一个句子
        is_last: 是否为最后一个句子
        model_input: 用于TTS生成的输入数据
        generated_audio: 存储生成的音频数据
        adjusted_start: 调整后的开始时间（毫秒）
        adjusted_duration: 调整后的时长（毫秒）
        segment_index: 所属分段的索引
        segment_start: 所属分段的起始时间（秒）
        task_id: 任务ID
    """
    raw_text: str
    start: float  # 单位：毫秒
    end: float  # 单位：毫秒
    speaker_id: int
    trans_text: str = field(default="")
    sentence_id: int = field(default=-1)
    audio: torch.Tensor = field(default=None)
    target_duration: float = field(default=None)  # 单位：毫秒
    duration: float = field(default=0.0)
    diff: float = field(default=0.0)
    adjusted_duration: float = field(default=0.0)  # 单位：毫秒
    silence_duration: float = field(default=0.0)
    speed: float = field(default=1.0)
    is_first: bool = field(default=False)
    is_last: bool = field(default=False)
    model_input: Dict = field(default_factory=dict)  # 用于存储与TTS相关的数据
    generated_audio: np.ndarray = field(default=None)  # 存储生成的音频数据
    adjusted_start: float = field(default=0.0)  # 单位：毫秒
    segment_index: int = field(default=-1)  # 所属分段的索引
    segment_start: float = field(default=0.0)  # 所属分段的起始时间（秒）
    task_id: str = field(default="")

def tokens_timestamp_sentence(tokens: List[Token], timestamps: List[Timestamp], speaker_segments: List[SpeakerSegment], tokenizer: Any, config: Config) -> List[Tuple[List[Token], List[Timestamp], int]]:
    sentences = []
    current_tokens = []
    current_timestamps = []
    token_index = 0

    for segment in speaker_segments:
        seg_start_ms = int(segment[0] * 1000)
        seg_end_ms = int(segment[1] * 1000)
        speaker_id = segment[2]

        while token_index < len(tokens):
            token = tokens[token_index]
            token_start, token_end = timestamps[token_index]

            if token_start >= seg_end_ms:
                break
            if token_end <= seg_start_ms:
                token_index += 1
                continue

            current_tokens.append(token)
            current_timestamps.append(timestamps[token_index])
            token_index += 1

            if token in config.STRONG_END_TOKENS and len(current_tokens) <= config.MIN_SENTENCE_LENGTH:
                if sentences:
                    # 计算当前句子与前一个句子的时间差
                    previous_end_time = sentences[-1][1][-1][1]
                    current_start_time = current_timestamps[0][0]
                    time_gap = current_start_time - previous_end_time

                    if time_gap > config.SHORT_SENTENCE_MERGE_THRESHOLD_MS:
                        continue

                    # 将当前短句子与前一个句子合并
                    sentences[-1] = (
                        sentences[-1][0] + current_tokens[:],
                        sentences[-1][1] + current_timestamps[:],
                        sentences[-1][2]
                    )
                    current_tokens.clear()
                    current_timestamps.clear()
                continue

            if (token in config.STRONG_END_TOKENS or len(current_tokens) > config.MAX_TOKENS_PER_SENTENCE):
                sentences.append((current_tokens[:], current_timestamps[:], speaker_id))
                current_tokens.clear()
                current_timestamps.clear()

        if current_tokens:
            if len(current_tokens) >= config.MIN_SENTENCE_LENGTH or not sentences:
                sentences.append((current_tokens[:], current_timestamps[:], speaker_id))
                current_tokens.clear()
                current_timestamps.clear()
            else:
                continue

    if current_tokens:
        if len(current_tokens) >= config.MIN_SENTENCE_LENGTH or not sentences:
            sentences.append((current_tokens[:], current_timestamps[:], speaker_id))
            current_tokens.clear()
            current_timestamps.clear()
        else:
            sentences[-1] = (
                sentences[-1][0] + current_tokens[:],
                sentences[-1][1] + current_timestamps[:],
                sentences[-1][2]
            )
            current_tokens.clear()
            current_timestamps.clear()

    return sentences

def merge_sentences(raw_sentences: List[Tuple[List[Token], List[Timestamp], int]], 
                   tokenizer: Any,
                   input_duration: float,  # 输入音频总长度(ms)
                   config: Config) -> List[Sentence]:
    merged_sentences = []
    current = None
    current_tokens_count = 0

    for tokens, timestamps, speaker_id in raw_sentences:
        time_gap = timestamps[0][0] - current.end if current else float('inf')
        
        if (current and 
            current.speaker_id == speaker_id and 
            current_tokens_count + len(tokens) <= config.MAX_TOKENS_PER_SENTENCE and
            time_gap <= config.MAX_GAP_MS):
            current.raw_text += tokenizer.decode(tokens)
            current.end = timestamps[-1][1]
            current_tokens_count += len(tokens)
        else:
            if current:
                current.target_duration = timestamps[0][0] - current.start
                merged_sentences.append(current)
            
            text = tokenizer.decode(tokens)
            current = Sentence(
                raw_text=text, 
                start=timestamps[0][0], 
                end=timestamps[-1][1], 
                speaker_id=speaker_id,
            )
            current_tokens_count = len(tokens)

    if current:
        current.target_duration = input_duration - current.start  # 最后一个句子的目标时长直到输入结束
        merged_sentences.append(current)

    # 标记第一个和最后一个句子
    if merged_sentences:
        merged_sentences[0].is_first = True
        merged_sentences[-1].is_last = True

    return merged_sentences

def extract_audio(sentences: List[Sentence], speech: torch.Tensor, sr: int, config: Config) -> List[Sentence]:
    """提取说话人音频并保存
    
    Args:
        sentences: 句子列表
        speech: 输入音频
        sr: 采样率
        config: 配置对象
        
    Returns:
        List[Sentence]: 处理后的句子列表
    """
    target_samples = int(config.SPEAKER_AUDIO_TARGET_DURATION * sr)
    speech = speech.unsqueeze(0) if speech.dim() == 1 else speech

    # 按说话人ID分组并计算每段音频长度
    speaker_segments: Dict[int, List[Tuple[int, int, int]]] = {}  # speaker_id -> List[(start_sample, end_sample, sentence_idx)]
    for idx, s in enumerate(sentences):
        start_sample = int(s.start * sr / 1000)
        end_sample = int(s.end * sr / 1000)
        speaker_segments.setdefault(s.speaker_id, []).append((start_sample, end_sample, idx))

    # 为每个说话人选择最长的音频片段
    speaker_audio_cache: Dict[int, torch.Tensor] = {}  # 缓存每个说话人的音频片段

    for speaker_id, segments in speaker_segments.items():
        # 按音频长度降序排序
        segments.sort(key=lambda x: x[1] - x[0], reverse=True)

        # 获取最长的音频片段
        longest_start, longest_end, _ = segments[0]

        # 计算需要忽略的起始样本数
        ignore_samples = int(0.5 * sr)

        # 计算调整后的起始位置
        adjusted_start = longest_start + ignore_samples

        # 计算从调整后的起始位置开始的可用长度
        available_length_adjusted = longest_end - adjusted_start

        if available_length_adjusted > 0:
            # 如果忽略前 0.5 秒后还有足够的音频
            audio_length = min(target_samples, available_length_adjusted)
            speaker_audio = speech[:, adjusted_start:adjusted_start + audio_length]
        else:
            # 如果忽略前 0.5 秒后没有足够的音频，则使用原始的音频片段长度
            available_length_original = longest_end - longest_start
            audio_length = min(target_samples, available_length_original)
            speaker_audio = speech[:, longest_start:longest_start + audio_length]

        # 缓存该说话人的音频片段
        speaker_audio_cache[speaker_id] = speaker_audio

    # 为所有句子音频，使用引用而不是复制
    for sentence in sentences:
        sentence.audio = speaker_audio_cache.get(sentence.speaker_id)

    # 保存每个说话人的音频文件
    output_dir = Path(config.TASKS_DIR) / sentences[0].task_id / 'speakers'
    output_dir.mkdir(parents=True, exist_ok=True)

    # 只保存每个说话人的一个音频文件
    for speaker_id, audio in speaker_audio_cache.items():
        if audio is not None:
            output_path = output_dir / f'speaker_{speaker_id}.wav'
            torchaudio.save(str(output_path), audio, sr)

    return sentences

def get_sentences(tokens: List[Token],
                  timestamps: List[Timestamp],
                  speech: torch.Tensor,
                  tokenizer: Any,
                  sd_time_list: List[SpeakerSegment],
                  sample_rate: int = 16000,
                  config: Config = None) -> List[Sentence]:
    """
    主处理函数，整合所有步骤。
    """
    if config is None:
        config = Config()  # 使用默认配置

    # 计算输入音频的总长度(ms)
    input_duration = (speech.shape[-1] / sample_rate) * 1000

    raw_sentences = tokens_timestamp_sentence(tokens, timestamps, sd_time_list, tokenizer, config)
    merged_sentences = merge_sentences(raw_sentences, tokenizer, input_duration, config)
    sentences_with_audio = extract_audio(merged_sentences, speech, sample_rate, config)

    return sentences_with_audio

================
File: core/tts_token_gener.py
================
import logging
import asyncio
from concurrent.futures import ThreadPoolExecutor
import uuid
import torch
import os

class TTSTokenGenerator:
    def __init__(self, cosyvoice_model, Hz=25, max_workers=None):
        self.cosyvoice_model = cosyvoice_model.model
        self.Hz = Hz
        cpu_count = os.cpu_count()
        self.max_workers = min(max_workers or cpu_count, cpu_count)
        self.executor = None
        self.logger = logging.getLogger(__name__)

    async def tts_token_maker(self, sentences, reuse_uuid=False):
        """
        并发为句子生成 TTS token。
        :param reuse_uuid: 若为 True，则复用句子已有的 uuid；若没有，则自动生成。
                           若为 False，则为每个句子生成新的 uuid。
        """
        try:
            if self.executor is None:
                self.executor = ThreadPoolExecutor(max_workers=self.max_workers)
                self.logger.debug(f"创建线程池, max_workers={self.max_workers}")

            loop = asyncio.get_event_loop()
            tasks = []

            for s in sentences:
                current_uuid = (
                    s.model_input.get('uuid') if reuse_uuid and s.model_input.get('uuid')
                    else str(uuid.uuid1())
                )
                task = loop.run_in_executor(
                    self.executor,
                    self._generate_tts_single,
                    s,
                    current_uuid
                )
                tasks.append(task)

            processed = await asyncio.gather(*tasks)

            # 检查生成结果
            for sen in processed:
                if not sen.model_input.get('segment_speech_tokens'):
                    self.logger.error(f"TTS token 生成失败: {sen.trans_text}")

            return processed

        except Exception as e:
            self.logger.error(f"TTS token 生成失败: {e}")
            raise

    def _generate_tts_single(self, sentence, main_uuid):
        """
        同步方法, 生成分段 tokens, 计算时长
        """
        model_input = sentence.model_input
        segment_tokens_list = []
        segment_uuids = []
        total_token_count = 0

        try:
            for i, (text, text_len) in enumerate(zip(model_input['text'], model_input['text_len'])):
                seg_uuid = f"{main_uuid}_seg_{i}"
                with self.cosyvoice_model.lock:
                    self.cosyvoice_model.tts_speech_token_dict[seg_uuid] = []
                    self.cosyvoice_model.llm_end_dict[seg_uuid] = False
                    if hasattr(self.cosyvoice_model, 'mel_overlap_dict'):
                        self.cosyvoice_model.mel_overlap_dict[seg_uuid] = None
                    self.cosyvoice_model.hift_cache_dict[seg_uuid] = None

                # LLM job 生成 tokens
                self.cosyvoice_model.llm_job(
                    text,
                    model_input.get('prompt_text', torch.zeros(1, 0, dtype=torch.int32)),
                    model_input.get('llm_prompt_speech_token', torch.zeros(1, 0, dtype=torch.int32)),
                    model_input.get('llm_embedding', torch.zeros(0, 192)),
                    seg_uuid
                )

                seg_tokens = self.cosyvoice_model.tts_speech_token_dict[seg_uuid]
                segment_tokens_list.append(seg_tokens)
                segment_uuids.append(seg_uuid)
                total_token_count += len(seg_tokens)

            total_duration_s = total_token_count / self.Hz
            sentence.duration = total_duration_s * 1000

            model_input['segment_speech_tokens'] = segment_tokens_list
            model_input['segment_uuids'] = segment_uuids
            model_input['uuid'] = main_uuid

            self.logger.info(
                f"TTS token 生成完成 (UUID={main_uuid}, 时长={total_duration_s:.2f}s, 段数={len(segment_uuids)})"
            )
            return sentence

        except Exception as e:
            self.logger.error(f"生成失败 (UUID={main_uuid}): {e}")
            # 失败时, 清理缓存
            with self.cosyvoice_model.lock:
                for seg_uuid in segment_uuids:
                    self.cosyvoice_model.tts_speech_token_dict.pop(seg_uuid, None)
                    self.cosyvoice_model.llm_end_dict.pop(seg_uuid, None)
                    self.cosyvoice_model.hift_cache_dict.pop(seg_uuid, None)
                    if hasattr(self.cosyvoice_model, 'mel_overlap_dict'):
                        self.cosyvoice_model.mel_overlap_dict.pop(seg_uuid, None)
            raise

================
File: utils/decorators.py
================
import logging
import functools
from typing import Callable, Any, Optional, Coroutine, AsyncGenerator, TypeVar, Union, Literal
import asyncio
import time

logger = logging.getLogger(__name__)

T = TypeVar('T')
WorkerResult = Union[T, AsyncGenerator[T, None]]
WorkerMode = Literal['base', 'stream']

def handle_errors(custom_logger: Optional[logging.Logger] = None) -> Callable:
    """
    错误处理装饰器。可应用于需要统一捕获日志的异步函数。
    """
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        async def wrapper(*args, **kwargs) -> Any:
            # 选用 custom_logger 或从 args[0].logger 获取
            actual_logger = custom_logger
            if not actual_logger and args and hasattr(args[0], 'logger'):
                actual_logger = args[0].logger

            if not actual_logger:
                actual_logger = logger

            start_time = time.time()
            try:
                result = await func(*args, **kwargs)
                elapsed = time.time() - start_time
                actual_logger.debug(f"{func.__name__} 正常结束，耗时 {elapsed:.2f}s")
                return result
            except Exception as e:
                elapsed = time.time() - start_time
                actual_logger.error(f"{func.__name__} 执行出错，耗时 {elapsed:.2f}s, 错误: {e}", exc_info=True)
                raise
        return wrapper
    return decorator

def worker_decorator(
    input_queue_attr: str,
    next_queue_attr: Optional[str] = None,
    worker_name: Optional[str] = None,
    mode: WorkerMode = 'base'
) -> Callable:
    """
    通用 Worker 装饰器，用于将一个异步函数变成“从指定队列读取数据 -> 处理 -> 放入下游队列”的流式流程。
    新增更多日志输出，如队列长度、处理耗时等，以便在后台看到详细信息。
    """
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        async def wrapper(self, task_state, *args, **kwargs):
            """
            self: 通常是 pipeline_scheduler.PipelineScheduler 实例
            task_state: 每个任务独立的状态
            *args, **kwargs: 可能的额外参数
            """
            worker_display_name = worker_name or func.__name__
            wlogger = getattr(self, 'logger', logger)

            # 队列引用
            input_queue = getattr(task_state, input_queue_attr)
            next_queue = getattr(task_state, next_queue_attr) if next_queue_attr else None

            wlogger.info(f"[{worker_display_name}] 启动 (TaskID={task_state.task_id}). "
                         f"输入队列: {input_queue_attr}, 下游队列: {next_queue_attr if next_queue_attr else '无'}")

            processed_count = 0  # 记录本Worker已处理多少item
            try:
                while True:
                    try:
                        queue_size_before = input_queue.qsize()
                        item = await input_queue.get()
                        if item is None:
                            # 结束信号
                            if next_queue:
                                await next_queue.put(None)
                            wlogger.info(f"[{worker_display_name}] 收到停止信号。已处理 {processed_count} 个item。")
                            break

                        # 记录处理前队列长度
                        wlogger.debug(f"[{worker_display_name}] 从 {input_queue_attr} 队列取出一个item. "
                                      f"队列剩余: {queue_size_before} -> (处理中)")

                        start_time = time.time()
                        if mode == 'stream':
                            async for result in func(self, item, task_state, *args, **kwargs):
                                if result is not None and next_queue:
                                    await next_queue.put(result)
                        else:
                            result = await func(self, item, task_state, *args, **kwargs)
                            if result is not None and next_queue:
                                await next_queue.put(result)

                        processed_count += 1
                        elapsed = time.time() - start_time
                        wlogger.debug(f"[{worker_display_name}] item处理完成，耗时 {elapsed:.2f}s. "
                                      f"TaskID={task_state.task_id}, 已处理计数: {processed_count}")

                    except asyncio.CancelledError:
                        wlogger.warning(f"[{worker_display_name}] 被取消 (TaskID={task_state.task_id}). "
                                        f"已处理 {processed_count} 个item")
                        if next_queue:
                            await next_queue.put(None)
                        break
                    except Exception as e:
                        wlogger.error(f"[{worker_display_name}] 发生异常: {e} (TaskID={task_state.task_id}). "
                                      f"已处理 {processed_count} 个item", exc_info=True)
                        # 出错后可选择是否结束下游
                        if next_queue:
                            await next_queue.put(None)
                        break
            finally:
                wlogger.info(f"[{worker_display_name}] 结束 (TaskID={task_state.task_id}). 共处理 {processed_count} 个item.")
        return wrapper
    return decorator

================
File: utils/media_utils.py
================
import logging
import asyncio
import numpy as np
import torch
import torchaudio
import librosa
import soundfile as sf
from pathlib import Path
from typing import List, Tuple, Dict, Union
import subprocess
from utils.decorators import handle_errors
from utils.temp_file_manager import TempFileManager

logger = logging.getLogger(__name__)

class MediaUtils:
    def __init__(self, config, audio_separator, target_sr: int = 24000):
        self.config = config
        self.target_sr = target_sr
        self.logger = logging.getLogger(__name__)
        self.audio_separator = audio_separator

    def normalize_and_resample(self, 
                             audio_input: Union[Tuple[int, np.ndarray], np.ndarray],
                             target_sr: int = None) -> np.ndarray:
        """音频标准化和重采样处理"""
        
        # 解析输入
        if isinstance(audio_input, tuple):
            fs, audio_input = audio_input
        else:
            fs = target_sr
        
        # 确保数据类型为 float32
        audio_input = audio_input.astype(np.float32)
        
        # 归一化到 [-1, 1]
        max_val = np.abs(audio_input).max()
        if max_val > 0:  # 避免除以0
            audio_input = audio_input / max_val
        
        # 转换为单声道
        if len(audio_input.shape) > 1:
            audio_input = audio_input.mean(axis=-1)
        
        # 重采样到目标采样率
        if fs != target_sr:
            audio_input = np.ascontiguousarray(audio_input)
            resampler = torchaudio.transforms.Resample(
                orig_freq=fs,
                new_freq=target_sr,
                dtype=torch.float32
            )
            audio_input = resampler(torch.from_numpy(audio_input)[None, :])[0].numpy()
        
        return audio_input

    @handle_errors(logger)
    async def _run_ffmpeg_command(self, cmd: List[str]) -> Tuple[bytes, bytes]:
        """执行 FFmpeg 命令"""
        process = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        stdout, stderr = await process.communicate()
        
        if process.returncode != 0:
            error_msg = stderr.decode() if stderr else "Unknown error"
            raise RuntimeError(f"FFmpeg 命令执行失败: {error_msg}")
            
        return stdout, stderr

    @handle_errors(logger)
    async def extract_audio(self, video_path: str, output_path: str, start: float = 0, duration: float = None) -> str:
        """从视频中提取音频
        
        Args:
            video_path: 视频文件路径
            output_path: 输出音频文件路径
            start: 开始时间（秒）
            duration: 持续时间（秒），不指定则处理到文件结束
        """
        cmd = [
            'ffmpeg', '-y',
            '-i', video_path,
            '-ss', str(start)
        ]
        if duration is not None:
            cmd.extend(['-t', str(duration)])
        
        cmd.extend([
            '-vn',
            '-acodec', 'pcm_f32le',
            '-ac', '1',
            output_path
        ])
        
        await self._run_ffmpeg_command(cmd)
        return output_path

    @handle_errors(logger)
    async def extract_video(self, video_path: str, output_path: str, start: float = 0, duration: float = None) -> str:
        """提取无声视频
        
        Args:
            video_path: 视频文件路径
            output_path: 输出视频文件路径
            start: 开始时间（秒）
            duration: 持续时间（秒），不指定则处理到文件结束
        """
        # 如果是从头开始提取，直接使用 -an 去除音频
        if start == 0:
            cmd = [
                'ffmpeg', '-y',
                '-i', video_path,
                '-an',  # 移除音频
                '-c:v', 'copy',  # 直接复制视频流，避免重新编码
                output_path
            ]
        else:
            # 如果需要从中间开始，先解码再编码以确保精确的时间点
            cmd = [
                'ffmpeg', '-y',
                '-i', video_path,
                '-ss', str(start)
            ]
            if duration is not None:
                cmd.extend(['-t', str(duration)])
            
            cmd.extend([
                '-an',  # 移除音频
                '-c:v', 'libx264',  # 使用 x264 编码器
                '-preset', 'ultrafast',  # 使用最快的编码预设
                '-crf', '18',  # 使用较高质量的设置，因为这是中间片段
                '-tune', 'fastdecode',  # 优化解码速度
                output_path
            ])
        
        self.logger.debug(f"执行 FFmpeg 命令: {' '.join(cmd)}")
        await self._run_ffmpeg_command(cmd)
        return output_path

    @handle_errors(logger)
    async def get_audio_segments(self, duration: float) -> List[Tuple[float, float]]:
        """获取音频分段信息
        
        Args:
            duration: 音频时长（秒）
            
        Returns:
            List[Tuple[float, float]]: 分段列表，每个元素为 (开始时间, 持续时间)
        """
        segment_length = self.config.SEGMENT_MINUTES * 60
        min_length = self.config.MIN_SEGMENT_MINUTES * 60
        
        if duration <= min_length:
            return [(0, duration)]
        
        segments = []
        current_pos = 0
        
        while current_pos < duration:
            remaining_duration = duration - current_pos
            
            if remaining_duration <= segment_length:
                if remaining_duration < min_length and segments:
                    # 如果剩余时长小于最小分段长度且不是第一段，则并入前一段
                    start = segments[-1][0]
                    new_duration = duration - start
                    segments[-1] = (start, new_duration)
                else:
                    segments.append((current_pos, remaining_duration))
                break
            
            segments.append((current_pos, segment_length))
            current_pos += segment_length
        
        return segments

    @handle_errors(logger)
    async def extract_segment(self, video_path: str, start: float, duration: float, output_dir: Path, segment_index: int) -> Dict[str, Union[str, float]]:
        """从视频中提取并分离片段
        
        Args:
            video_path: 视频文件路径
            start: 开始时间（秒）
            duration: 持续时间（秒）
            output_dir: 输出目录
            segment_index: 分段索引
            
        Returns:
            Dict[str, Union[str, float]]: 包含分离后的媒体文件路径
        """
        temp_files = {}
        try:
            # 为当前分段创建输出路径
            silent_video = str(output_dir / f"video_silent_{segment_index}.mp4")
            full_audio = str(output_dir / f"audio_full_{segment_index}.wav")
            vocals_audio = str(output_dir / f"vocals_{segment_index}.wav")
            background_audio = str(output_dir / f"background_{segment_index}.wav")
            
            # 并行提取音频和无声视频
            await asyncio.gather(
                self.extract_audio(video_path, full_audio, start, duration),
                self.extract_video(video_path, silent_video, start, duration)
            )
            
            # 使用音频分离器处理音频
            vocals, background, sr = self.audio_separator.separate_audio(full_audio)
            
            # 重采样背景音频
            background = self.normalize_and_resample((sr, background), self.target_sr)
            
            # 保存人声和背景音频
            sf.write(vocals_audio, vocals, sr, subtype='FLOAT')
            sf.write(background_audio, background, self.target_sr, subtype='FLOAT')
            
            # 计算人声音频时长
            segment_duration = len(vocals) / sr
            
            # 删除原始音频文件
            Path(full_audio).unlink()
            
            # 将临时文件路径添加到返回字典
            temp_files = {
                'video': silent_video,
                'vocals': vocals_audio,
                'background': background_audio,
                'duration': segment_duration
            }
            
            return temp_files
            
        except Exception as e:
            # 发生错误时清理已创建的临时文件
            for file_path in temp_files.values():
                if isinstance(file_path, str) and Path(file_path).exists():
                    Path(file_path).unlink()
            raise

    @handle_errors(logger)
    async def get_video_duration(self, video_path: str) -> float:
        """获取视频时长
        
        Args:
            video_path: 视频文件路径
            
        Returns:
            float: 视频时长（秒）
        """
        cmd = [
            'ffprobe',
            '-v', 'error',
            '-show_entries', 'format=duration',
            '-of', 'default=noprint_wrappers=1:nokey=1',
            video_path
        ]
        
        process = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        stdout, stderr = await process.communicate()
        
        if process.returncode != 0:
            raise RuntimeError(f"获取视频时长失败: {stderr.decode()}")
        
        duration = float(stdout.decode().strip())
        return duration

================
File: utils/sentence_logger.py
================
import logging
import json
from pathlib import Path
from typing import List, Dict, Any
import asyncio
from utils.decorators import handle_errors

logger = logging.getLogger(__name__)

class SentenceLogger:
    """句子日志记录器"""
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self._lock = asyncio.Lock()
    
    def _format_sentence(self, sentence: Dict[str, Any]) -> Dict[str, Any]:
        """格式化句子信息"""
        return {
            'id': getattr(sentence, 'sentence_id', -1),
            'start_time': getattr(sentence, 'start', 0),
            'end_time': getattr(sentence, 'end', 0),
            'text': getattr(sentence, 'text', ''),
            'translation': getattr(sentence, 'translation', ''),
            'duration': getattr(sentence, 'duration', 0),
            'speaker_id': getattr(sentence, 'speaker_id', 0),
            'speaker_similarity': getattr(sentence, 'speaker_similarity', 0),
            'speaker_embedding': getattr(sentence, 'speaker_embedding', []).tolist() if hasattr(getattr(sentence, 'speaker_embedding', []), 'tolist') else getattr(sentence, 'speaker_embedding', [])
        }
    
    @handle_errors(logger)
    async def save_sentences(self, sentences: List[Dict[str, Any]], output_path: Path, task_id: str) -> None:
        """保存句子信息到文件
        
        Args:
            sentences: 句子列表
            output_path: 输出文件路径
            task_id: 任务ID
        """
        async with self._lock:
            try:
                # 格式化句子信息
                formatted_sentences = [self._format_sentence(s) for s in sentences]
                
                # 创建输出目录
                output_path.parent.mkdir(parents=True, exist_ok=True)
                
                # 保存到文件
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(formatted_sentences, f, ensure_ascii=False, indent=2)
                
                self.logger.debug(f"已保存 {len(sentences)} 个句子到 {output_path}")
                
            except Exception as e:
                self.logger.error(f"保存句子信息失败: {e}")
                raise

================
File: utils/task_state.py
================
from dataclasses import dataclass, field
from typing import Optional, Any
import asyncio
from utils.task_storage import TaskPaths

@dataclass
class TaskState:
    """
    每个任务的独立状态：包括队列、处理进度、分段信息等
    """
    task_id: str
    video_path: str
    task_paths: TaskPaths
    hls_manager: Any = None
    target_language: str = "zh"

    sentence_counter: int = 0
    current_time: float = 0
    batch_counter: int = 0
    segment_media_files: dict = field(default_factory=dict)

    # 每个任务的队列
    translation_queue: asyncio.Queue = field(default_factory=asyncio.Queue)
    modelin_queue: asyncio.Queue = field(default_factory=asyncio.Queue)
    tts_token_queue: asyncio.Queue = field(default_factory=asyncio.Queue)
    duration_align_queue: asyncio.Queue = field(default_factory=asyncio.Queue)
    audio_gen_queue: asyncio.Queue = field(default_factory=asyncio.Queue)
    mixing_queue: asyncio.Queue = field(default_factory=asyncio.Queue)

    # 第一段处理完成的同步
    mixing_complete: asyncio.Queue = field(default_factory=asyncio.Queue)
    first_segment_batch_count: int = 0
    first_segment_processed_count: int = 0

================
File: utils/task_storage.py
================
import shutil
import logging
import os
from pathlib import Path
from config import Config

logger = logging.getLogger(__name__)

class TaskPaths:
    def __init__(self, config: Config, task_id: str):
        self.config = config
        self.task_id = task_id

        self.task_dir = config.TASKS_DIR / task_id
        self.input_dir = self.task_dir / "input"
        self.processing_dir = self.task_dir / "processing"
        self.output_dir = self.task_dir / "output"

        self.segments_dir = config.PUBLIC_DIR / "segments" / task_id
        self.playlist_path = config.PUBLIC_DIR / "playlists" / f"playlist_{task_id}.m3u8"

        self.media_dir = self.processing_dir / "media"
        self.processing_segments_dir = self.processing_dir / "segments"

    def create_directories(self):
        dirs = [
            self.task_dir,
            self.input_dir,
            self.processing_dir,
            self.output_dir,
            self.segments_dir,
            self.media_dir,
            self.processing_segments_dir
        ]
        for d in dirs:
            d.mkdir(parents=True, exist_ok=True)
            logger.debug(f"[TaskPaths] 创建目录: {d}")

    async def cleanup(self, keep_output: bool = False):
        """
        清理任务目录
        """
        try:
            if keep_output:
                logger.info(f"[TaskPaths] 保留输出目录, 即将清理输入/processing/segments")
                dirs_to_clean = [self.input_dir, self.processing_dir, self.segments_dir]
                for d in dirs_to_clean:
                    if d.exists():
                        shutil.rmtree(d)
                        logger.debug(f"[TaskPaths] 已清理: {d}")
            else:
                logger.info(f"[TaskPaths] 全量清理任务目录: {self.task_dir}")
                if self.task_dir.exists():
                    shutil.rmtree(str(self.task_dir))
                    logger.debug(f"[TaskPaths] 已删除: {self.task_dir}")

                if self.segments_dir.exists():
                    shutil.rmtree(str(self.segments_dir))
                    logger.debug(f"[TaskPaths] 已删除: {self.segments_dir}")
        except Exception as e:
            logger.error(f"[TaskPaths] 清理任务目录失败: {e}", exc_info=True)
            raise

================
File: utils/temp_file_manager.py
================
import logging
from pathlib import Path
from typing import Set

logger = logging.getLogger(__name__)

class TempFileManager:
    """临时文件管理器"""
    def __init__(self, base_dir: Path):
        self.base_dir = base_dir
        self.temp_files: Set[Path] = set()
    
    def add_file(self, file_path: Path) -> None:
        """添加临时文件到管理器"""
        self.temp_files.add(Path(file_path))
    
    async def cleanup(self) -> None:
        """清理所有临时文件"""
        for file_path in self.temp_files:
            try:
                if file_path.exists():
                    file_path.unlink()
                    logger.debug(f"已删除临时文件: {file_path}")
            except Exception as e:
                logger.warning(f"清理临时文件失败: {file_path}, 错误: {e}")
        self.temp_files.clear()

================
File: __init__.py
================
# 空文件即可，标识这是一个 Python 包

================
File: .cursorignore
================
# 忽略模型文件夹，因为包含大量模型文件和第三方代码
models/

================
File: .env.example
================
# ================================
# .env.example
# ================================

# 【可选】修改 FLASK_ENV，默认使用 development 方便调试
FLASK_ENV=development

# 翻译模型选择 (可选值: deepseek, glm4, gemini)
TRANSLATION_MODEL=deepseek

# API Keys (请替换为实际的密钥)
ZHIPUAI_API_KEY=your_zhipuai_key_here
GEMINI_API_KEY=your_gemini_key_here
DEEPSEEK_API_KEY=your_deepseek_key_here

================
File: api.py
================
# ================================
# File: api.py
# ================================
import sys
from pathlib import Path
import logging
import uuid
import asyncio
from typing import Dict

import uvicorn
from fastapi import FastAPI, File, UploadFile, HTTPException, Request, Form, BackgroundTasks
from fastapi.responses import JSONResponse, FileResponse, HTMLResponse
from fastapi.templating import Jinja2Templates
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
import aiofiles

# 使用我们新的简化Config
from config import Config

# 初始化配置并创建必要目录
config = Config()
config.init_directories()

# 将系统路径加入 sys.path
sys.path.extend(config.SYSTEM_PATHS)

# 在这里配置全局日志: 直接DEBUG级别
logging.basicConfig(
    level=logging.DEBUG,
    format="%(levelname)s | %(asctime)s | %(name)s | L%(lineno)d | %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)
logger = logging.getLogger(__name__)

# 导入视频翻译器等模块
from video_translator import ViTranslator
from core.hls_manager import HLSManager
from utils.task_storage import TaskPaths

# 创建 FastAPI
app = FastAPI(debug=True)

# 添加 CORS 中间件
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # 在生产环境中应改为实际域名
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# 配置模板
current_dir = Path(__file__).parent
templates = Jinja2Templates(directory=str(current_dir / "templates"))

# 初始化翻译器（全局单例）
vi_translator = ViTranslator(config=config)

# 存储任务结果
task_results: Dict[str, dict] = {}


@app.get("/")
async def index(request: Request):
    """提供主页"""
    return templates.TemplateResponse("index.html", {"request": request})


@app.post("/upload")
async def upload_video(
    video: UploadFile = File(...),
    target_language: str = Form("zh")
):
    """处理视频上传"""
    try:
        if not video:
            raise HTTPException(status_code=400, detail="没有文件上传")
        
        # 检查文件类型
        if not video.content_type.startswith('video/'):
            raise HTTPException(status_code=400, detail="只支持视频文件")
            
        # 验证目标语言
        if target_language not in ["zh", "en", "ja", "ko"]:
            raise HTTPException(status_code=400, detail=f"不支持的目标语言: {target_language}")
        
        # 生成任务ID和创建任务路径
        task_id = str(uuid.uuid4())
        task_paths = TaskPaths(config, task_id)
        task_paths.create_directories()
        
        # 保存上传的文件
        video_path = task_paths.input_dir / f"original_{video.filename}"
        try:
            async with aiofiles.open(video_path, "wb") as f:
                content = await video.read()
                await f.write(content)
        except Exception as e:
            logger.error(f"保存文件失败: {str(e)}")
            await task_paths.cleanup()
            raise HTTPException(status_code=500, detail="文件保存失败")
        
        # 创建 HLS 管理器
        hls_manager = HLSManager(config, task_id, task_paths)
        
        # 创建后台任务，直接传入所需参数
        task = asyncio.create_task(vi_translator.trans_video(
            video_path=str(video_path),
            task_id=task_id,
            task_paths=task_paths,
            hls_manager=hls_manager,
            target_language=target_language
        ))
        
        # 存储任务信息
        task_results[task_id] = {
            "status": "processing",
            "message": "视频处理中",
            "progress": 0
        }
        
        # 设置任务完成回调
        async def on_task_complete(t):
            try:
                result = await t
                if result.get('status') == 'success':
                    task_results[task_id].update({
                        "status": "success",
                        "message": "处理完成",
                        "progress": 100
                    })
                    # 清理处理文件，保留输出(如有需要可在这里做)
                    # await task_paths.cleanup(keep_output=True)
                else:
                    task_results[task_id].update({
                        "status": "error",
                        "message": result.get('message', '处理失败'),
                        "progress": 0
                    })
                    # 清理所有文件
                    await task_paths.cleanup()
            except Exception as e:
                logger.error(f"任务处理失败: {str(e)}")
                task_results[task_id].update({
                    "status": "error",
                    "message": str(e),
                    "progress": 0
                })
                # 清理所有文件
                await task_paths.cleanup()
        
        task.add_done_callback(lambda t: asyncio.create_task(on_task_complete(t)))
        
        return {
            'status': 'processing',
            'task_id': task_id,
            'message': '视频上传成功，开始处理'
        }
    except HTTPException as e:
        raise e
    except Exception as e:
        logger.error(f"上传处理失败: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/task/{task_id}")
async def get_task_status(task_id: str):
    """获取任务状态"""
    result = task_results.get(task_id)
    if not result:
        return {
            "status": "error",
            "message": "任务不存在",
            "progress": 0
        }
    return result


# 挂载静态文件目录 (播放列表)
app.mount("/playlists", StaticFiles(directory=str(config.PUBLIC_DIR / "playlists")), name="playlists")


@app.get("/playlists/{task_id}/{filename}")
async def serve_playlist(task_id: str, filename: str):
    """服务 m3u8 播放列表文件"""
    try:
        playlist_path = config.PUBLIC_DIR / "playlists" / filename
        if not playlist_path.exists():
            logger.error(f"播放列表未找到: {playlist_path}")
            raise HTTPException(status_code=404, detail="播放列表未找到")
        
        logger.info(f"提供播放列表: {playlist_path}")
        return FileResponse(
            str(playlist_path), 
            media_type='application/vnd.apple.mpegurl',
            headers={
                "Cache-Control": "public, max-age=3600",  # 1小时的缓存
                "Access-Control-Allow-Origin": "*"
            }
        )
    except Exception as e:
        logger.error(f"服务播放列表失败: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/segments/{task_id}/{filename}")
async def serve_segments(task_id: str, filename: str):
    """服务视频片段文件"""
    try:
        segment_path = config.PUBLIC_DIR / "segments" / task_id / filename
        if not segment_path.exists():
            logger.error(f"片段文件未找到: {segment_path}")
            raise HTTPException(status_code=404, detail="片段文件未找到")
        
        logger.info(f"提供视频片段: {segment_path}")
        return FileResponse(
            str(segment_path),
            media_type='video/MP2T',
            headers={
                "Cache-Control": "no-cache, no-store, must-revalidate",
                "Pragma": "no-cache",
                "Expires": "0",
                "Access-Control-Allow-Origin": "*"
            }
        )
    except Exception as e:
        logger.error(f"服务视频片段失败: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


# 在本文件被直接运行时，用 uvicorn.run 启动服务
if __name__ == "__main__":
    uvicorn.run(
        app,
        host=config.SERVER_HOST,
        port=config.SERVER_PORT,
        # 强制 uvicorn 也用 debug 日志
        log_level="debug"
    )

================
File: config.py
================
# ================================
# File: config.py
# ================================
import os
from pathlib import Path
from dotenv import load_dotenv

# 获取 backend 目录路径
current_dir = Path(__file__).parent

# 加载 .env 文件（如果有的话）
env_path = current_dir / '.env'
load_dotenv(env_path)

# 获取项目相关目录
project_dir = current_dir.parent
storage_dir = project_dir / 'storage'


class Config:
    """
    仅保留一个配置类，固定 LOG_LEVEL="DEBUG"。
    原先按 FLASK_ENV 切换的逻辑全部移除。
    """

    # 服务器基础配置
    SERVER_HOST = "0.0.0.0"
    SERVER_PORT = 8000

    # 日志级别统一使用 DEBUG
    LOG_LEVEL = "DEBUG"

    # 存储相关配置
    BASE_DIR = storage_dir
    TASKS_DIR = BASE_DIR / "tasks"      # 任务工作目录
    PUBLIC_DIR = BASE_DIR / "public"    # 公共访问目录

    # 批处理配置
    BATCH_SIZE = 6

    # 音频配置
    TARGET_SPEAKER_AUDIO_DURATION = 8
    VAD_SR = 16000
    VOCALS_VOLUME = 0.7
    BACKGROUND_VOLUME = 0.3
    AUDIO_OVERLAP = 1024
    NORMALIZATION_THRESHOLD = 0.9

    # 视频分段配置
    SEGMENT_MINUTES = 5
    MIN_SEGMENT_MINUTES = 3

    # 模型配置：可从 .env 或环境变量中获取
    # 支持的翻译模型: glm4, gemini, deepseek
    TRANSLATION_MODEL = os.getenv("TRANSLATION_MODEL", "deepseek")
    ZHIPUAI_API_KEY = os.getenv("ZHIPUAI_API_KEY", "")
    GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "")
    DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY", "")

    # 系统路径配置
    SYSTEM_PATHS = [
        str(current_dir / 'models' / 'CosyVoice'),
        str(current_dir / 'models' / 'ClearVoice'),
        str(current_dir / 'models' / 'CosyVoice' / 'third_party' / 'Matcha-TTS')
    ]

    # 额外可选属性：MODEL_DIR、MODEL_PATH
    MODEL_DIR = project_dir / "models"

    @property
    def MODEL_PATH(self) -> Path:
        return Path(self.MODEL_DIR)

    @property
    def BASE_PATH(self) -> Path:
        return self.BASE_DIR

    @property
    def TASKS_PATH(self) -> Path:
        return self.TASKS_DIR

    @property
    def PUBLIC_PATH(self) -> Path:
        return self.PUBLIC_DIR

    # 初始化必须存在的目录
    @classmethod
    def init_directories(cls):
        """初始化所有必要的目录"""
        directories = [
            cls.BASE_DIR,
            cls.TASKS_DIR,
            cls.PUBLIC_DIR,
            cls.PUBLIC_DIR / "playlists",
            cls.PUBLIC_DIR / "segments"
        ]
        for dir_path in directories:
            dir_path.mkdir(parents=True, exist_ok=True)
            os.chmod(str(dir_path), 0o755)

    # 句子分割与合并时使用的阈值等
    MAX_GAP_MS = 2000  # merge_sentences中的最大间隔时间
    SHORT_SENTENCE_MERGE_THRESHOLD_MS = 1000  # process_tokens中短句合并的时间差阈值
    MAX_TOKENS_PER_SENTENCE = 80  # tokens_timestamp_sentence中的最大token数限制
    MIN_SENTENCE_LENGTH = 4       # tokens_timestamp_sentence中的最小句子长度阈值
    SENTENCE_END_TOKENS = {9686, 9688, 9676, 9705, 9728, 9729, 20046, 24883, 24879}
    STRONG_END_TOKENS = {9688, 9676, 9705, 9729, 20046, 24883}  # 句号、感叹号、问号等
    WEAK_END_TOKENS = {9686, 9728, 24879}
    SPEAKER_AUDIO_TARGET_DURATION = 8.0  # 提取说话人音频的目标长度(秒)
    TRANSLATION_BATCH_SIZE = 50         # 每批翻译最大句子数
    MODELIN_BATCH_SIZE = 3              # 每批模型输入最大句子数

================
File: pipeline_scheduler.py
================
import asyncio
import logging
from typing import List
from utils.decorators import worker_decorator
from utils.task_state import TaskState
from core.sentence_tools import Sentence

logger = logging.getLogger(__name__)

class PipelineScheduler:
    """
    多个Worker(翻译、模型输入、TTS Token、时长对齐、音频生成、混音)的调度器。
    每个任务对应一个TaskState，用task_state.*_queue来存放数据。
    """

    def __init__(
        self,
        translator,
        model_in,
        tts_token_generator,
        duration_aligner,
        audio_generator,
        timestamp_adjuster,
        mixer,
        config
    ):
        self.logger = logging.getLogger(__name__)
        self.translator = translator
        self.model_in = model_in
        self.tts_token_generator = tts_token_generator
        self.duration_aligner = duration_aligner
        self.audio_generator = audio_generator
        self.timestamp_adjuster = timestamp_adjuster
        self.mixer = mixer
        self.config = config

        self._workers = []

    async def start_workers(self, task_state: "TaskState"):
        """启动worker并记录日志"""
        self.logger.info(f"[PipelineScheduler] start_workers -> TaskID={task_state.task_id}")
        self._workers = [
            asyncio.create_task(self._translation_worker(task_state)),
            asyncio.create_task(self._modelin_worker(task_state)),
            asyncio.create_task(self._tts_token_worker(task_state)),
            asyncio.create_task(self._duration_align_worker(task_state)),
            asyncio.create_task(self._audio_generation_worker(task_state)),
            asyncio.create_task(self._mixing_worker(task_state))
        ]

    async def stop_workers(self, task_state: "TaskState"):
        """停止worker并等待结束"""
        self.logger.info(f"[PipelineScheduler] stop_workers -> TaskID={task_state.task_id}")
        # 放置 None 到最上游队列
        await task_state.translation_queue.put(None)
        # 等待全部worker
        await asyncio.gather(*self._workers, return_exceptions=True)
        self.logger.info(f"[PipelineScheduler] 所有Worker已结束 -> TaskID={task_state.task_id}")

    async def push_sentences_to_pipeline(self, task_state: "TaskState", sentences: List[Sentence], is_first_segment=False):
        """将ASR得到的句子推入翻译队列，并记录日志"""
        if is_first_segment:
            batch_size = self.config.TRANSLATION_BATCH_SIZE
            total = len(sentences)
            task_state.first_segment_batch_count = (total + batch_size - 1) // batch_size
            self.logger.info(f"[push_sentences_to_pipeline] 第一段预估批次数: {task_state.first_segment_batch_count} (TaskID={task_state.task_id})")

        self.logger.debug(f"[push_sentences_to_pipeline] 放入 {len(sentences)} 个句子到 translation_queue, TaskID={task_state.task_id}")
        await task_state.translation_queue.put(sentences)

    async def wait_first_segment_done(self, task_state: "TaskState"):
        """等待第一段完成"""
        self.logger.info(f"等待第一段所有batch完成 -> TaskID={task_state.task_id}")
        await task_state.mixing_complete.get()  # 阻塞等待
        self.logger.info(f"第一段完成 -> TaskID={task_state.task_id}")

    # ------------------- Worker 实现 -------------------

    @worker_decorator(
        input_queue_attr='translation_queue',
        next_queue_attr='modelin_queue',
        worker_name='翻译Worker',
        mode='stream'
    )
    async def _translation_worker(self, sentences_list: List[Sentence], task_state: "TaskState"):
        """翻译Worker，使用 translator.translate_sentences 做流式翻译"""
        if not sentences_list:
            return
        self.logger.debug(f"[翻译Worker] 收到 {len(sentences_list)} 句子, TaskID={task_state.task_id}")

        async for translated_batch in self.translator.translate_sentences(
            sentences_list,
            batch_size=self.config.TRANSLATION_BATCH_SIZE,
            target_language=task_state.target_language
        ):
            self.logger.debug(f"[翻译Worker] 翻译完成一批 -> size={len(translated_batch)}, TaskID={task_state.task_id}")
            yield translated_batch

    @worker_decorator(
        input_queue_attr='modelin_queue',
        next_queue_attr='tts_token_queue',
        worker_name='模型输入Worker',
        mode='stream'
    )
    async def _modelin_worker(self, sentences_batch: List[Sentence], task_state: "TaskState"):
        """对翻译好的句子更新模型输入特征"""
        if not sentences_batch:
            return
        self.logger.debug(f"[模型输入Worker] 收到 {len(sentences_batch)} 句子, TaskID={task_state.task_id}")

        async for updated_batch in self.model_in.modelin_maker(
            sentences_batch,
            reuse_speaker=False,
            reuse_uuid=False,
            batch_size=self.config.MODELIN_BATCH_SIZE
        ):
            self.logger.debug(f"[模型输入Worker] 处理完成一批 -> size={len(updated_batch)}, TaskID={task_state.task_id}")
            yield updated_batch

    @worker_decorator(
        input_queue_attr='tts_token_queue',
        next_queue_attr='duration_align_queue',
        worker_name='TTS Token生成Worker'
    )
    async def _tts_token_worker(self, sentences_batch: List[Sentence], task_state: "TaskState"):
        """批量生成 TTS token"""
        if not sentences_batch:
            return
        self.logger.debug(f"[TTS Token生成Worker] 收到 {len(sentences_batch)} 句子, TaskID={task_state.task_id}")

        await self.tts_token_generator.tts_token_maker(sentences_batch, reuse_uuid=False)
        self.logger.debug(f"[TTS Token生成Worker] 已为本批次生成token -> size={len(sentences_batch)}, TaskID={task_state.task_id}")
        return sentences_batch

    @worker_decorator(
        input_queue_attr='duration_align_queue',
        next_queue_attr='audio_gen_queue',
        worker_name='时长对齐Worker'
    )
    async def _duration_align_worker(self, sentences_batch: List[Sentence], task_state: "TaskState"):
        """对句子时长进行对齐修正"""
        if not sentences_batch:
            return
        self.logger.debug(f"[时长对齐Worker] 收到 {len(sentences_batch)} 句子, TaskID={task_state.task_id}")

        await self.duration_aligner.align_durations(sentences_batch)
        self.logger.debug(f"[时长对齐Worker] 对齐完成 -> size={len(sentences_batch)}, TaskID={task_state.task_id}")
        return sentences_batch

    @worker_decorator(
        input_queue_attr='audio_gen_queue',
        next_queue_attr='mixing_queue',
        worker_name='音频生成Worker'
    )
    async def _audio_generation_worker(self, sentences_batch: List[Sentence], task_state: "TaskState"):
        """生成语音音频并更新时间戳"""
        if not sentences_batch:
            return
        self.logger.debug(f"[音频生成Worker] 收到 {len(sentences_batch)} 句子, TaskID={task_state.task_id}")

        await self.audio_generator.vocal_audio_maker(sentences_batch)
        task_state.current_time = self.timestamp_adjuster.update_timestamps(
            sentences_batch, start_time=task_state.current_time
        )
        if not self.timestamp_adjuster.validate_timestamps(sentences_batch):
            self.logger.warning(f"[音频生成Worker] 检测到时间戳不连续, TaskID={task_state.task_id}")

        self.logger.debug(f"[音频生成Worker] 音频生成完成 -> size={len(sentences_batch)}, TaskID={task_state.task_id}")
        return sentences_batch

    @worker_decorator(
        input_queue_attr='mixing_queue',
        worker_name='混音Worker'
    )
    async def _mixing_worker(self, sentences_batch: List[Sentence], task_state: "TaskState"):
        """混音 + HLS 分段"""
        if not sentences_batch:
            return
        seg_index = sentences_batch[0].segment_index
        self.logger.debug(f"[混音Worker] 收到 {len(sentences_batch)} 句, segment={seg_index}, TaskID={task_state.task_id}")

        output_path = task_state.task_paths.segments_dir / f"segment_{task_state.batch_counter}.mp4"

        success = await self.mixer.mixed_media_maker(
            sentences=sentences_batch,
            task_state=task_state,
            output_path=str(output_path)
        )

        if success and task_state.hls_manager:
            await task_state.hls_manager.add_segment(str(output_path), task_state.batch_counter)
            self.logger.info(f"[混音Worker] 分段 {task_state.batch_counter} 已加入 HLS, TaskID={task_state.task_id}")

        # 如果是第一段
        if seg_index == 0:
            task_state.first_segment_processed_count += 1
            self.logger.info(
                f"[混音Worker] 第一段处理: {task_state.first_segment_processed_count}/{task_state.first_segment_batch_count}, "
                f"TaskID={task_state.task_id}"
            )
            if task_state.first_segment_processed_count >= task_state.first_segment_batch_count and task_state.mixing_complete:
                await task_state.mixing_complete.put(True)

        task_state.batch_counter += 1
        self.logger.debug(f"[混音Worker] 本批次混音完成 -> batch_counter={task_state.batch_counter}, TaskID={task_state.task_id}")
        return None

================
File: postcss.config.json
================
{ "plugins": { "postcss-preset-env": {}, "autoprefixer": {} } }

================
File: requirements.txt
================
--extra-index-url https://download.pytorch.org/whl/cu118
conformer==0.3.2
deepspeed==0.14.2; sys_platform == 'linux'
diffusers==0.27.2
gdown==5.1.0
gradio==4.32.2
grpcio==1.57.0
grpcio-tools==1.57.0
hydra-core==1.3.2
HyperPyYAML==1.2.2
inflect==7.3.1
librosa==0.10.2
lightning==2.2.4
matplotlib==3.7.5
modelscope==1.15.0
networkx==3.1
omegaconf==2.3.0
onnx==1.16.0
onnxruntime-gpu
openai-whisper==20231117
protobuf==4.25
pydantic==2.7.0
rich==13.7.1
soundfile==0.12.1
tensorboard==2.14.0
torch==2.0.1
torchaudio==2.0.2
uvicorn==0.30.0
wget==3.2
fastapi==0.111.0
fastapi-cli==0.0.4
WeTextProcessing==1.0.3
celery
redis
python-multipart
numpy
torch
torchaudio
soundfile
audio-separator
pydantic
pydantic-settings
python-magic
python-magic-bin; platform_system == "Windows"

================
File: video_translator.py
================
import logging
from typing import List, Dict, Any
from pathlib import Path
from core.auto_sense import SenseAutoModel
from models.CosyVoice.cosyvoice.cli.cosyvoice import CosyVoice2
from core.translation.translator import Translator
from core.translation.deepseek_client import DeepSeekClient
from core.tts_token_gener import TTSTokenGenerator
from core.audio_gener import AudioGenerator
from core.timeadjust.duration_aligner import DurationAligner
from core.timeadjust.timestamp_adjuster import TimestampAdjuster
from core.media_mixer import MediaMixer
from utils.media_utils import MediaUtils
from pipeline_scheduler import PipelineScheduler
from core.audio_separator import ClearVoiceSeparator
from core.model_in import ModelIn
from utils.task_storage import TaskPaths
from config import Config
from utils.task_state import TaskState


logger = logging.getLogger(__name__)

class ViTranslator:
    """
    全局持有大模型(ASR/TTS/翻译)对象, 每次 trans_video 时创建新的 TaskState + PipelineScheduler
    """

    def __init__(self, config: Config = None):
        self.logger = logger
        self.config = config or Config()
        self._init_global_models()

    def _init_global_models(self):
        self.logger.info("[ViTranslator] 初始化模型和工具...")

        self.audio_separator = ClearVoiceSeparator(model_name='MossFormer2_SE_48K')
        self.sense_model = SenseAutoModel(
            config=self.config,
            model="iic/SenseVoiceSmall",
            remote_code="./models/SenseVoice/model.py",
            vad_model="iic/speech_fsmn_vad_zh-cn-16k-common-pytorch",
            vad_kwargs={"max_single_segment_time": 30000},
            spk_model="cam++",
            trust_remote_code=True,
            disable_update=True,
            device="cuda"
        )
        self.cosyvoice_model = CosyVoice2("models/CosyVoice/pretrained_models/CosyVoice2-0.5B")
        self.target_sr = self.cosyvoice_model.sample_rate

        self.media_utils = MediaUtils(config=self.config, audio_separator=self.audio_separator, target_sr=self.target_sr)
        self.model_in = ModelIn(self.cosyvoice_model)
        self.tts_generator = TTSTokenGenerator(self.cosyvoice_model, Hz=25)
        self.audio_generator = AudioGenerator(self.cosyvoice_model, sample_rate=self.target_sr)

        # 选择翻译模型(此处演示 DeepSeekClient)
        translation_model = (self.config.TRANSLATION_MODEL or "deepseek").strip().lower()
        if translation_model == "deepseek":
            self.translator = Translator(DeepSeekClient(api_key=self.config.DEEPSEEK_API_KEY))
        else:
            raise ValueError(f"不支持的翻译模型：{translation_model}")

        self.duration_aligner = DurationAligner(
            model_in=self.model_in,
            simplifier=self.translator,
            tts_token_gener=self.tts_generator,
            max_speed=1.2
        )
        self.timestamp_adjuster = TimestampAdjuster(sample_rate=self.target_sr)
        self.mixer = MediaMixer(config=self.config, sample_rate=self.target_sr)

        self.logger.info("[ViTranslator] 初始化完成")

    async def trans_video(
        self,
        video_path: str,
        task_id: str,
        task_paths: TaskPaths,
        hls_manager=None,
        target_language="zh"
    ) -> Dict[str, Any]:
        """对外主函数：翻译一个视频文件"""
        self.logger.info(
            f"[trans_video] 开始处理视频: {video_path}, task_id={task_id}, target_language={target_language}"
        )

        # 1) 构建 TaskState
        task_state = TaskState(
            task_id=task_id,
            video_path=video_path,
            task_paths=task_paths,
            hls_manager=hls_manager,
            target_language=target_language
        )

        # 2) 创建 PipelineScheduler 并启动 Worker
        pipeline = PipelineScheduler(
            translator=self.translator,
            model_in=self.model_in,
            tts_token_generator=self.tts_generator,
            duration_aligner=self.duration_aligner,
            audio_generator=self.audio_generator,
            timestamp_adjuster=self.timestamp_adjuster,
            mixer=self.mixer,
            config=self.config
        )
        await pipeline.start_workers(task_state)

        try:
            # 3) 分段
            duration = await self.media_utils.get_video_duration(video_path)
            segments = await self.media_utils.get_audio_segments(duration)
            self.logger.info(f"总长度={duration:.2f}s, 分段数={len(segments)}, 任务ID={task_id}")

            if not segments:
                self.logger.warning(f"没有可用分段 -> 任务ID={task_id}")
                await pipeline.stop_workers(task_state)
                return {"status": "error", "message": "无法获取有效分段"}

            # 4) 先处理第一段
            first_start, first_dur = segments[0]
            await self._process_segment(pipeline, task_state, 0, first_start, first_dur, is_first_segment=True)
            # 等第一段完成(如果需要)
            await pipeline.wait_first_segment_done(task_state)

            # 处理后续分段
            for i, (seg_start, seg_dur) in enumerate(segments[1:], start=1):
                await self._process_segment(pipeline, task_state, i, seg_start, seg_dur)

            # 5) 所有分段都投递后，停止 Worker
            await pipeline.stop_workers(task_state)

            # 6) HLS
            if hls_manager and hls_manager.has_segments:
                await hls_manager.finalize_playlist()
                self.logger.info(f"[trans_video] 任务ID={task_id} 完成并已生成HLS。")
                return {"status": "success", "message": "视频翻译完成"}
            else:
                self.logger.warning(f"任务ID={task_id} 没有可用片段写入HLS")
                return {"status": "error", "message": "没有成功写入HLS片段"}
        except Exception as e:
            self.logger.exception(f"[trans_video] 任务ID={task_id} 出错: {e}")
            return {"status": "error", "message": str(e)}

    async def _process_segment(
        self,
        pipeline: PipelineScheduler,
        task_state: TaskState,
        segment_index: int,
        start: float,
        seg_duration: float,
        is_first_segment: bool = False
    ):
        """
        处理单个分段: 提取 -> ASR -> 推到 pipeline
        """
        self.logger.info(
            f"[_process_segment] 任务ID={task_state.task_id}, segment_index={segment_index}, start={start:.2f}, dur={seg_duration:.2f}"
        )
        media_files = await self.media_utils.extract_segment(
            video_path=task_state.video_path,
            start=start,
            duration=seg_duration,
            output_dir=task_state.task_paths.processing_dir,
            segment_index=segment_index
        )
        task_state.segment_media_files[segment_index] = media_files

        # ASR
        sentences = self.sense_model.generate(
            input=media_files['vocals'],
            cache={},
            language="auto",
            use_itn=True,
            batch_size_s=60,
            merge_vad=False
        )
        self.logger.info(f"[_process_segment] ASR识别到 {len(sentences)} 条句子, seg={segment_index}, TaskID={task_state.task_id}")
        if not sentences:
            return

        # 设置附加属性
        for s in sentences:
            s.segment_index = segment_index
            s.segment_start = start
            s.task_id = task_state.task_id
            s.sentence_id = task_state.sentence_counter
            task_state.sentence_counter += 1

        # 推送到 pipeline
        await pipeline.push_sentences_to_pipeline(task_state, sentences, is_first_segment)
