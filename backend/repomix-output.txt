This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: models/
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
services/
  cosyvoice/
    proto/
      __init__.py
      cosyvoice_pb2_grpc.py
      cosyvoice_pb2.py
      cosyvoice.proto
    cache.py
    client.py
    service.py
  hls/
    proto/
      __init__.py
      hls_service_pb2_grpc.py
      hls_service_pb2.py
      hls_service.proto
    __init__.py
    client.py
    server.py
    service.py
  __init__.py
  storage_service.py
utils/
  concurrency.py
  decorators.py
  ffmpeg_utils.py
  sentence_logger.py
  task_state.py
  task_storage.py
  temp_file_manager.py
workers/
  asr_worker/
    auto_sense.py
    sentence_tools.py
    worker.py
  audio_gen_worker/
    audio_gener.py
    timestamp_adjuster.py
    worker.py
  duration_worker/
    duration_aligner.py
    worker.py
  mixer_worker/
    media_mixer.py
    worker.py
  modelin_worker/
    model_in.py
    worker.py
  segment_worker/
    audio_separator.py
    video_segmenter.py
    worker.py
  translation_worker/
    translation/
      __init__.py
      deepseek_client.py
      gemini_client.py
      glm4_client.py
      prompt.py
      translator.py
    worker.py
  tts_worker/
    tts_token_gener.py
    worker.py
__init__.py
.cursorignore
.env.example
api.py
config.py
pipeline_scheduler.py
postcss.config.json
run_cosyvoice_service.py
run_hls_service.py
video_translator.py

================================================================
Files
================================================================

================
File: services/cosyvoice/proto/__init__.py
================
from .cosyvoice_pb2 import *
from .cosyvoice_pb2_grpc import *

================
File: services/cosyvoice/proto/cosyvoice_pb2_grpc.py
================
# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!
"""Client and server classes corresponding to protobuf-defined services."""
import grpc

from . import cosyvoice_pb2 as cosyvoice__pb2


class CosyVoiceServiceStub(object):
    """========== Service 定义 ==========
    """

    def __init__(self, channel):
        """Constructor.

        Args:
            channel: A grpc.Channel.
        """
        self.NormalizeText = channel.unary_unary(
                '/cosyvoice.CosyVoiceService/NormalizeText',
                request_serializer=cosyvoice__pb2.NormalizeTextRequest.SerializeToString,
                response_deserializer=cosyvoice__pb2.NormalizeTextResponse.FromString,
                )
        self.ExtractSpeakerFeatures = channel.unary_unary(
                '/cosyvoice.CosyVoiceService/ExtractSpeakerFeatures',
                request_serializer=cosyvoice__pb2.ExtractSpeakerFeaturesRequest.SerializeToString,
                response_deserializer=cosyvoice__pb2.ExtractSpeakerFeaturesResponse.FromString,
                )
        self.GenerateTTSTokens = channel.unary_unary(
                '/cosyvoice.CosyVoiceService/GenerateTTSTokens',
                request_serializer=cosyvoice__pb2.GenerateTTSTokensRequest.SerializeToString,
                response_deserializer=cosyvoice__pb2.GenerateTTSTokensResponse.FromString,
                )
        self.Token2Wav = channel.unary_unary(
                '/cosyvoice.CosyVoiceService/Token2Wav',
                request_serializer=cosyvoice__pb2.Token2WavRequest.SerializeToString,
                response_deserializer=cosyvoice__pb2.Token2WavResponse.FromString,
                )
        self.Cleanup = channel.unary_unary(
                '/cosyvoice.CosyVoiceService/Cleanup',
                request_serializer=cosyvoice__pb2.CleanupRequest.SerializeToString,
                response_deserializer=cosyvoice__pb2.CleanupResponse.FromString,
                )


class CosyVoiceServiceServicer(object):
    """========== Service 定义 ==========
    """

    def NormalizeText(self, request, context):
        """Missing associated documentation comment in .proto file."""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details('Method not implemented!')
        raise NotImplementedError('Method not implemented!')

    def ExtractSpeakerFeatures(self, request, context):
        """Missing associated documentation comment in .proto file."""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details('Method not implemented!')
        raise NotImplementedError('Method not implemented!')

    def GenerateTTSTokens(self, request, context):
        """Missing associated documentation comment in .proto file."""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details('Method not implemented!')
        raise NotImplementedError('Method not implemented!')

    def Token2Wav(self, request, context):
        """Missing associated documentation comment in .proto file."""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details('Method not implemented!')
        raise NotImplementedError('Method not implemented!')

    def Cleanup(self, request, context):
        """Missing associated documentation comment in .proto file."""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details('Method not implemented!')
        raise NotImplementedError('Method not implemented!')


def add_CosyVoiceServiceServicer_to_server(servicer, server):
    rpc_method_handlers = {
            'NormalizeText': grpc.unary_unary_rpc_method_handler(
                    servicer.NormalizeText,
                    request_deserializer=cosyvoice__pb2.NormalizeTextRequest.FromString,
                    response_serializer=cosyvoice__pb2.NormalizeTextResponse.SerializeToString,
            ),
            'ExtractSpeakerFeatures': grpc.unary_unary_rpc_method_handler(
                    servicer.ExtractSpeakerFeatures,
                    request_deserializer=cosyvoice__pb2.ExtractSpeakerFeaturesRequest.FromString,
                    response_serializer=cosyvoice__pb2.ExtractSpeakerFeaturesResponse.SerializeToString,
            ),
            'GenerateTTSTokens': grpc.unary_unary_rpc_method_handler(
                    servicer.GenerateTTSTokens,
                    request_deserializer=cosyvoice__pb2.GenerateTTSTokensRequest.FromString,
                    response_serializer=cosyvoice__pb2.GenerateTTSTokensResponse.SerializeToString,
            ),
            'Token2Wav': grpc.unary_unary_rpc_method_handler(
                    servicer.Token2Wav,
                    request_deserializer=cosyvoice__pb2.Token2WavRequest.FromString,
                    response_serializer=cosyvoice__pb2.Token2WavResponse.SerializeToString,
            ),
            'Cleanup': grpc.unary_unary_rpc_method_handler(
                    servicer.Cleanup,
                    request_deserializer=cosyvoice__pb2.CleanupRequest.FromString,
                    response_serializer=cosyvoice__pb2.CleanupResponse.SerializeToString,
            ),
    }
    generic_handler = grpc.method_handlers_generic_handler(
            'cosyvoice.CosyVoiceService', rpc_method_handlers)
    server.add_generic_rpc_handlers((generic_handler,))


 # This class is part of an EXPERIMENTAL API.
class CosyVoiceService(object):
    """========== Service 定义 ==========
    """

    @staticmethod
    def NormalizeText(request,
            target,
            options=(),
            channel_credentials=None,
            call_credentials=None,
            insecure=False,
            compression=None,
            wait_for_ready=None,
            timeout=None,
            metadata=None):
        return grpc.experimental.unary_unary(request, target, '/cosyvoice.CosyVoiceService/NormalizeText',
            cosyvoice__pb2.NormalizeTextRequest.SerializeToString,
            cosyvoice__pb2.NormalizeTextResponse.FromString,
            options, channel_credentials,
            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)

    @staticmethod
    def ExtractSpeakerFeatures(request,
            target,
            options=(),
            channel_credentials=None,
            call_credentials=None,
            insecure=False,
            compression=None,
            wait_for_ready=None,
            timeout=None,
            metadata=None):
        return grpc.experimental.unary_unary(request, target, '/cosyvoice.CosyVoiceService/ExtractSpeakerFeatures',
            cosyvoice__pb2.ExtractSpeakerFeaturesRequest.SerializeToString,
            cosyvoice__pb2.ExtractSpeakerFeaturesResponse.FromString,
            options, channel_credentials,
            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)

    @staticmethod
    def GenerateTTSTokens(request,
            target,
            options=(),
            channel_credentials=None,
            call_credentials=None,
            insecure=False,
            compression=None,
            wait_for_ready=None,
            timeout=None,
            metadata=None):
        return grpc.experimental.unary_unary(request, target, '/cosyvoice.CosyVoiceService/GenerateTTSTokens',
            cosyvoice__pb2.GenerateTTSTokensRequest.SerializeToString,
            cosyvoice__pb2.GenerateTTSTokensResponse.FromString,
            options, channel_credentials,
            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)

    @staticmethod
    def Token2Wav(request,
            target,
            options=(),
            channel_credentials=None,
            call_credentials=None,
            insecure=False,
            compression=None,
            wait_for_ready=None,
            timeout=None,
            metadata=None):
        return grpc.experimental.unary_unary(request, target, '/cosyvoice.CosyVoiceService/Token2Wav',
            cosyvoice__pb2.Token2WavRequest.SerializeToString,
            cosyvoice__pb2.Token2WavResponse.FromString,
            options, channel_credentials,
            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)

    @staticmethod
    def Cleanup(request,
            target,
            options=(),
            channel_credentials=None,
            call_credentials=None,
            insecure=False,
            compression=None,
            wait_for_ready=None,
            timeout=None,
            metadata=None):
        return grpc.experimental.unary_unary(request, target, '/cosyvoice.CosyVoiceService/Cleanup',
            cosyvoice__pb2.CleanupRequest.SerializeToString,
            cosyvoice__pb2.CleanupResponse.FromString,
            options, channel_credentials,
            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)

================
File: services/cosyvoice/proto/cosyvoice_pb2.py
================
# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: cosyvoice.proto
"""Generated protocol buffer code."""
from google.protobuf import descriptor as _descriptor
from google.protobuf import descriptor_pool as _descriptor_pool
from google.protobuf import symbol_database as _symbol_database
from google.protobuf.internal import builder as _builder
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()




DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x0f\x63osyvoice.proto\x12\tcosyvoice\"$\n\x14NormalizeTextRequest\x12\x0c\n\x04text\x18\x01 \x01(\t\"*\n\x15NormalizeTextResponse\x12\x11\n\ttext_uuid\x18\x01 \x01(\t\"Y\n\x1d\x45xtractSpeakerFeaturesRequest\x12\x14\n\x0cspeaker_uuid\x18\x01 \x01(\t\x12\r\n\x05\x61udio\x18\x02 \x01(\x0c\x12\x13\n\x0bsample_rate\x18\x03 \x01(\x05\"1\n\x1e\x45xtractSpeakerFeaturesResponse\x12\x0f\n\x07success\x18\x01 \x01(\x08\"C\n\x18GenerateTTSTokensRequest\x12\x11\n\ttext_uuid\x18\x01 \x01(\t\x12\x14\n\x0cspeaker_uuid\x18\x02 \x01(\t\"A\n\x19GenerateTTSTokensResponse\x12\x13\n\x0b\x64uration_ms\x18\x01 \x01(\x05\x12\x0f\n\x07success\x18\x02 \x01(\x08\"J\n\x10Token2WavRequest\x12\x11\n\ttext_uuid\x18\x01 \x01(\t\x12\x14\n\x0cspeaker_uuid\x18\x02 \x01(\t\x12\r\n\x05speed\x18\x03 \x01(\x02\"8\n\x11Token2WavResponse\x12\r\n\x05\x61udio\x18\x01 \x01(\x0c\x12\x14\n\x0c\x64uration_sec\x18\x02 \x01(\x02\"2\n\x0e\x43leanupRequest\x12\x0c\n\x04uuid\x18\x01 \x01(\t\x12\x12\n\nis_speaker\x18\x02 \x01(\x08\"\"\n\x0f\x43leanupResponse\x12\x0f\n\x07success\x18\x01 \x01(\x08\x32\xbf\x03\n\x10\x43osyVoiceService\x12R\n\rNormalizeText\x12\x1f.cosyvoice.NormalizeTextRequest\x1a .cosyvoice.NormalizeTextResponse\x12m\n\x16\x45xtractSpeakerFeatures\x12(.cosyvoice.ExtractSpeakerFeaturesRequest\x1a).cosyvoice.ExtractSpeakerFeaturesResponse\x12^\n\x11GenerateTTSTokens\x12#.cosyvoice.GenerateTTSTokensRequest\x1a$.cosyvoice.GenerateTTSTokensResponse\x12\x46\n\tToken2Wav\x12\x1b.cosyvoice.Token2WavRequest\x1a\x1c.cosyvoice.Token2WavResponse\x12@\n\x07\x43leanup\x12\x19.cosyvoice.CleanupRequest\x1a\x1a.cosyvoice.CleanupResponseb\x06proto3')

_globals = globals()
_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)
_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'cosyvoice_pb2', _globals)
if _descriptor._USE_C_DESCRIPTORS == False:

  DESCRIPTOR._options = None
  _globals['_NORMALIZETEXTREQUEST']._serialized_start=30
  _globals['_NORMALIZETEXTREQUEST']._serialized_end=66
  _globals['_NORMALIZETEXTRESPONSE']._serialized_start=68
  _globals['_NORMALIZETEXTRESPONSE']._serialized_end=110
  _globals['_EXTRACTSPEAKERFEATURESREQUEST']._serialized_start=112
  _globals['_EXTRACTSPEAKERFEATURESREQUEST']._serialized_end=201
  _globals['_EXTRACTSPEAKERFEATURESRESPONSE']._serialized_start=203
  _globals['_EXTRACTSPEAKERFEATURESRESPONSE']._serialized_end=252
  _globals['_GENERATETTSTOKENSREQUEST']._serialized_start=254
  _globals['_GENERATETTSTOKENSREQUEST']._serialized_end=321
  _globals['_GENERATETTSTOKENSRESPONSE']._serialized_start=323
  _globals['_GENERATETTSTOKENSRESPONSE']._serialized_end=388
  _globals['_TOKEN2WAVREQUEST']._serialized_start=390
  _globals['_TOKEN2WAVREQUEST']._serialized_end=464
  _globals['_TOKEN2WAVRESPONSE']._serialized_start=466
  _globals['_TOKEN2WAVRESPONSE']._serialized_end=522
  _globals['_CLEANUPREQUEST']._serialized_start=524
  _globals['_CLEANUPREQUEST']._serialized_end=574
  _globals['_CLEANUPRESPONSE']._serialized_start=576
  _globals['_CLEANUPRESPONSE']._serialized_end=610
  _globals['_COSYVOICESERVICE']._serialized_start=613
  _globals['_COSYVOICESERVICE']._serialized_end=1060
# @@protoc_insertion_point(module_scope)

================
File: services/cosyvoice/proto/cosyvoice.proto
================
syntax = "proto3";

package cosyvoice;

// ========== Service 定义 ==========
service CosyVoiceService {
  rpc NormalizeText (NormalizeTextRequest) returns (NormalizeTextResponse);
  rpc ExtractSpeakerFeatures (ExtractSpeakerFeaturesRequest) returns (ExtractSpeakerFeaturesResponse);
  rpc GenerateTTSTokens (GenerateTTSTokensRequest) returns (GenerateTTSTokensResponse);
  rpc Token2Wav (Token2WavRequest) returns (Token2WavResponse);
  rpc Cleanup (CleanupRequest) returns (CleanupResponse);
}

// ========== RPC 请求与响应 ==========

// 1) NormalizeText
message NormalizeTextRequest {
  string text = 1;
}

message NormalizeTextResponse {
  string text_uuid = 1;  // 返回文本特征的UUID
}

// 2) ExtractSpeakerFeatures
message ExtractSpeakerFeaturesRequest {
  string speaker_uuid = 1;  // 说话人特征的UUID
  bytes audio = 2;         // 音频数据
  int32 sample_rate = 3;
}

message ExtractSpeakerFeaturesResponse {
  bool success = 1;
}

// 3) GenerateTTSTokens
message GenerateTTSTokensRequest {
  string text_uuid = 1;     // 文本特征的UUID
  string speaker_uuid = 2;  // 说话人特征的UUID
}

message GenerateTTSTokensResponse {
  int32 duration_ms = 1;
  bool success = 2;
}

// 4) Token2Wav
message Token2WavRequest {
  string text_uuid = 1;     // 文本特征的UUID
  string speaker_uuid = 2;  // 说话人特征的UUID
  float speed = 3;
}

message Token2WavResponse {
  bytes audio = 1;
  float duration_sec = 2;
}

// 5) Cleanup
message CleanupRequest {
  string uuid = 1;          // 要清理的UUID（可以是text_uuid或speaker_uuid）
  bool is_speaker = 2;      // 是否清理说话人特征
}

message CleanupResponse {
  bool success = 1;
}

================
File: services/cosyvoice/cache.py
================
import threading
from typing import Dict, Optional, List, Any
import torch

class TextFeatureData:
    """存储文本特征数据"""
    def __init__(self):
        self.normalized_texts: List[str] = []
        self.text_tokens: List[torch.Tensor] = []  # [batch_size, seq_len]
        self.tts_tokens: List[List[int]] = []      # TTS tokens
        self.tts_segment_uuids: List[str] = []

class SpeakerFeatureData:
    """存储说话人特征数据，保持完整特征字典"""
    def __init__(self):
        self.features: Optional[Dict[str, Any]] = None  # 存储完整的特征字典

class FeaturesCache:
    def __init__(self):
        self._text_cache: Dict[str, TextFeatureData] = {}      # text_uuid -> TextFeatureData
        self._speaker_cache: Dict[str, SpeakerFeatureData] = {}  # speaker_uuid -> SpeakerFeatureData
        self._lock = threading.Lock()
    
    def get_text(self, text_uuid: str) -> Optional[TextFeatureData]:
        """获取文本特征数据"""
        with self._lock:
            return self._text_cache.get(text_uuid)
    
    def get_speaker(self, speaker_uuid: str) -> Optional[Dict[str, Any]]:
        """获取说话人特征字典"""
        with self._lock:
            data = self._speaker_cache.get(speaker_uuid)
            return data.features if data else None
    
    def set_text(self, text_uuid: str, data: TextFeatureData) -> None:
        """设置文本特征数据"""
        with self._lock:
            self._text_cache[text_uuid] = data
    
    def set_speaker(self, speaker_uuid: str, features: Dict[str, Any]) -> None:
        """设置说话人特征字典"""
        with self._lock:
            data = SpeakerFeatureData()
            data.features = features
            self._speaker_cache[speaker_uuid] = data
    
    def update_text_features(self, text_uuid: str, normalized_texts: List[str], text_tokens: List[torch.Tensor]) -> bool:
        """更新文本特征"""
        with self._lock:
            if text_uuid not in self._text_cache:
                data = TextFeatureData()
                data.normalized_texts = normalized_texts
                data.text_tokens = text_tokens
                self._text_cache[text_uuid] = data
            else:
                self._text_cache[text_uuid].normalized_texts = normalized_texts
                self._text_cache[text_uuid].text_tokens = text_tokens
            return True
    
    def update_speaker_features(self, speaker_uuid: str, features: Dict[str, Any]) -> bool:
        """更新说话人特征字典"""
        with self._lock:
            data = SpeakerFeatureData()
            data.features = features
            self._speaker_cache[speaker_uuid] = data
            return True
    
    def update_tts_tokens(self, text_uuid: str, tokens: List[List[int]], segment_uuids: List[str]) -> bool:
        """更新TTS tokens"""
        with self._lock:
            if text_uuid not in self._text_cache:
                return False
            data = self._text_cache[text_uuid]
            data.tts_tokens = tokens
            data.tts_segment_uuids = segment_uuids
            return True
    
    def delete(self, uuid: str, is_speaker: bool = False) -> None:
        """删除特征数据"""
        with self._lock:
            if is_speaker:
                self._speaker_cache.pop(uuid, None)
            else:
                self._text_cache.pop(uuid, None)
    
    def exists_text(self, text_uuid: str) -> bool:
        """检查文本UUID是否存在"""
        with self._lock:
            return text_uuid in self._text_cache
    
    def exists_speaker(self, speaker_uuid: str) -> bool:
        """检查说话人UUID是否存在"""
        with self._lock:
            return speaker_uuid in self._speaker_cache

================
File: services/cosyvoice/client.py
================
import grpc
import numpy as np
import torch
import logging
from typing import Tuple, Optional

from .proto import cosyvoice_pb2
from .proto import cosyvoice_pb2_grpc

logger = logging.getLogger(__name__)

class CosyVoiceClient:
    def __init__(self, address="localhost:50052"):
        self.channel = grpc.insecure_channel(address)
        self.stub = cosyvoice_pb2_grpc.CosyVoiceServiceStub(self.channel)

    def normalize_text(self, text: str) -> str:
        """
        文本标准化，返回文本UUID
        """
        try:
            req = cosyvoice_pb2.NormalizeTextRequest(text=text)
            resp = self.stub.NormalizeText(req)
            return resp.text_uuid
        except Exception as e:
            logger.error(f"NormalizeText调用失败: {e}")
            raise

    def extract_speaker_features(self, speaker_uuid: str, audio: np.ndarray, sr: int = 24000) -> bool:
        """
        提取说话人特征。
        :param speaker_uuid: 说话人标识
        :param audio: 输入音频（numpy.ndarray）
        :param sr: 采样率，默认24000
        :return: 是否成功
        """
        try:
            # 直接将numpy array转换为bytes
            audio_bytes = audio.tobytes()
            req = cosyvoice_pb2.ExtractSpeakerFeaturesRequest(
                speaker_uuid=speaker_uuid,
                audio=audio_bytes,
                sample_rate=sr
            )
            resp = self.stub.ExtractSpeakerFeatures(req)
            return resp.success
        except Exception as e:
            logger.error(f"ExtractSpeakerFeatures调用失败: {e}")
            raise

    def generate_tts_tokens(self, text_uuid: str, speaker_uuid: str) -> Tuple[int, bool]:
        """
        根据文本UUID和说话人UUID生成TTS tokens并返回预估时长
        """
        try:
            req = cosyvoice_pb2.GenerateTTSTokensRequest(text_uuid=text_uuid, speaker_uuid=speaker_uuid)
            resp = self.stub.GenerateTTSTokens(req)
            return resp.duration_ms, resp.success
        except Exception as e:
            logger.error(f"GenerateTTSTokens调用失败: {e}")
            raise

    def token2wav(self, text_uuid: str, speaker_uuid: str, speed: float = 1.0) -> Tuple[np.ndarray, float]:
        """
        将文本UUID和说话人UUID转换为音频
        """
        try:
            req = cosyvoice_pb2.Token2WavRequest(text_uuid=text_uuid, speaker_uuid=speaker_uuid, speed=speed)
            resp = self.stub.Token2Wav(req)
            audio_np = np.frombuffer(resp.audio, dtype=np.int16).astype(np.float32) / (2**15)
            return audio_np, resp.duration_sec
        except Exception as e:
            logger.error(f"Token2Wav调用失败: {e}")
            raise

    def cleanup(self, uuid: str, is_speaker: bool = False) -> bool:
        """
        清理服务端缓存
        """
        try:
            req = cosyvoice_pb2.CleanupRequest(uuid=uuid, is_speaker=is_speaker)
            resp = self.stub.Cleanup(req)
            return resp.success
        except Exception as e:
            logger.error(f"Cleanup调用失败: {e}")
            raise

================
File: services/cosyvoice/service.py
================
# services/cosyvoice/service.py

import logging
import numpy as np
import torch
import grpc
from concurrent import futures
import uuid
import os
import soundfile as sf

from .proto import cosyvoice_pb2
from .proto import cosyvoice_pb2_grpc
from .cache import FeaturesCache, TextFeatureData, SpeakerFeatureData

# 假设我们使用 CosyVoice2 作为模型
from models.CosyVoice.cosyvoice.cli.cosyvoice import CosyVoice2

logger = logging.getLogger(__name__)

class CosyVoiceServiceServicer(cosyvoice_pb2_grpc.CosyVoiceServiceServicer):
    def __init__(self, model_path="models/CosyVoice/pretrained_models/CosyVoice2-0.5B"):
        try:
            self.cosyvoice = CosyVoice2(model_path)
            self.frontend = self.cosyvoice.frontend
            self.model = self.cosyvoice.model
            self.sample_rate = self.cosyvoice.sample_rate
            self.cache = FeaturesCache()
            
            # 创建调试音频保存目录
            self.debug_dir = "debug_audio"
            os.makedirs(self.debug_dir, exist_ok=True)
            
            logger.info('CosyVoice服务初始化成功')
        except Exception as e:
            logger.error(f'CosyVoice服务初始化失败: {e}')
            raise

    def _generate_uuid(self) -> str:
        """生成唯一的UUID"""
        return str(uuid.uuid4())

    def NormalizeText(self, request, context):
        """
        文本标准化，生成文本特征数据并缓存
        """
        try:
            text = request.text or ""
            normalized_texts = self.frontend.text_normalize(text, split=True, text_frontend=False)
            text_tokens = []
            
            for seg in normalized_texts:
                tokens, _ = self.frontend._extract_text_token(seg)
                text_tokens.append(tokens)

            text_uuid = self._generate_uuid()
            self.cache.update_text_features(text_uuid, normalized_texts, text_tokens)

            return cosyvoice_pb2.NormalizeTextResponse(text_uuid=text_uuid)
        except Exception as e:
            logger.error(f"NormalizeText失败: {e}")
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(str(e))
            return cosyvoice_pb2.NormalizeTextResponse()

    def ExtractSpeakerFeatures(self, request, context):
        """
        提取说话人特征并缓存
        """
        try:
            speaker_uuid = request.speaker_uuid
            # 转换为tensor，确保数组可写
            audio_np = np.frombuffer(request.audio, dtype=np.float32).copy()
            audio = torch.from_numpy(audio_np).unsqueeze(0)  # [1, T]

            # 使用tensor调用frontend，这里的sample_rate是目标采样率
            features = self.frontend.frontend_cross_lingual("", audio, request.sample_rate)
            
            # 直接存储完整的特征字典
            success = self.cache.update_speaker_features(speaker_uuid, features)

            if not success:
                context.set_code(grpc.StatusCode.INTERNAL)
                context.set_details("更新说话人特征缓存失败")
                return cosyvoice_pb2.ExtractSpeakerFeaturesResponse()

            return cosyvoice_pb2.ExtractSpeakerFeaturesResponse(success=True)
        except Exception as e:
            logger.error(f"ExtractSpeakerFeatures失败: {e}")
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(str(e))
            return cosyvoice_pb2.ExtractSpeakerFeaturesResponse()

    def GenerateTTSTokens(self, request, context):
        """
        使用文本UUID和说话人UUID生成TTS tokens并更新缓存
        """
        try:
            text_uuid = request.text_uuid
            speaker_uuid = request.speaker_uuid
            text_features = self.cache.get_text(text_uuid)
            speaker_features = self.cache.get_speaker(speaker_uuid)

            if not text_features:
                context.set_code(grpc.StatusCode.NOT_FOUND)
                context.set_details(f"Text UUID {text_uuid} 不存在")
                return cosyvoice_pb2.GenerateTTSTokensResponse()
            if not speaker_features:
                context.set_code(grpc.StatusCode.NOT_FOUND)
                context.set_details(f"Speaker UUID {speaker_uuid} 不存在")
                return cosyvoice_pb2.GenerateTTSTokensResponse()

            total_duration_ms = 0
            tts_tokens = []
            segment_uuids = []

            for i, text_tokens in enumerate(text_features.text_tokens):
                seg_uuid = f"{text_uuid}_seg_{i}"
                segment_uuids.append(seg_uuid)

                with self.model.lock:
                    self.model.tts_speech_token_dict[seg_uuid] = []
                    self.model.llm_end_dict[seg_uuid] = False

                try:
                    self.model.llm_job(
                        text=text_tokens,
                        prompt_text=torch.zeros(1, 0, dtype=torch.int32),
                        llm_prompt_speech_token=speaker_features.get('llm_prompt_speech_token', torch.zeros(1, 0, dtype=torch.int32)),
                        llm_embedding=speaker_features.get('llm_embedding', torch.zeros(0, 192)),
                        uuid=seg_uuid
                    )

                    seg_tokens = self.model.tts_speech_token_dict[seg_uuid]
                    tts_tokens.append(seg_tokens)
                    total_duration_ms += len(seg_tokens) / 25.0 * 1000

                finally:
                    self.model.tts_speech_token_dict.pop(seg_uuid, None)
                    self.model.llm_end_dict.pop(seg_uuid, None)

            success = self.cache.update_tts_tokens(text_uuid, tts_tokens, segment_uuids)
            if not success:
                context.set_code(grpc.StatusCode.INTERNAL)
                context.set_details("更新TTS tokens缓存失败")
                return cosyvoice_pb2.GenerateTTSTokensResponse()

            return cosyvoice_pb2.GenerateTTSTokensResponse(
                duration_ms=int(total_duration_ms),
                success=True
            )

        except Exception as e:
            logger.error(f"GenerateTTSTokens失败: {e}")
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(str(e))
            return cosyvoice_pb2.GenerateTTSTokensResponse()

    def Token2Wav(self, request, context):
        """
        使用文本UUID和说话人UUID将TTS tokens转换为音频
        """
        try:
            text_uuid = request.text_uuid
            speaker_uuid = request.speaker_uuid
            text_features = self.cache.get_text(text_uuid)
            speaker_features = self.cache.get_speaker(speaker_uuid)

            if not text_features:
                context.set_code(grpc.StatusCode.NOT_FOUND)
                context.set_details(f"Text UUID {text_uuid} 不存在")
                return cosyvoice_pb2.Token2WavResponse()
            if not speaker_features:
                context.set_code(grpc.StatusCode.NOT_FOUND)
                context.set_details(f"Speaker UUID {speaker_uuid} 不存在")
                return cosyvoice_pb2.Token2WavResponse()

            speed = request.speed
            audio_pieces = []
            total_duration_sec = 0.0

            for seg_tokens, seg_uuid in zip(text_features.tts_tokens, text_features.tts_segment_uuids):
                if not seg_tokens:
                    continue

                self.model.hift_cache_dict[seg_uuid] = None
                try:
                    seg_audio_out = self.model.token2wav(
                        token=torch.tensor(seg_tokens).unsqueeze(0),
                        prompt_token=speaker_features.get('flow_prompt_speech_token', torch.zeros(1, 0, dtype=torch.int32)),
                        prompt_feat=speaker_features.get('prompt_speech_feat', torch.zeros(1, 0, 80)),
                        embedding=speaker_features.get('llm_embedding', torch.zeros(0)),
                        uuid=seg_uuid,
                        token_offset=0,
                        finalize=True,
                        speed=speed
                    )
                finally:
                    self.model.hift_cache_dict.pop(seg_uuid, None)

                # 处理音频：移除batch维度，如果是多通道则取平均
                seg_audio = seg_audio_out.cpu().numpy()
                if seg_audio.ndim > 1:
                    seg_audio = seg_audio.mean(axis=0)  # 如果还有多个通道，取平均得到单通道
                audio_pieces.append(seg_audio)
                total_duration_sec += len(seg_audio) / self.sample_rate

            if not audio_pieces:
                logger.warning(f"Token2Wav: Text UUID {text_uuid} 未生成任何音频")
                return cosyvoice_pb2.Token2WavResponse()

            final_audio = np.concatenate(audio_pieces)
            audio_int16 = (final_audio * (2**15)).astype(np.int16).tobytes()

            return cosyvoice_pb2.Token2WavResponse(
                audio=audio_int16,
                duration_sec=total_duration_sec
            )

        except Exception as e:
            logger.error(f"Token2Wav失败: {e}")
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(str(e))
            return cosyvoice_pb2.Token2WavResponse()

    def Cleanup(self, request, context):
        """
        清理指定UUID的文本或说话人特征
        """
        try:
            uuid = request.uuid
            is_speaker = request.is_speaker
            self.cache.delete(uuid, is_speaker)
            return cosyvoice_pb2.CleanupResponse(success=True)
        except Exception as e:
            logger.error(f"Cleanup失败: {e}")
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(str(e))
            return cosyvoice_pb2.CleanupResponse(success=False)

def serve(args):
    host = getattr(args, 'host', '0.0.0.0')
    port = getattr(args, 'port', 50052)
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
    cosyvoice_pb2_grpc.add_CosyVoiceServiceServicer_to_server(
        CosyVoiceServiceServicer(args.model_dir), server
    )
    address = f'{host}:{port}'
    server.add_insecure_port(address)
    server.start()
    logger.info(f'CosyVoice服务已启动: {address}')
    server.wait_for_termination()

================
File: services/hls/proto/__init__.py
================
from . import hls_service_pb2
from . import hls_service_pb2_grpc

__all__ = ['hls_service_pb2', 'hls_service_pb2_grpc']

================
File: services/hls/proto/hls_service_pb2_grpc.py
================
# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!
"""Client and server classes corresponding to protobuf-defined services."""
from google.protobuf import descriptor as _descriptor
from google.protobuf import message as _message
from google.protobuf import reflection as _reflection
from google.protobuf import symbol_database as _symbol_database
import grpc

from . import hls_service_pb2 as hls__service__pb2


class HLSServiceStub(object):
    """HLS服务定义
    """

    def __init__(self, channel):
        """Constructor.

        Args:
            channel: A grpc.Channel.
        """
        self.InitTask = channel.unary_unary(
                '/hls.HLSService/InitTask',
                request_serializer=hls__service__pb2.InitTaskRequest.SerializeToString,
                response_deserializer=hls__service__pb2.InitTaskResponse.FromString,
                )
        self.AddSegment = channel.unary_unary(
                '/hls.HLSService/AddSegment',
                request_serializer=hls__service__pb2.AddSegmentRequest.SerializeToString,
                response_deserializer=hls__service__pb2.AddSegmentResponse.FromString,
                )
        self.FinalizeTask = channel.unary_unary(
                '/hls.HLSService/FinalizeTask',
                request_serializer=hls__service__pb2.FinalizeTaskRequest.SerializeToString,
                response_deserializer=hls__service__pb2.FinalizeTaskResponse.FromString,
                )
        self.CleanupTask = channel.unary_unary(
                '/hls.HLSService/CleanupTask',
                request_serializer=hls__service__pb2.CleanupTaskRequest.SerializeToString,
                response_deserializer=hls__service__pb2.CleanupTaskResponse.FromString,
                )


class HLSServiceServicer(object):
    """HLS服务定义
    """

    def InitTask(self, request, context):
        """初始化任务
        """
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details('Method not implemented!')
        raise NotImplementedError('Method not implemented!')

    def AddSegment(self, request, context):
        """添加视频片段
        """
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details('Method not implemented!')
        raise NotImplementedError('Method not implemented!')

    def FinalizeTask(self, request, context):
        """完成任务
        """
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details('Method not implemented!')
        raise NotImplementedError('Method not implemented!')

    def CleanupTask(self, request, context):
        """清理任务资源
        """
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details('Method not implemented!')
        raise NotImplementedError('Method not implemented!')


def add_HLSServiceServicer_to_server(servicer, server):
    rpc_method_handlers = {
            'InitTask': grpc.unary_unary_rpc_method_handler(
                    servicer.InitTask,
                    request_deserializer=hls__service__pb2.InitTaskRequest.FromString,
                    response_serializer=hls__service__pb2.InitTaskResponse.SerializeToString,
            ),
            'AddSegment': grpc.unary_unary_rpc_method_handler(
                    servicer.AddSegment,
                    request_deserializer=hls__service__pb2.AddSegmentRequest.FromString,
                    response_serializer=hls__service__pb2.AddSegmentResponse.SerializeToString,
            ),
            'FinalizeTask': grpc.unary_unary_rpc_method_handler(
                    servicer.FinalizeTask,
                    request_deserializer=hls__service__pb2.FinalizeTaskRequest.FromString,
                    response_serializer=hls__service__pb2.FinalizeTaskResponse.SerializeToString,
            ),
            'CleanupTask': grpc.unary_unary_rpc_method_handler(
                    servicer.CleanupTask,
                    request_deserializer=hls__service__pb2.CleanupTaskRequest.FromString,
                    response_serializer=hls__service__pb2.CleanupTaskResponse.SerializeToString,
            ),
    }
    generic_handler = grpc.method_handlers_generic_handler(
            'hls.HLSService', rpc_method_handlers)
    server.add_generic_rpc_handlers((generic_handler,))


 # This class is part of an EXPERIMENTAL API.
class HLSService(object):
    """HLS服务定义
    """

    @staticmethod
    def InitTask(request,
            target,
            options=(),
            channel_credentials=None,
            call_credentials=None,
            insecure=False,
            compression=None,
            wait_for_ready=None,
            timeout=None,
            metadata=None):
        return grpc.experimental.unary_unary(request, target, '/hls.HLSService/InitTask',
            hls__service__pb2.InitTaskRequest.SerializeToString,
            hls__service__pb2.InitTaskResponse.FromString,
            options, channel_credentials,
            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)

    @staticmethod
    def AddSegment(request,
            target,
            options=(),
            channel_credentials=None,
            call_credentials=None,
            insecure=False,
            compression=None,
            wait_for_ready=None,
            timeout=None,
            metadata=None):
        return grpc.experimental.unary_unary(request, target, '/hls.HLSService/AddSegment',
            hls__service__pb2.AddSegmentRequest.SerializeToString,
            hls__service__pb2.AddSegmentResponse.FromString,
            options, channel_credentials,
            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)

    @staticmethod
    def FinalizeTask(request,
            target,
            options=(),
            channel_credentials=None,
            call_credentials=None,
            insecure=False,
            compression=None,
            wait_for_ready=None,
            timeout=None,
            metadata=None):
        return grpc.experimental.unary_unary(request, target, '/hls.HLSService/FinalizeTask',
            hls__service__pb2.FinalizeTaskRequest.SerializeToString,
            hls__service__pb2.FinalizeTaskResponse.FromString,
            options, channel_credentials,
            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)

    @staticmethod
    def CleanupTask(request,
            target,
            options=(),
            channel_credentials=None,
            call_credentials=None,
            insecure=False,
            compression=None,
            wait_for_ready=None,
            timeout=None,
            metadata=None):
        return grpc.experimental.unary_unary(request, target, '/hls.HLSService/CleanupTask',
            hls__service__pb2.CleanupTaskRequest.SerializeToString,
            hls__service__pb2.CleanupTaskResponse.FromString,
            options, channel_credentials,
            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)

================
File: services/hls/proto/hls_service_pb2.py
================
# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: hls_service.proto
"""Generated protocol buffer code."""
from google.protobuf import descriptor as _descriptor
from google.protobuf import descriptor_pool as _descriptor_pool
from google.protobuf import symbol_database as _symbol_database
from google.protobuf.internal import builder as _builder
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()




DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x11hls_service.proto\x12\x03hls\"\"\n\x0fInitTaskRequest\x12\x0f\n\x07task_id\x18\x01 \x01(\t\"4\n\x10InitTaskResponse\x12\x0f\n\x07success\x18\x01 \x01(\x08\x12\x0f\n\x07message\x18\x02 \x01(\t\"Q\n\x11\x41\x64\x64SegmentRequest\x12\x0f\n\x07task_id\x18\x01 \x01(\t\x12\x14\n\x0csegment_path\x18\x02 \x01(\t\x12\x15\n\rsegment_index\x18\x03 \x01(\x05\"K\n\x12\x41\x64\x64SegmentResponse\x12\x0f\n\x07success\x18\x01 \x01(\x08\x12\x0f\n\x07message\x18\x02 \x01(\t\x12\x13\n\x0bsegment_url\x18\x03 \x01(\t\"&\n\x13\x46inalizeTaskRequest\x12\x0f\n\x07task_id\x18\x01 \x01(\t\"N\n\x14\x46inalizeTaskResponse\x12\x0f\n\x07success\x18\x01 \x01(\x08\x12\x0f\n\x07message\x18\x02 \x01(\t\x12\x14\n\x0cplaylist_url\x18\x03 \x01(\t\"%\n\x12\x43leanupTaskRequest\x12\x0f\n\x07task_id\x18\x01 \x01(\t\"7\n\x13\x43leanupTaskResponse\x12\x0f\n\x07success\x18\x01 \x01(\x08\x12\x0f\n\x07message\x18\x02 \x01(\t2\x8b\x02\n\nHLSService\x12\x37\n\x08InitTask\x12\x14.hls.InitTaskRequest\x1a\x15.hls.InitTaskResponse\x12=\n\nAddSegment\x12\x16.hls.AddSegmentRequest\x1a\x17.hls.AddSegmentResponse\x12\x43\n\x0c\x46inalizeTask\x12\x18.hls.FinalizeTaskRequest\x1a\x19.hls.FinalizeTaskResponse\x12@\n\x0b\x43leanupTask\x12\x17.hls.CleanupTaskRequest\x1a\x18.hls.CleanupTaskResponseb\x06proto3')

_globals = globals()
_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)
_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'hls_service_pb2', _globals)
if _descriptor._USE_C_DESCRIPTORS == False:

  DESCRIPTOR._options = None
  _globals['_INITTASKREQUEST']._serialized_start=26
  _globals['_INITTASKREQUEST']._serialized_end=60
  _globals['_INITTASKRESPONSE']._serialized_start=62
  _globals['_INITTASKRESPONSE']._serialized_end=114
  _globals['_ADDSEGMENTREQUEST']._serialized_start=116
  _globals['_ADDSEGMENTREQUEST']._serialized_end=197
  _globals['_ADDSEGMENTRESPONSE']._serialized_start=199
  _globals['_ADDSEGMENTRESPONSE']._serialized_end=274
  _globals['_FINALIZETASKREQUEST']._serialized_start=276
  _globals['_FINALIZETASKREQUEST']._serialized_end=314
  _globals['_FINALIZETASKRESPONSE']._serialized_start=316
  _globals['_FINALIZETASKRESPONSE']._serialized_end=394
  _globals['_CLEANUPTASKREQUEST']._serialized_start=396
  _globals['_CLEANUPTASKREQUEST']._serialized_end=433
  _globals['_CLEANUPTASKRESPONSE']._serialized_start=435
  _globals['_CLEANUPTASKRESPONSE']._serialized_end=490
  _globals['_HLSSERVICE']._serialized_start=493
  _globals['_HLSSERVICE']._serialized_end=760
# @@protoc_insertion_point(module_scope)

================
File: services/hls/proto/hls_service.proto
================
syntax = "proto3";

package hls;

// HLS服务定义
service HLSService {
  // 初始化任务
  rpc InitTask(InitTaskRequest) returns (InitTaskResponse);
  
  // 添加视频片段
  rpc AddSegment(AddSegmentRequest) returns (AddSegmentResponse);
  
  // 完成任务
  rpc FinalizeTask(FinalizeTaskRequest) returns (FinalizeTaskResponse);
  
  // 清理任务资源
  rpc CleanupTask(CleanupTaskRequest) returns (CleanupTaskResponse);
}

// 初始化任务请求
message InitTaskRequest {
  string task_id = 1;
}

message InitTaskResponse {
  bool success = 1;
  string message = 2;
}

// 添加片段请求
message AddSegmentRequest {
  string task_id = 1;
  string segment_path = 2;  // 暂时保留本地路径
  int32 segment_index = 3;
}

message AddSegmentResponse {
  bool success = 1;
  string message = 2;
  string segment_url = 3;  // 返回可访问的URL
}

// 完成任务请求
message FinalizeTaskRequest {
  string task_id = 1;
}

message FinalizeTaskResponse {
  bool success = 1;
  string message = 2;
  string playlist_url = 3;  // 返回最终的播放列表URL
}

// 清理任务请求
message CleanupTaskRequest {
  string task_id = 1;
}

message CleanupTaskResponse {
  bool success = 1;
  string message = 2;
}

================
File: services/hls/__init__.py
================
from .service import HLSService
from .client import HLSClient
from .server import HLSServicer, serve

__all__ = ['HLSService', 'HLSClient', 'HLSServicer', 'serve']

================
File: services/hls/client.py
================
import logging
import grpc
from pathlib import Path
import asyncio
from .proto import hls_service_pb2
from .proto import hls_service_pb2_grpc
from config import Config

logger = logging.getLogger(__name__)

class HLSClient:
    """HLS gRPC客户端"""
    
    def __init__(self, config):
        self.config = config
        self.channel = None
        self.stub = None
        self._max_retries = 3
        self._retry_delay = 1.0  # 重试延迟（秒）
        
    @classmethod
    async def create(cls, config):
        """异步工厂方法创建客户端实例"""
        self = cls(config)
        await self._init_client()
        return self
        
    async def _init_client(self):
        """异步初始化客户端"""
        try:
            host = self.config.HLS_SERVICE_HOST
            port = self.config.HLS_SERVICE_PORT
            self.channel = grpc.aio.insecure_channel(f"{host}:{port}")
            self.stub = hls_service_pb2_grpc.HLSServiceStub(self.channel)
        except Exception as e:
            logger.error(f"初始化HLS客户端失败: {str(e)}")
            raise

    async def _ensure_connection(self):
        """确保连接可用，如果断开则尝试重连"""
        if not self.channel or self.channel.get_state() in [grpc.ChannelConnectivity.SHUTDOWN, grpc.ChannelConnectivity.TRANSIENT_FAILURE]:
            await self._init_client()
            
    async def _retry_rpc(self, rpc_func, *args, **kwargs):
        """带重试机制的 RPC 调用"""
        for attempt in range(self._max_retries):
            try:
                await self._ensure_connection()
                return await rpc_func(*args, **kwargs)
            except grpc.aio.AioRpcError as e:
                if e.code() == grpc.StatusCode.UNAVAILABLE:
                    if attempt < self._max_retries - 1:
                        logger.warning(f"RPC调用失败，{self._retry_delay}秒后重试 (尝试 {attempt + 1}/{self._max_retries})")
                        await asyncio.sleep(self._retry_delay)
                        continue
                raise
        return False
        
    async def init_task(self, task_id: str) -> bool:
        """初始化HLS任务"""
        try:
            async def _init():
                request = hls_service_pb2.InitTaskRequest(task_id=task_id)
                response = await self.stub.InitTask(request)
                if not response.success:
                    logger.error(f"初始化任务失败: {response.message}")
                return response.success
            return await self._retry_rpc(_init)
        except Exception as e:
            logger.error(f"调用InitTask失败: {str(e)}")
            return False
            
    async def add_segment(self, task_id: str, segment_path: Path, segment_index: int) -> bool:
        """添加视频片段"""
        try:
            async def _add():
                request = hls_service_pb2.AddSegmentRequest(
                    task_id=task_id,
                    segment_path=str(segment_path),
                    segment_index=segment_index
                )
                response = await self.stub.AddSegment(request)
                if not response.success:
                    logger.error(f"添加片段失败: {response.message}")
                return response.success
            return await self._retry_rpc(_add)
        except Exception as e:
            logger.error(f"调用AddSegment失败: {str(e)}")
            return False
            
    async def finalize_task(self, task_id: str) -> bool:
        """完成HLS任务"""
        try:
            async def _finalize():
                request = hls_service_pb2.FinalizeTaskRequest(task_id=task_id)
                response = await self.stub.FinalizeTask(request)
                if not response.success:
                    logger.error(f"完成任务失败: {response.message}")
                return response.success
            return await self._retry_rpc(_finalize)
        except Exception as e:
            logger.error(f"调用FinalizeTask失败: {str(e)}")
            return False
            
    async def cleanup_task(self, task_id: str) -> bool:
        """清理任务资源"""
        try:
            async def _cleanup():
                request = hls_service_pb2.CleanupTaskRequest(task_id=task_id)
                response = await self.stub.CleanupTask(request)
                if not response.success:
                    logger.error(f"清理任务失败: {response.message}")
                return response.success
            return await self._retry_rpc(_cleanup)
        except Exception as e:
            logger.error(f"调用CleanupTask失败: {str(e)}")
            return False
            
    async def close(self):
        """关闭gRPC通道"""
        if self.channel:
            await self.channel.close()

================
File: services/hls/server.py
================
import logging
from concurrent import futures
import grpc
from pathlib import Path

from .proto import hls_service_pb2
from .proto import hls_service_pb2_grpc
from .service import HLSService
from ..storage_service import LocalStorageService
from config import Config

logger = logging.getLogger(__name__)

class HLSServicer(hls_service_pb2_grpc.HLSServiceServicer):
    """HLS gRPC服务实现"""
    
    def __init__(self, config: Config):
        self.config = config
        self.storage_service = LocalStorageService(config)
        self.hls_service = HLSService(config, self.storage_service)
        self._active_tasks = set()  # 跟踪活动的任务
        
    async def InitTask(self, request, context):
        """初始化HLS任务"""
        try:
            task_id = request.task_id
            if task_id in self._active_tasks:
                return hls_service_pb2.InitTaskResponse(
                    success=False,
                    message=f"任务 {task_id} 已存在"
                )
            
            await self.hls_service.init_task(task_id)
            self._active_tasks.add(task_id)
            return hls_service_pb2.InitTaskResponse(
                success=True,
                message=f"任务 {task_id} 初始化成功"
            )
        except Exception as e:
            logger.error(f"初始化任务失败: {str(e)}")
            return hls_service_pb2.InitTaskResponse(
                success=False,
                message=str(e)
            )
    
    async def AddSegment(self, request, context):
        """添加视频片段"""
        try:
            task_id = request.task_id
            if task_id not in self._active_tasks:
                return hls_service_pb2.AddSegmentResponse(
                    success=False,
                    message=f"任务 {task_id} 不存在或未初始化"
                )
                
            segment_path = Path(request.segment_path)
            if not segment_path.exists():
                return hls_service_pb2.AddSegmentResponse(
                    success=False,
                    message=f"片段文件不存在: {segment_path}"
                )
                
            success = await self.hls_service.add_segment(
                task_id,
                segment_path,
                request.segment_index
            )
            
            if success:
                return hls_service_pb2.AddSegmentResponse(
                    success=True,
                    message="片段添加成功",
                    segment_url=f"/segments/{task_id}/segment_{request.segment_index}.ts"
                )
            else:
                return hls_service_pb2.AddSegmentResponse(
                    success=False,
                    message="片段添加失败"
                )
                
        except Exception as e:
            logger.error(f"添加片段失败: {str(e)}")
            return hls_service_pb2.AddSegmentResponse(
                success=False,
                message=str(e)
            )
    
    async def FinalizeTask(self, request, context):
        """完成HLS任务"""
        try:
            task_id = request.task_id
            if task_id not in self._active_tasks:
                return hls_service_pb2.FinalizeTaskResponse(
                    success=False,
                    message=f"任务 {task_id} 不存在或未初始化"
                )
            
            await self.hls_service.finalize_task(task_id)
            return hls_service_pb2.FinalizeTaskResponse(
                success=True,
                message=f"任务 {task_id} 完成",
                playlist_url=f"/playlists/{task_id}.m3u8"
            )
        except Exception as e:
            logger.error(f"完成任务失败: {str(e)}")
            return hls_service_pb2.FinalizeTaskResponse(
                success=False,
                message=str(e)
            )
    
    async def CleanupTask(self, request, context):
        """清理任务资源"""
        try:
            task_id = request.task_id
            if task_id not in self._active_tasks:
                return hls_service_pb2.CleanupTaskResponse(
                    success=False,
                    message=f"任务 {task_id} 不存在或未初始化"
                )
            
            await self.hls_service.cleanup_task(task_id)
            self._active_tasks.remove(task_id)
            return hls_service_pb2.CleanupTaskResponse(
                success=True,
                message=f"任务 {task_id} 资源已清理"
            )
        except Exception as e:
            logger.error(f"清理任务失败: {str(e)}")
            return hls_service_pb2.CleanupTaskResponse(
                success=False,
                message=str(e)
            )

def serve(config: Config):
    """启动 gRPC 服务器"""
    server = grpc.aio.server(futures.ThreadPoolExecutor(max_workers=10))
    hls_service_pb2_grpc.add_HLSServiceServicer_to_server(
        HLSServicer(config), server
    )
    server.add_insecure_port(f"{config.HLS_GRPC_HOST}:{config.HLS_GRPC_PORT}")
    return server

================
File: services/hls/service.py
================
import m3u8
import logging
from pathlib import Path
from typing import Optional, Dict
from ..storage_service import StorageService
from utils.ffmpeg_utils import FFmpegTool

logger = logging.getLogger(__name__)

class HLSService:
    """HLS 流媒体服务"""
    
    def __init__(self, config, storage_service: StorageService):
        self.config = config
        self.storage = storage_service
        self.playlists: Dict[str, m3u8.M3U8] = {}
        self.logger = logger
        self.ffmpeg_tool = FFmpegTool()
        
    def _create_playlist(self, task_id: str) -> m3u8.M3U8:
        """创建新的播放列表"""
        playlist = m3u8.M3U8()
        playlist.version = 3
        playlist.target_duration = self.config.HLS_SEGMENT_DURATION
        playlist.media_sequence = 0
        playlist.playlist_type = 'VOD'
        playlist.is_endlist = False
        
        self.playlists[task_id] = playlist
        return playlist
        
    async def init_task(self, task_id: str) -> None:
        """初始化任务的HLS资源"""
        playlist = self._create_playlist(task_id)
        await self.storage.update_playlist(task_id, playlist.dumps())
        
    async def add_segment(self, task_id: str, segment_path: Path, segment_index: int) -> bool:
        """添加新的视频片段"""
        try:
            # 1. 使用 FFmpeg 进行 HLS 分段
            segment_filename = f'segment_{segment_index}_%03d.ts'
            target_dir = self.config.PUBLIC_DIR / "segments" / task_id
            target_dir.mkdir(parents=True, exist_ok=True)
            
            segment_pattern = str(target_dir / segment_filename)
            temp_playlist_path = target_dir / f'temp_{segment_index}.m3u8'

            # 调用 FFmpeg 进行分段，添加更多参数以确保准确切片
            await self.ffmpeg_tool.hls_segment(
                input_path=str(segment_path),               
                hls_time=self.config.HLS_TIME,
                segment_pattern=segment_pattern,
                playlist_path=str(temp_playlist_path),
            )

            # 2. 读取临时播放列表并更新主播放列表
            temp_m3u8 = m3u8.load(str(temp_playlist_path))
            
            playlist = self.playlists.get(task_id)
            if not playlist:
                playlist = self._create_playlist(task_id)

            # 添加不连续标记
            discontinuity_segment = m3u8.Segment(discontinuity=True)
            playlist.segments.append(discontinuity_segment)

            # 添加分片
            for segment in temp_m3u8.segments:
                segment.uri = f"/segments/{task_id}/{Path(segment.uri).name}"
                playlist.segments.append(segment)

            # 3. 保存更新后的播放列表
            await self.storage.update_playlist(
                task_id,
                playlist.dumps()
            )
            
            # 4. 清理临时播放列表
            if temp_playlist_path.exists():
                temp_playlist_path.unlink()

            self.logger.debug(f"已添加分片 {segment_index} 到任务 {task_id}")
            return True
            
        except Exception as e:
            self.logger.error(f"添加分片失败: {e}")
            return False
            
    async def finalize_task(self, task_id: str) -> None:
        """完成任务的HLS处理"""
        try:
            playlist = self.playlists.get(task_id)
            if playlist:
                playlist.is_endlist = True
                await self.storage.update_playlist(
                    task_id,
                    playlist.dumps()
                )
                # 清理缓存
                self.playlists.pop(task_id, None)
                self.logger.info(f"任务 {task_id} 的HLS流已完成")
        except Exception as e:
            self.logger.error(f"完成HLS流失败: {e}")
            
    async def cleanup_task(self, task_id: str) -> None:
        """清理任务的所有HLS资源"""
        try:
            await self.storage.cleanup_task(task_id)
            self.playlists.pop(task_id, None)
            self.logger.info(f"已清理任务 {task_id} 的HLS资源")
        except Exception as e:
            self.logger.error(f"清理HLS资源失败: {e}")

================
File: services/__init__.py
================
from .storage_service import StorageService, LocalStorageService
from .hls import HLSService, HLSClient, HLSServicer, serve

__all__ = [
    'StorageService',
    'LocalStorageService',
    'HLSService',
    'HLSClient',
    'HLSServicer',
    'serve'
]

================
File: services/storage_service.py
================
from abc import ABC, abstractmethod
from pathlib import Path
import logging
import shutil
import aiofiles
from typing import Optional

logger = logging.getLogger(__name__)

class StorageService(ABC):
    """存储服务抽象基类"""
    
    @abstractmethod
    async def upload_segment(self, task_id: str, segment_path: Path, segment_index: int) -> str:
        """
        上传分片并返回访问路径
        Args:
            task_id: 任务ID
            segment_path: 分片文件路径
            segment_index: 分片索引
        Returns:
            str: 分片访问路径
        """
        pass
    
    @abstractmethod
    async def update_playlist(self, task_id: str, playlist_content: str) -> str:
        """
        更新播放列表并返回访问路径
        Args:
            task_id: 任务ID
            playlist_content: m3u8内容
        Returns:
            str: 播放列表访问路径
        """
        pass
        
    @abstractmethod
    async def cleanup_task(self, task_id: str) -> None:
        """清理任务相关的存储资源"""
        pass

class LocalStorageService(StorageService):
    """本地文件系统存储实现"""
    
    def __init__(self, config):
        self.config = config
        self.public_dir = config.PUBLIC_DIR
        
    async def upload_segment(self, task_id: str, segment_path: Path, segment_index: int) -> str:
        """本地存储实现 - 复制文件到公共目录"""
        target_dir = self.public_dir / "segments" / task_id
        target_dir.mkdir(parents=True, exist_ok=True)
        
        target_path = target_dir / f"segment_{segment_index}.ts"
        shutil.copy2(str(segment_path), str(target_path))
        
        return f"/segments/{task_id}/segment_{segment_index}.ts"
        
    async def update_playlist(self, task_id: str, playlist_content: str) -> str:
        """本地存储实现 - 写入m3u8文件"""
        playlist_path = self.public_dir / "playlists" / f"{task_id}.m3u8"
        async with aiofiles.open(playlist_path, 'w') as f:
            await f.write(playlist_content)
        return f"/playlists/{task_id}.m3u8"
        
    async def cleanup_task(self, task_id: str) -> None:
        """清理任务文件"""
        # 清理分片
        segment_dir = self.public_dir / "segments" / task_id
        if segment_dir.exists():
            shutil.rmtree(str(segment_dir))
            
        # 清理播放列表    
        playlist_path = self.public_dir / "playlists" / f"{task_id}.m3u8"
        if playlist_path.exists():
            playlist_path.unlink()

================
File: utils/concurrency.py
================
# utils/concurrency.py
import functools
import asyncio
import os
from concurrent.futures import ThreadPoolExecutor

CPU_COUNT = os.cpu_count() or 1
GLOBAL_EXECUTOR = ThreadPoolExecutor(max_workers=CPU_COUNT)

async def run_sync(func, *args, **kwargs):
    loop = asyncio.get_running_loop()
    partial_func = functools.partial(func, *args, **kwargs)
    return await loop.run_in_executor(GLOBAL_EXECUTOR, partial_func)

================
File: utils/decorators.py
================
import logging
import functools
from typing import Callable, Any, Optional, AsyncGenerator, TypeVar, Union, Literal
import asyncio
import time

logger = logging.getLogger(__name__)
T = TypeVar('T')
WorkerResult = Union[T, AsyncGenerator[T, None]]
WorkerMode = Literal['base', 'stream']

def handle_errors(custom_logger: Optional[logging.Logger] = None) -> Callable:
    """错误处理装饰器。可应用于需要统一捕获日志的异步函数。"""
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        async def wrapper(*args, **kwargs) -> Any:
            # 如果当前对象有 logger 属性则使用，否则用传入的或全局 logger
            actual_logger = custom_logger if custom_logger else (getattr(args[0], 'logger', logger) if args else logger)
            start_time = time.time()
            try:
                result = await func(*args, **kwargs)
                elapsed = time.time() - start_time
                actual_logger.debug(f"{func.__name__} 正常结束，耗时 {elapsed:.2f}s")
                return result
            except Exception as e:
                elapsed = time.time() - start_time
                actual_logger.error(f"{func.__name__} 执行出错，耗时 {elapsed:.2f}s, 错误: {e}", exc_info=True)
                raise
        return wrapper
    return decorator

def worker_decorator(
    input_queue_attr: str,
    next_queue_attr: Optional[str] = None,
    worker_name: Optional[str] = None,
    mode: WorkerMode = 'base'
) -> Callable:
    """通用 Worker 装饰器"""
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        async def wrapper(self, task_state, *args, **kwargs):
            worker_display_name = worker_name or func.__name__
            wlogger = getattr(self, 'logger', logger)

            input_queue = getattr(task_state, input_queue_attr)
            next_queue = getattr(task_state, next_queue_attr) if next_queue_attr else None

            wlogger.info(f"[{worker_display_name}] 启动 (TaskID={task_state.task_id}). "
                         f"输入队列: {input_queue_attr}, 下游队列: {next_queue_attr if next_queue_attr else '无'}")

            processed_count = 0
            try:
                while True:
                    try:
                        queue_size_before = input_queue.qsize()
                        item = await input_queue.get()
                        if item is None:
                            if next_queue:
                                await next_queue.put(None)
                            wlogger.info(f"[{worker_display_name}] 收到停止信号。已处理 {processed_count} 个item。")
                            break

                        wlogger.debug(f"[{worker_display_name}] 从 {input_queue_attr} 取出一个item. 队列剩余: {queue_size_before}")

                        start_time = time.time()
                        if mode == 'stream':
                            async for result in func(self, item, task_state, *args, **kwargs):
                                if result is not None and next_queue:
                                    await next_queue.put(result)
                        else:
                            result = await func(self, item, task_state, *args, **kwargs)
                            if result is not None and next_queue:
                                await next_queue.put(result)

                        processed_count += 1
                        elapsed = time.time() - start_time
                        wlogger.debug(f"[{worker_display_name}] item处理完成，耗时 {elapsed:.2f}s. "
                                      f"TaskID={task_state.task_id}, 已处理计数: {processed_count}")

                    except asyncio.CancelledError:
                        wlogger.warning(f"[{worker_display_name}] 被取消 (TaskID={task_state.task_id}). "
                                        f"已处理 {processed_count} 个item")
                        if next_queue:
                            await next_queue.put(None)
                        break
                    except Exception as e:
                        wlogger.error(f"[{worker_display_name}] 发生异常: {e} (TaskID={task_state.task_id}). "
                                      f"已处理 {processed_count} 个item", exc_info=True)
                        if next_queue:
                            await next_queue.put(None)
                        break
            finally:
                wlogger.info(f"[{worker_display_name}] 结束 (TaskID={task_state.task_id}). 共处理 {processed_count} 个item.")

        return wrapper
    return decorator

================
File: utils/ffmpeg_utils.py
================
# --------------------------------------
# utils/ffmpeg_utils.py
# 彻底移除 force_style, 仅使用 .ass 内部样式
# --------------------------------------
import asyncio
import logging
from pathlib import Path
from typing import List, Tuple, Optional

logger = logging.getLogger(__name__)

class FFmpegTool:
    """
    统一封装 FFmpeg 常见用法的工具类。
    通过异步方式执行 ffmpeg 命令，并在出错时抛出异常。
    """

    async def run_command(self, cmd: List[str]) -> Tuple[bytes, bytes]:
        """
        运行 ffmpeg 命令，返回 (stdout, stderr)。
        若命令返回码非 0，则抛出 RuntimeError。
        """
        logger.debug(f"[FFmpegTool] Running command: {' '.join(cmd)}")
        process = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        stdout, stderr = await process.communicate()

        if process.returncode != 0:
            error_msg = stderr.decode() or "Unknown error"
            logger.error(f"[FFmpegTool] Command failed with error: {error_msg}")
            raise RuntimeError(f"FFmpeg command failed: {error_msg}")

        return stdout, stderr

    async def extract_audio(
        self,
        input_path: str,
        output_path: str,
        start: float = 0.0,
        duration: Optional[float] = None
    ) -> None:
        """
        提取音频，可选指定起始时间与持续时长。
        输出为单声道 PCM float32 (48k/16k 视需求).
        """
        cmd = ["ffmpeg", "-y", "-i", input_path]
        if start > 0:
            cmd += ["-ss", str(start)]
        if duration is not None:
            cmd += ["-t", str(duration)]

        cmd += [
            "-vn",                # 去掉视频
            "-acodec", "pcm_f32le",
            "-ac", "1",
            output_path
        ]
        await self.run_command(cmd)

    async def extract_video(
        self,
        input_path: str,
        output_path: str,
        start: float = 0.0,
        duration: Optional[float] = None
    ) -> None:
        """
        提取纯视频（去掉音轨），可选指定起始时间与持续时长。
        """
        cmd = ["ffmpeg", "-y", "-i", input_path]
        if start > 0:
            cmd += ["-ss", str(start)]
        if duration is not None:
            cmd += ["-t", str(duration)]

        cmd += [
            "-an",                # 去掉音频
            "-c:v", "libx264",
            "-preset", "ultrafast",
            "-crf", "18",
            "-tune", "fastdecode",
            output_path
        ]
        await self.run_command(cmd)

    async def hls_segment(
        self,
        input_path: str,
        segment_pattern: str,
        playlist_path: str,
        hls_time: int = 10,
        hls_flags: str = "independent_segments",
    ) -> None:
        """
        将输入视频切分成HLS分片
        Args:
            input_path: 输入视频路径
            segment_pattern: 分片文件名模式
            playlist_path: 临时m3u8文件路径
            hls_time: 每个分片的目标时长(秒)
            hls_flags: HLS特殊标志
            extra_options: 额外的 FFmpeg 选项
        """
        cmd = [
            "ffmpeg", "-y",
            "-i", input_path,
            "-c", "copy",
            "-f", "hls",
            "-hls_time", str(hls_time),
            "-hls_list_size", "0",
            "-hls_segment_type", "mpegts",
            "-hls_segment_filename", segment_pattern,
            playlist_path
        ]
        
        await self.run_command(cmd)

    async def cut_video_track(
        self,
        input_path: str,
        output_path: str,
        start: float,
        end: float
    ) -> None:
        """
        截取 [start, end] 的无声视频段，end为绝对秒数。
        """
        duration = end - start
        if duration <= 0:
            raise ValueError(f"Invalid duration: {duration}")

        cmd = [
            "ffmpeg", "-y",
            "-i", input_path,
            "-ss", str(start),
            "-t", str(duration),
            "-c:v", "libx264",
            "-preset", "superfast",
            "-an",  # 去除音轨
            "-vsync", "vfr",
            output_path
        ]
        await self.run_command(cmd)

    async def cut_video_with_audio(
        self,
        input_video_path: str,
        input_audio_path: str,
        output_path: str
    ) -> None:
        """
        将无声视频与音频合并 (视频copy，音频AAC)。
        """
        cmd = [
            "ffmpeg", "-y",
            "-i", input_video_path,
            "-i", input_audio_path,
            "-c:v", "copy",
            "-c:a", "aac",
            output_path
        ]
        await self.run_command(cmd)

    async def cut_video_with_subtitles_and_audio(
        self,
        input_video_path: str,
        input_audio_path: str,
        subtitles_path: str,
        output_path: str
    ) -> None:
        """
        将无声视频 + 音频 + .ass字幕 合并输出到 output_path。
        (无 force_style, 由 .ass 内样式全权决定)
        
        若字幕渲染失败，则回退到仅合并音视频。
        """
        # 检查输入文件是否存在
        for file_path in [input_video_path, input_audio_path, subtitles_path]:
            if not Path(file_path).exists():
                raise FileNotFoundError(f"文件不存在: {file_path}")

        # 构建"subtitles"过滤器, 不带 force_style
        escaped_path = subtitles_path.replace(':', r'\\:')

        try:
            # 方式1: subtitles 滤镜
            # 当前设置是合理的，但需要注意：
            cmd = [
                "ffmpeg", "-y",
                "-i", input_video_path,
                "-i", input_audio_path,
                "-filter_complex",
                f"[0:v]scale=1920:-2:flags=lanczos,subtitles='{escaped_path}'[v]",  # 修改点说明：
                # 1. scale=1920:-2 保持宽高比，-2 保证高度为偶数（兼容编码要求）
                # 2. flags=lanczos 使用高质量的缩放算法
                # 3. 滤镜顺序：先缩放视频，再加字幕（确保字幕在缩放后的画面上）
                "-map", "[v]",
                "-map", "1:a",
                "-c:v", "libx264",
                "-preset", "superfast",
                "-crf", "23",  # 建议添加 CRF 参数控制视频质量
                "-c:a", "aac",
                output_path
            ]
            await self.run_command(cmd)

        except RuntimeError as e:
            logger.warning(f"[FFmpegTool] subtitles滤镜方案失败: {str(e)}")
            # 方式2: 最终回退 - 仅合并音视频
            cmd = [
                "ffmpeg", "-y",
                "-i", input_video_path,
                "-i", input_audio_path,
                "-c:v", "copy",
                "-c:a", "aac",
                output_path
            ]
            await self.run_command(cmd)
            logger.warning("[FFmpegTool] 已跳过字幕，仅合并音视频")

    async def get_duration(self, input_path: str) -> float:
        """
        调用 ffprobe 获取输入文件的时长(秒)。
        """
        cmd = [
            "ffprobe",
            "-v", "error",
            "-show_entries", "format=duration",
            "-of", "default=noprint_wrappers=1:nokey=1",
            input_path
        ]
        stdout, stderr = await self.run_command(cmd)
        return float(stdout.decode().strip())

================
File: utils/sentence_logger.py
================
import logging
import json
from pathlib import Path
from typing import List, Dict, Any
import asyncio
from utils.decorators import handle_errors

logger = logging.getLogger(__name__)

class SentenceLogger:
    """句子日志记录器"""
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self._lock = asyncio.Lock()
    
    def _format_sentence(self, sentence: Dict[str, Any]) -> Dict[str, Any]:
        return {
            'id': getattr(sentence, 'sentence_id', -1),
            'start_time': getattr(sentence, 'start', 0),
            'end_time': getattr(sentence, 'end', 0),
            'text': getattr(sentence, 'text', ''),
            'translation': getattr(sentence, 'translation', ''),
            'duration': getattr(sentence, 'duration', 0),
            'speaker_id': getattr(sentence, 'speaker_id', 0),
            'speaker_similarity': getattr(sentence, 'speaker_similarity', 0),
            'speaker_embedding': (
                getattr(sentence, 'speaker_embedding', []).tolist() 
                if hasattr(getattr(sentence, 'speaker_embedding', []), 'tolist') 
                else getattr(sentence, 'speaker_embedding', [])
            )
        }
    
    @handle_errors(logger)
    async def save_sentences(self, sentences: List[Dict[str, Any]], output_path: Path, task_id: str) -> None:
        async with self._lock:
            try:
                formatted_sentences = [self._format_sentence(s) for s in sentences]
                output_path.parent.mkdir(parents=True, exist_ok=True)
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(formatted_sentences, f, ensure_ascii=False, indent=2)
                
                self.logger.debug(f"已保存 {len(sentences)} 个句子到 {output_path}")
            except Exception as e:
                self.logger.error(f"保存句子信息失败: {e}")
                raise

================
File: utils/task_state.py
================
# ---------------------------------
# backend/utils/task_state.py (完整可复制版本)
# ---------------------------------
from dataclasses import dataclass, field
from typing import Any, Dict
import asyncio
from utils.task_storage import TaskPaths

@dataclass
class TaskState:
    """
    每个任务的独立状态：包括队列、处理进度、分段信息等
    """
    task_id: str
    task_paths: TaskPaths
    hls_manager: Any = None
    target_language: str = "zh"
    generate_subtitle: bool = False

    # 已处理到的句子计数
    sentence_counter: int = 0

    # 时间戳记录
    current_time: float = 0

    # 第几个 HLS 批次 (混音后输出)
    batch_counter: int = 0

    # 每个分段对应的媒体文件信息
    segment_media_files: Dict[int, Dict[str, Any]] = field(default_factory=dict)

    # 各个异步队列 (按处理流程顺序排列)
    segment_init_queue: asyncio.Queue = field(default_factory=asyncio.Queue)  # 分段初始化队列
    segment_queue: asyncio.Queue = field(default_factory=asyncio.Queue)       # 分段提取队列
    asr_queue: asyncio.Queue = field(default_factory=asyncio.Queue)          # ASR处理队列
    translation_queue: asyncio.Queue = field(default_factory=asyncio.Queue)   # 翻译队列
    modelin_queue: asyncio.Queue = field(default_factory=asyncio.Queue)      # 模型输入队列
    tts_token_queue: asyncio.Queue = field(default_factory=asyncio.Queue)    # TTS Token队列
    duration_align_queue: asyncio.Queue = field(default_factory=asyncio.Queue) # 时长对齐队列
    audio_gen_queue: asyncio.Queue = field(default_factory=asyncio.Queue)    # 音频生成队列
    mixing_queue: asyncio.Queue = field(default_factory=asyncio.Queue)       # 混音队列

    # 记录 mixing_worker 产出的每个 segment_xxx.mp4
    merged_segments: list = field(default_factory=list)

    # 错误记录
    errors: list = field(default_factory=list)

================
File: utils/task_storage.py
================
import shutil
import logging
import os
from pathlib import Path
from config import Config

logger = logging.getLogger(__name__)

class TaskPaths:
    def __init__(self, config: Config, task_id: str):
        self.config = config
        self.task_id = task_id

        self.task_dir = config.TASKS_DIR / task_id
        self.input_dir = self.task_dir / "input"
        self.processing_dir = self.task_dir / "processing"
        self.output_dir = self.task_dir / "output"

        self.segments_dir = config.PUBLIC_DIR / "segments" / task_id
        self.playlist_path = config.PUBLIC_DIR / "playlists" / f"playlist_{task_id}.m3u8"

        self.media_dir = self.processing_dir / "media"
        self.processing_segments_dir = self.processing_dir / "segments"

    def create_directories(self):
        dirs = [
            self.task_dir,
            self.input_dir,
            self.processing_dir,
            self.output_dir,
            self.segments_dir,
            self.media_dir,
            self.processing_segments_dir
        ]
        for d in dirs:
            d.mkdir(parents=True, exist_ok=True)
            logger.debug(f"[TaskPaths] 创建目录: {d}")

    async def cleanup(self, keep_output: bool = False):
        try:
            if keep_output:
                logger.info(f"[TaskPaths] 保留输出目录, 即将清理输入/processing/segments")
                dirs_to_clean = [self.input_dir, self.processing_dir, self.segments_dir]
                for d in dirs_to_clean:
                    if d.exists():
                        shutil.rmtree(d)
                        logger.debug(f"[TaskPaths] 已清理: {d}")
            else:
                logger.info(f"[TaskPaths] 全量清理任务目录: {self.task_dir}")
                if self.task_dir.exists():
                    shutil.rmtree(str(self.task_dir))
                    logger.debug(f"[TaskPaths] 已删除: {self.task_dir}")

                if self.segments_dir.exists():
                    shutil.rmtree(str(self.segments_dir))
                    logger.debug(f"[TaskPaths] 已删除: {self.segments_dir}")
        except Exception as e:
            logger.error(f"[TaskPaths] 清理任务目录失败: {e}", exc_info=True)
            raise

================
File: utils/temp_file_manager.py
================
import logging
from pathlib import Path
from typing import Set

logger = logging.getLogger(__name__)

class TempFileManager:
    """临时文件管理器"""
    def __init__(self, base_dir: Path):
        self.base_dir = base_dir
        self.temp_files: Set[Path] = set()
    
    def add_file(self, file_path: Path) -> None:
        self.temp_files.add(Path(file_path))
    
    async def cleanup(self) -> None:
        for file_path in self.temp_files:
            try:
                if file_path.exists():
                    file_path.unlink()
                    logger.debug(f"已删除临时文件: {file_path}")
            except Exception as e:
                logger.warning(f"清理临时文件失败: {file_path}, 错误: {e}")
        self.temp_files.clear()

================
File: workers/asr_worker/auto_sense.py
================
import logging
import os
import importlib.util
import sys
import torch
import numpy as np
from tqdm import tqdm
from pathlib import Path
import time
from funasr.register import tables
from funasr.auto.auto_model import AutoModel as BaseAutoModel
from funasr.auto.auto_model import prepare_data_iterator
from funasr.utils.misc import deep_update
from funasr.models.campplus.utils import sv_chunk, postprocess
from funasr.models.campplus.cluster_backend import ClusterBackend
from .sentence_tools import get_sentences
from funasr.utils.vad_utils import slice_padding_audio_samples
from funasr.utils.load_utils import load_audio_text_image_video

# [MODIFIED] 新增以下导入，用于在 async 函数中包装同步调用
from utils import concurrency
from functools import partial

class SenseAutoModel(BaseAutoModel):
    def __init__(self, config, **kwargs):
        super().__init__(**kwargs)
        self.logger = logging.getLogger(__name__)
        self.config = config
        
        if self.spk_model is not None:
            self.cb_model = ClusterBackend().to(kwargs["device"])
            spk_mode = kwargs.get("spk_mode", "punc_segment")
            if spk_mode not in ["default", "vad_segment", "punc_segment"]:
                self.logger.error("spk_mode 应该是 'default', 'vad_segment' 或 'punc_segment' 之一。")
            self.spk_mode = spk_mode

    def inference_with_vad(self, input, input_len=None, **cfg):
        kwargs = self.kwargs
        self.tokenizer = kwargs.get("tokenizer")
        deep_update(self.vad_kwargs, cfg)
        
        res = self.inference(input, input_len=input_len, model=self.vad_model, kwargs=self.vad_kwargs, **cfg)
        model = self.model
        deep_update(kwargs, cfg)
        kwargs["batch_size"] = max(int(kwargs.get("batch_size_s", 300)) * 1000, 1)
        batch_size_threshold_ms = int(kwargs.get("batch_size_threshold_s", 60)) * 1000

        key_list, data_list = prepare_data_iterator(input, input_len=input_len, data_type=kwargs.get("data_type", None))
        results_ret_list = []

        pbar_total = tqdm(total=len(res), dynamic_ncols=True, disable=kwargs.get("disable_pbar", False))

        for i, item in enumerate(res):
            key, vadsegments = item["key"], item["value"]
            input_i = data_list[i]
            fs = kwargs["frontend"].fs if hasattr(kwargs["frontend"], "fs") else 16000
            speech = load_audio_text_image_video(input_i, fs=fs, audio_fs=kwargs.get("fs", 16000))
            speech_lengths = len(speech)
            self.logger.debug(f"音频长度: {speech_lengths} 样本")

            if speech_lengths < 400:
                self.logger.warning(f"音频太短（{speech_lengths} 样本），可能导致处理错误")

            sorted_data = sorted([(seg, idx) for idx, seg in enumerate(vadsegments)], key=lambda x: x[0][1] - x[0][0])
            if not sorted_data:
                self.logger.info(f"解码, utt: {key}, 空语音")
                continue

            results_sorted = []
            all_segments = []
            beg_idx, end_idx = 0, 1
            max_len_in_batch = 0

            for j in range(len(sorted_data)):
                sample_length = sorted_data[j][0][1] - sorted_data[j][0][0]
                potential_batch_length = max(max_len_in_batch, sample_length) * (j + 1 - beg_idx)

                if (j < len(sorted_data) - 1 and 
                    sample_length < batch_size_threshold_ms and 
                    potential_batch_length < kwargs["batch_size"]):
                    max_len_in_batch = max(max_len_in_batch, sample_length)
                    end_idx += 1
                    continue

                speech_j, _ = slice_padding_audio_samples(speech, speech_lengths, sorted_data[beg_idx:end_idx])
                results = self.inference(speech_j, input_len=None, model=model, kwargs=kwargs, **cfg)

                if self.spk_model is not None:
                    for _b, speech_segment in enumerate(speech_j):
                        vad_segment = sorted_data[beg_idx:end_idx][_b][0]
                        segments = sv_chunk([[vad_segment[0] / 1000.0, vad_segment[1] / 1000.0, np.array(speech_segment)]])
                        all_segments.extend(segments)
                        speech_b = [seg[2] for seg in segments]
                        spk_res = self.inference(speech_b, input_len=None, model=self.spk_model, kwargs=kwargs, **cfg)
                        results[_b]["spk_embedding"] = spk_res[0]["spk_embedding"]
                beg_idx, end_idx = end_idx, end_idx + 1
                max_len_in_batch = sample_length
                results_sorted.extend(results)

            if len(results_sorted) != len(sorted_data):
                self.logger.info(f"解码，utt: {key}，空结果")
                continue

            restored_data = [0] * len(sorted_data)
            for j, (_, idx) in enumerate(sorted_data):
                restored_data[idx] = results_sorted[j]

            result = self.combine_results(restored_data, vadsegments)

            if self.spk_model is not None and kwargs.get("return_spk_res", True):
                all_segments.sort(key=lambda x: x[0])
                spk_embedding = result["spk_embedding"]
                labels = self.cb_model(spk_embedding.cpu(), oracle_num=kwargs.get("preset_spk_num", None))
                sv_output = postprocess(all_segments, None, labels, spk_embedding.cpu())

                if "timestamp" not in result:
                    self.logger.error(f"speaker diarization 依赖于时间戳对于 utt: {key}")
                    sentence_list = []
                else:
                    sentence_list = get_sentences(
                        tokens=result["token"],
                        timestamps=result["timestamp"],
                        tokenizer=self.tokenizer,
                        speech=speech,
                        sd_time_list=sv_output,
                        sample_rate=fs,
                        config=self.config
                    )
                    results_ret_list = sentence_list
            else:
                sentence_list = []
            pbar_total.update(1)

        pbar_total.close()
        return results_ret_list

    def combine_results(self, restored_data, vadsegments):
        result = {}
        for j, data in enumerate(restored_data):
            for k, v in data.items():
                if k.startswith("timestamp"):
                    if k not in result:
                        result[k] = []
                    for t in v:
                        t[0] += vadsegments[j][0]
                        t[1] += vadsegments[j][0]
                    result[k].extend(v)
                elif k == "spk_embedding":
                    if k not in result:
                        result[k] = v
                    else:
                        result[k] = torch.cat([result[k], v], dim=0)
                elif "token" in k:
                    if k not in result:
                        result[k] = v
                    else:
                        result[k].extend(v)
                else:
                    if k not in result:
                        result[k] = v
                    else:
                        result[k] += v
        return result

    # [MODIFIED] 统一使用 concurrency.run_sync 来执行 self.generate
    async def generate_async(self, input, input_len=None, **cfg):
        func = partial(self.generate, input, input_len, **cfg)
        return await concurrency.run_sync(func)

================
File: workers/asr_worker/sentence_tools.py
================
import os
import torch
import torchaudio
import numpy as np
from typing import List, Tuple, Dict
from dataclasses import dataclass, field
from pathlib import Path
from config import Config

Token = int
Timestamp = Tuple[float, float]
SpeakerSegment = Tuple[float, float, int]

@dataclass
class Sentence:
    raw_text: str
    start: float
    end: float
    speaker_id: int
    trans_text: str = field(default="")
    sentence_id: int = field(default=-1)
    audio: torch.Tensor = field(default=None)
    target_duration: float = field(default=None)
    duration: float = field(default=0.0)
    diff: float = field(default=0.0)
    silence_duration: float = field(default=0.0)
    speed: float = field(default=1.0)
    is_first: bool = field(default=False)
    is_last: bool = field(default=False)
    model_input: Dict = field(default_factory=dict)
    generated_audio: np.ndarray = field(default=None)
    adjusted_start: float = field(default=0.0)
    adjusted_duration: float = field(default=0.0)
    segment_index: int = field(default=-1)
    segment_start: float = field(default=0.0)
    task_id: str = field(default="")

def tokens_timestamp_sentence(tokens: List[Token], timestamps: List[Timestamp], speaker_segments: List[SpeakerSegment], tokenizer, config: Config) -> List[Tuple[List[Token], List[Timestamp], int]]:
    sentences = []
    current_tokens = []
    current_timestamps = []
    token_index = 0

    for segment in speaker_segments:
        seg_start_ms = int(segment[0] * 1000)
        seg_end_ms = int(segment[1] * 1000)
        speaker_id = segment[2]

        while token_index < len(tokens):
            token = tokens[token_index]
            token_start, token_end = timestamps[token_index]

            if token_start >= seg_end_ms:
                break
            if token_end <= seg_start_ms:
                token_index += 1
                continue

            current_tokens.append(token)
            current_timestamps.append(timestamps[token_index])
            token_index += 1

            if token in config.STRONG_END_TOKENS and len(current_tokens) <= config.MIN_SENTENCE_LENGTH:
                if sentences:
                    previous_end_time = sentences[-1][1][-1][1]
                    current_start_time = current_timestamps[0][0]
                    time_gap = current_start_time - previous_end_time

                    if time_gap > config.SHORT_SENTENCE_MERGE_THRESHOLD_MS:
                        continue

                    sentences[-1] = (
                        sentences[-1][0] + current_tokens[:],
                        sentences[-1][1] + current_timestamps[:],
                        sentences[-1][2]
                    )
                    current_tokens.clear()
                    current_timestamps.clear()
                continue

            if (token in config.STRONG_END_TOKENS or len(current_tokens) > config.MAX_TOKENS_PER_SENTENCE):
                sentences.append((current_tokens[:], current_timestamps[:], speaker_id))
                current_tokens.clear()
                current_timestamps.clear()

        if current_tokens:
            if len(current_tokens) >= config.MIN_SENTENCE_LENGTH or not sentences:
                sentences.append((current_tokens[:], current_timestamps[:], speaker_id))
                current_tokens.clear()
                current_timestamps.clear()
            else:
                continue

    if current_tokens:
        if len(current_tokens) >= config.MIN_SENTENCE_LENGTH or not sentences:
            sentences.append((current_tokens[:], current_timestamps[:], speaker_id))
            current_tokens.clear()
            current_timestamps.clear()
        else:
            sentences[-1] = (
                sentences[-1][0] + current_tokens[:],
                sentences[-1][1] + current_timestamps[:],
                sentences[-1][2]
            )
            current_tokens.clear()
            current_timestamps.clear()

    return sentences

def merge_sentences(raw_sentences: List[Tuple[List[Token], List[Timestamp], int]], 
                   tokenizer,
                   input_duration: float,
                   config: Config) -> List[Sentence]:
    merged_sentences = []
    current = None
    current_tokens_count = 0

    for tokens, timestamps, speaker_id in raw_sentences:
        time_gap = timestamps[0][0] - current.end if current else float('inf')
        
        if (current and 
            current.speaker_id == speaker_id and 
            current_tokens_count + len(tokens) <= config.MAX_TOKENS_PER_SENTENCE and
            time_gap <= config.MAX_GAP_MS):
            current.raw_text += tokenizer.decode(tokens)
            current.end = timestamps[-1][1]
            current_tokens_count += len(tokens)
        else:
            if current:
                current.target_duration = timestamps[0][0] - current.start
                merged_sentences.append(current)
            
            text = tokenizer.decode(tokens)
            current = Sentence(
                raw_text=text, 
                start=timestamps[0][0], 
                end=timestamps[-1][1], 
                speaker_id=speaker_id,
            )
            current_tokens_count = len(tokens)

    if current:
        current.target_duration = input_duration - current.start
        merged_sentences.append(current)

    if merged_sentences:
        merged_sentences[0].is_first = True
        merged_sentences[-1].is_last = True

    return merged_sentences

def extract_audio(sentences: List[Sentence], speech: torch.Tensor, sr: int, config: Config) -> List[Sentence]:
    target_samples = int(config.SPEAKER_AUDIO_TARGET_DURATION * sr)
    speech = speech.unsqueeze(0) if speech.dim() == 1 else speech

    speaker_segments: Dict[int, List[Tuple[int, int, int]]] = {}
    for idx, s in enumerate(sentences):
        start_sample = int(s.start * sr / 1000)
        end_sample = int(s.end * sr / 1000)
        speaker_segments.setdefault(s.speaker_id, []).append((start_sample, end_sample, idx))

    speaker_audio_cache: Dict[int, torch.Tensor] = {}

    for speaker_id, segments in speaker_segments.items():
        segments.sort(key=lambda x: x[1] - x[0], reverse=True)
        longest_start, longest_end, _ = segments[0]

        ignore_samples = int(0.5 * sr)
        adjusted_start = longest_start + ignore_samples
        available_length_adjusted = longest_end - adjusted_start

        if available_length_adjusted > 0:
            audio_length = min(target_samples, available_length_adjusted)
            speaker_audio = speech[:, adjusted_start:adjusted_start + audio_length]
        else:
            available_length_original = longest_end - longest_start
            audio_length = min(target_samples, available_length_original)
            speaker_audio = speech[:, longest_start:longest_start + audio_length]

        speaker_audio_cache[speaker_id] = speaker_audio

    for sentence in sentences:
        sentence.audio = speaker_audio_cache.get(sentence.speaker_id)

    output_dir = Path(config.TASKS_DIR) / sentences[0].task_id / 'speakers'
    output_dir.mkdir(parents=True, exist_ok=True)

    for speaker_id, audio in speaker_audio_cache.items():
        if audio is not None:
            output_path = output_dir / f'speaker_{speaker_id}.wav'
            torchaudio.save(str(output_path), audio, sr)

    return sentences

def get_sentences(tokens: List[Token],
                  timestamps: List[Timestamp],
                  speech: torch.Tensor,
                  tokenizer,
                  sd_time_list: List[SpeakerSegment],
                  sample_rate: int = 16000,
                  config: Config = None) -> List[Sentence]:
    if config is None:
        config = Config()

    input_duration = (speech.shape[-1] / sample_rate) * 1000

    raw_sentences = tokens_timestamp_sentence(tokens, timestamps, sd_time_list, tokenizer, config)
    merged_sentences = merge_sentences(raw_sentences, tokenizer, input_duration, config)
    sentences_with_audio = extract_audio(merged_sentences, speech, sample_rate, config)

    return sentences_with_audio

================
File: workers/asr_worker/worker.py
================
import logging
from typing import Dict, Any
from utils.task_state import TaskState
from utils.decorators import worker_decorator
from .auto_sense import SenseAutoModel

logger = logging.getLogger(__name__)

class ASRWorker:
    """语音识别 Worker"""

    def __init__(self, config):
        """初始化 ASRWorker"""
        self.config = config
        self.logger = logger
        
        # 直接初始化 ASR 模型
        self.sense_model = SenseAutoModel(
            config=config,
            **config.ASR_MODEL_KWARGS
        )

    @worker_decorator(
        input_queue_attr='asr_queue',
        next_queue_attr='translation_queue',
        worker_name='ASR Worker'
    )
    async def run(self, segment_info: Dict[str, Any], task_state: TaskState) -> Dict[str, Any]:
        """处理音频识别"""
        try:
            self.logger.debug(
                f"[ASR Worker] 开始处理分段 {segment_info['segment_index']} -> TaskID={task_state.task_id}"
            )
            # 调用 ASR 模型（异步接口）
            sentences = await self.sense_model.generate_async(
                input=segment_info['vocals_path'],
                cache={},
                language="auto",
                use_itn=True,
                batch_size_s=60,
                merge_vad=False
            )
            if not sentences:
                self.logger.warning(
                    f"[ASR Worker] 分段 {segment_info['segment_index']} 未识别到语音 -> TaskID={task_state.task_id}"
                )
                return None

            self.logger.info(
                f"[ASR Worker] 识别到 {len(sentences)} 条句子, seg={segment_info['segment_index']}, TaskID={task_state.task_id}"
            )
            for s in sentences:
                s.segment_index = segment_info['segment_index']
                s.segment_start = segment_info['start']
                s.task_id = task_state.task_id
                s.sentence_id = task_state.sentence_counter
                task_state.sentence_counter += 1

            return sentences

        except Exception as e:
            self.logger.error(
                f"[ASR Worker] 分段 {segment_info['segment_index']} 处理失败: {e} -> TaskID={task_state.task_id}",
                exc_info=True
            )
            task_state.errors.append({
                'segment_index': segment_info['segment_index'],
                'stage': 'asr',
                'error': str(e)
            })
            return None

if __name__ == '__main__':
    print("ASR Worker 模块加载成功")

================
File: workers/audio_gen_worker/audio_gener.py
================
# core/audio_gener.py

import logging
import asyncio
import numpy as np
from typing import List
from services.cosyvoice.client import CosyVoiceClient
from utils import concurrency

class AudioGenerator:
    def __init__(self, cosyvoice_client: CosyVoiceClient, sample_rate: int = 24000):
        self.cosyvoice_client = cosyvoice_client
        self.sample_rate = sample_rate
        self.logger = logging.getLogger(__name__)

    async def vocal_audio_maker(self, sentences: List):
        """
        并发生成音频
        """
        tasks = []
        for s in sentences:
            text_uuid = s.model_input.get('text_uuid')
            speaker_uuid = s.model_input.get('speaker_uuid')
            if not text_uuid or not speaker_uuid:
                self.logger.warning("缺少text_uuid或speaker_uuid，无法生成音频")
                continue

            tasks.append(self._generate_single_async(s))

        try:
            results = await asyncio.gather(*tasks)
            return results
        except Exception as e:
            self.logger.error(f"音频生成失败: {str(e)}")
            raise

    async def _generate_single_async(self, sentence):
        try:
            final_audio = await concurrency.run_sync(self._generate_audio_single, sentence)
            sentence.generated_audio = final_audio
            return sentence
        except Exception as e:
            self.logger.error(
                f"音频生成失败 (text_uuid: {sentence.model_input.get('text_uuid', 'unknown')}, "
                f"speaker_uuid: {sentence.model_input.get('speaker_uuid', 'unknown')}): {str(e)}"
            )
            sentence.generated_audio = None
            return sentence

    def _generate_audio_single(self, sentence):
        """
        使用text_uuid和speaker_uuid从服务端获取音频
        """
        text_uuid = sentence.model_input.get('text_uuid')
        speaker_uuid = sentence.model_input.get('speaker_uuid')
        if not text_uuid or not speaker_uuid:
            self.logger.warning("缺少text_uuid或speaker_uuid，无法生成音频")
            return np.zeros(0, dtype=np.float32)

        speed = getattr(sentence, 'speed', 1.0) or 1.0
        audio_np, dur_sec = self.cosyvoice_client.token2wav(text_uuid, speaker_uuid, speed=speed)

        if getattr(sentence, 'is_first', False) and getattr(sentence, 'start', 0) > 0:
            silence_samples = int(sentence.start * self.sample_rate / 1000)
            audio_np = np.concatenate([
                np.zeros(silence_samples, dtype=np.float32),
                audio_np
            ])

        if hasattr(sentence, 'silence_duration') and sentence.silence_duration > 0:
            silence_samples = int(sentence.silence_duration * self.sample_rate / 1000)
            audio_np = np.concatenate([
                audio_np,
                np.zeros(silence_samples, dtype=np.float32)
            ])

        self.logger.debug(
            f"音频生成完成 (text_uuid={text_uuid}, speaker_uuid={speaker_uuid}), "
            f"长度={len(audio_np)/self.sample_rate:.2f}s"
        )
        return audio_np

================
File: workers/audio_gen_worker/timestamp_adjuster.py
================
import logging
from typing import List, Optional

logger = logging.getLogger(__name__)

class TimestampAdjuster:
    """句子时间戳调整器"""
    
    def __init__(self, config):
        """初始化时间戳调整器
        
        Args:
            config: 配置对象，用于获取采样率
        """
        self.logger = logging.getLogger(__name__)
        self.sample_rate = config.SAMPLE_RATE
        
    def update_timestamps(self, sentences: List, start_time: Optional[float] = None) -> float:
        """更新句子的时间戳信息
        
        Args:
            sentences: 要更新的句子列表
            start_time: 起始时间（毫秒），如果为 None 则使用第一个句子的开始时间
            
        Returns:
            float: 最后一个句子结束的时间点（毫秒）
        """
        if not sentences:
            return start_time if start_time is not None else 0
            
        # 使用传入的起始时间或第一个句子的开始时间
        current_time = start_time if start_time is not None else sentences[0].start
        
        for sentence in sentences:
            # 计算实际音频长度（毫秒）
            if sentence.generated_audio is not None:
                actual_duration = (len(sentence.generated_audio) / self.sample_rate) * 1000
            else:
                actual_duration = 0
                self.logger.warning(f"句子 {sentence.sentence_id} 没有生成音频")
            
            # 更新时间戳
            sentence.adjusted_start = current_time
            sentence.adjusted_duration = actual_duration
            
            # 更新差异值
            sentence.diff = sentence.duration - actual_duration
            
            # 更新下一个句子的开始时间
            current_time += actual_duration
            
        return current_time
    
    def validate_timestamps(self, sentences: List) -> bool:
        """验证时间戳的连续性和有效性
        
        Args:
            sentences: 要验证的句子列表
            
        Returns:
            bool: 时间戳是否有效
        """
        if not sentences:
            return True
            
        for i in range(len(sentences) - 1):
            current = sentences[i]
            next_sentence = sentences[i + 1]
            
            # 验证时间连续性
            expected_next_start = current.adjusted_start + current.adjusted_duration
            if abs(next_sentence.adjusted_start - expected_next_start) > 1:  # 允许1毫秒的误差
                self.logger.error(
                    f"时间戳不连续 - 句子 {current.sentence_id} 结束时间: {expected_next_start:.2f}ms, "
                    f"句子 {next_sentence.sentence_id} 开始时间: {next_sentence.adjusted_start:.2f}ms"
                )
                return False
                
            # 验证时长有效性
            if current.adjusted_duration <= 0:
                self.logger.error(f"句子 {current.sentence_id} 的时长无效: {current.adjusted_duration:.2f}ms")
                return False
                
        return True

================
File: workers/audio_gen_worker/worker.py
================
import logging
from typing import List, Any
from utils.decorators import worker_decorator
from utils.task_state import TaskState
from .audio_gener import AudioGenerator
from .timestamp_adjuster import TimestampAdjuster
from services.cosyvoice.client import CosyVoiceClient


logger = logging.getLogger(__name__)

class AudioGenWorker:
    """
    音频生成 Worker：利用 AudioGenerator 对句子生成合成音频。
    """

    def __init__(self, config):
        """初始化 AudioGenWorker"""
        self.config = config
        self.logger = logger
        
        # 初始化 CosyVoiceClient
        cosyvoice_address = f"{config.COSYVOICE_SERVICE_HOST}:{config.COSYVOICE_SERVICE_PORT}"
        cosyvoice_client = CosyVoiceClient(address=cosyvoice_address)
        self.audio_generator = AudioGenerator(
            cosyvoice_client=cosyvoice_client,
            sample_rate=config.SAMPLE_RATE
        )
        self.timestamp_adjuster = TimestampAdjuster(config=config)

    @worker_decorator(
        input_queue_attr='audio_gen_queue',
        next_queue_attr='mixing_queue',
        worker_name='音频生成 Worker'
    )
    async def run(self, sentences_batch: List[Any], task_state: TaskState):
        if not sentences_batch:
            return
        self.logger.debug(f"[音频生成 Worker] 收到 {len(sentences_batch)} 句子, TaskID={task_state.task_id}")

        await self.audio_generator.vocal_audio_maker(sentences_batch)
        task_state.current_time = self.timestamp_adjuster.update_timestamps(sentences_batch, start_time=task_state.current_time)
        valid = self.timestamp_adjuster.validate_timestamps(sentences_batch)
        if not valid:
            self.logger.warning(f"[音频生成 Worker] 检测到时间戳不连续, TaskID={task_state.task_id}")
        return sentences_batch

if __name__ == '__main__':
    print("Audio Generation Worker 模块加载成功")

================
File: workers/duration_worker/duration_aligner.py
================
import logging

class DurationAligner:
    def __init__(self, model_in=None, simplifier=None, tts_token_gener=None, max_speed=1.1):
        """
        model_in：生成模型接口，用于更新文本特征  
        simplifier：简化处理接口（Translator）  
        tts_token_gener：TTS token 生成接口  
        max_speed：语速阈值，超过该速率的句子需要进行简化
        """
        self.model_in = model_in
        self.simplifier = simplifier
        self.tts_token_gener = tts_token_gener
        self.max_speed = max_speed
        self.logger = logging.getLogger(__name__)

    async def align_durations(self, sentences):
        """
        对整批句子进行时长对齐，并检查是否需要精简（语速过快）。
        """
        if not sentences:
            return

        # 第一次对齐
        self._align_batch(sentences)

        # 查找语速过快的句子（speed > max_speed）
        retry_sentences = [s for s in sentences if s.speed > self.max_speed]
        if retry_sentences:
            self.logger.info(f"{len(retry_sentences)} 个句子语速过快, 正在精简...")
            success = await self._retry_sentences_batch(retry_sentences)
            if success:
                # 若精简文本成功，再次对齐
                self._align_batch(sentences)
            else:
                self.logger.warning("精简过程失败, 保持原结果")

    def _align_batch(self, sentences):
        """
        同批次句子进行时长对齐。
        """
        if not sentences:
            return

        # 计算每句需要调整的时间差
        for s in sentences:
            s.diff = s.duration - s.target_duration

        total_diff_to_adjust = sum(s.diff for s in sentences)
        positive_diff_sum = sum(x.diff for x in sentences if x.diff > 0)
        negative_diff_sum_abs = sum(abs(x.diff) for x in sentences if x.diff < 0)
        current_time = sentences[0].start

        for s in sentences:
            s.adjusted_start = current_time
            diff = s.diff
            s.speed = 1.0
            s.silence_duration = 0.0
            s.adjusted_duration = s.duration

            if total_diff_to_adjust != 0:
                if total_diff_to_adjust > 0 and diff > 0:
                    if positive_diff_sum > 0:
                        proportion = diff / positive_diff_sum
                        adjustment = total_diff_to_adjust * proportion
                        s.adjusted_duration = s.duration - adjustment
                        s.speed = s.duration / max(s.adjusted_duration, 0.001)
                elif total_diff_to_adjust < 0 and diff < 0:
                    if negative_diff_sum_abs > 0:
                        proportion = abs(diff) / negative_diff_sum_abs
                        total_needed = abs(total_diff_to_adjust) * proportion
                        max_slowdown = s.duration * 0.07
                        slowdown = min(total_needed, max_slowdown)
                        s.adjusted_duration = s.duration + slowdown
                        s.speed = s.duration / max(s.adjusted_duration, 0.001)
                        s.silence_duration = total_needed - slowdown
                        if s.silence_duration > 0:
                            s.adjusted_duration += s.silence_duration

            s.diff = s.duration - s.adjusted_duration
            current_time += s.adjusted_duration

            self.logger.info(
                f"对齐后: {s.trans_text}, duration: {s.duration}, "
                f"target_duration: {s.target_duration}, diff: {s.diff}, "
                f"speed: {s.speed}, silence_duration: {s.silence_duration}"
            )

    async def _retry_sentences_batch(self, sentences):
        """
        对语速过快的句子执行精简 + 更新 TTS token。
        """
        try:
            # 1. 分批对语速过快的句子进行精简
            async for _ in self.simplifier.simplify_sentences(sentences, target_speed=self.max_speed):
                pass
            # 2. 批量更新文本特征（复用 speaker 与 uuid）
            async for batch in self.model_in.modelin_maker(
                sentences,
                reuse_speaker=True,
                batch_size=3
            ):
                # 3. 再生成 token（复用 uuid）
                updated_batch = await self.tts_token_gener.tts_token_maker(batch)
            return True
        except Exception as e:
            self.logger.error(f"_retry_sentences_batch 出错: {e}")
            return False

================
File: workers/duration_worker/worker.py
================
import logging
from typing import List, Any
from utils.decorators import worker_decorator
from utils.task_state import TaskState
from .duration_aligner import DurationAligner
from workers.modelin_worker.model_in import ModelIn
from workers.tts_worker.tts_token_gener import TTSTokenGenerator
from workers.translation_worker.translation.translator import Translator
from workers.translation_worker.translation.deepseek_client import DeepSeekClient
from workers.translation_worker.translation.gemini_client import GeminiClient
from services.cosyvoice.client import CosyVoiceClient

logger = logging.getLogger(__name__)

class DurationWorker:
    """
    时长对齐 Worker：调用 DurationAligner 对句子进行时长调整。
    """

    def __init__(self, config):
        """初始化 DurationWorker"""
        self.config = config
        self.logger = logger
        
        # 初始化 CosyVoiceClient
        cosyvoice_address = f"{config.COSYVOICE_SERVICE_HOST}:{config.COSYVOICE_SERVICE_PORT}"
        cosyvoice_client = CosyVoiceClient(address=cosyvoice_address)
        
        # 初始化依赖组件
        model_in = ModelIn(cosyvoice_client=cosyvoice_client, max_concurrent_tasks=config.MAX_PARALLEL_SEGMENTS)
        tts_token_generator = TTSTokenGenerator(cosyvoice_client=cosyvoice_client)
        
        # 初始化翻译客户端
        translation_model = config.TRANSLATION_MODEL.lower()
        if translation_model == "deepseek":
            client = DeepSeekClient(api_key=config.DEEPSEEK_API_KEY)
        elif translation_model == "gemini":
            client = GeminiClient(api_key=config.GEMINI_API_KEY)
        else:
            raise ValueError(f"不支持的翻译模型: {translation_model}")
        simplifier = Translator(translation_client=client)
        
        # 直接初始化 DurationAligner
        self.duration_aligner = DurationAligner(
            model_in=model_in,
            simplifier=simplifier,
            tts_token_gener=tts_token_generator,
            max_speed=1.2
        )

    @worker_decorator(
        input_queue_attr='duration_align_queue',
        next_queue_attr='audio_gen_queue',
        worker_name='时长对齐 Worker'
    )
    async def run(self, sentences_batch: List[Any], task_state: TaskState):
        if not sentences_batch:
            return
        self.logger.debug(f"[时长对齐 Worker] 收到 {len(sentences_batch)} 句子, TaskID={task_state.task_id}")

        await self.duration_aligner.align_durations(sentences_batch)
        return sentences_batch

if __name__ == '__main__':
    print("Duration Worker 模块加载成功")

================
File: workers/mixer_worker/media_mixer.py
================
# ---------------------------------------------------
# backend/workers/mixer_worker/media_mixer.py
# ---------------------------------------------------
import numpy as np
import logging
import soundfile as sf
import os
import asyncio
from contextlib import ExitStack
from tempfile import NamedTemporaryFile
from typing import List, Any

import pysubs2

from utils.decorators import handle_errors
from utils.ffmpeg_utils import FFmpegTool
from config import Config
from utils.task_state import TaskState

logger = logging.getLogger(__name__)

class MediaMixer:
    """
    用于将多段句子的合成音频与原视频片段混合，并可生成带字幕的视频。
    支持:
      - 音频淡入淡出
      - 背景音乐混合
      - 基于 pysubs2 生成 .ass 字幕("YouTube风格")
      - 按语言自动决定单行最大长度
    """
    def __init__(self, config: Config):  # 移除 sample_rate 参数，从 config 获取
        self.config = config
        self.sample_rate = config.SAMPLE_RATE  # 从 config 获取全局采样率

        # 音量相关
        self.max_val = 1.0
        self.overlap = self.config.AUDIO_OVERLAP
        self.vocals_volume = self.config.VOCALS_VOLUME
        self.background_volume = self.config.BACKGROUND_VOLUME

        # 全局缓存, 可按需使用
        self.full_audio_buffer = np.array([], dtype=np.float32)

        self.ffmpeg_tool = FFmpegTool()

    @handle_errors(logger)
    async def mixed_media_maker(
        self,
        sentences: List[Any],
        task_state: TaskState,
        output_path: str,
        generate_subtitle: bool = False
    ) -> bool:
        """
        主入口: 处理一批句子的音频与视频，输出一段带音频的 MP4。
        根据 generate_subtitle 决定是否烧制字幕。

        Args:
            sentences: 本片段内的所有句子对象
            task_state: 任务状态，内部包含 target_language 等
            output_path: 生成的 MP4 文件路径
            generate_subtitle: 是否在最终视频里烧制字幕

        Returns:
            True / False 表示成功或失败
        """
        if not sentences:
            logger.warning("mixed_media_maker: 收到空的句子列表")
            return False

        segment_index = sentences[0].segment_index
        segment_files = task_state.segment_media_files.get(segment_index)
        if not segment_files:
            logger.error(f"找不到分段 {segment_index} 对应的媒体文件信息")
            return False

        # =========== (1) 拼接所有句子的合成音频 =============
        full_audio = np.array([], dtype=np.float32)
        for sentence in sentences:
            if sentence.generated_audio is not None:
                audio_data = np.asarray(sentence.generated_audio, dtype=np.float32)
                # 如果已经有前面累积的音频，做淡入淡出衔接
                if len(full_audio) > 0:
                    audio_data = self._apply_fade_effect(audio_data)
                full_audio = np.concatenate((full_audio, audio_data))
            else:
                logger.warning(
                    "句子音频生成失败: text=%r, UUID=%s",
                    sentence.raw_text,  # 或 sentence.trans_text
                    sentence.model_input.get("uuid", "unknown")
                )

        if len(full_audio) == 0:
            logger.error("mixed_media_maker: 没有有效的合成音频数据")
            return False

        # 计算当前片段的起始时间和时长(秒)
        start_time = 0.0
        if not sentences[0].is_first:
            start_time = (sentences[0].adjusted_start - sentences[0].segment_start * 1000) / 1000.0

        duration = sum(s.adjusted_duration for s in sentences) / 1000.0

        # =========== (2) 背景音乐混合 (可选) =============
        background_audio_path = segment_files['background']
        if background_audio_path is not None:
            full_audio = self._mix_with_background(
                bg_path=background_audio_path,
                start_time=start_time,
                duration=duration,
                audio_data=full_audio
            )
            full_audio = self._normalize_audio(full_audio)

        # (可按需储存到全局 mixer 缓存)
        self.full_audio_buffer = np.concatenate((self.full_audio_buffer, full_audio))

        # =========== (3) 如果有视频，就把音频合并到视频里 ============
        video_path = segment_files['video']
        if video_path:
            await self._add_video_segment(
                video_path=video_path,
                start_time=start_time,
                duration=duration,
                audio_data=full_audio,
                output_path=output_path,
                sentences=sentences,
                generate_subtitle=generate_subtitle,
                task_state=task_state  # 传入以获取 target_language
            )
            return True

        logger.warning("mixed_media_maker: 本片段无video_path可用")
        return False

    def _apply_fade_effect(self, audio_data: np.ndarray) -> np.ndarray:
        """在语音片段衔接处做 overlap 长度的淡入淡出衔接。"""
        if audio_data is None or len(audio_data) == 0:
            return np.array([], dtype=np.float32)

        cross_len = min(self.overlap, len(self.full_audio_buffer), len(audio_data))
        if cross_len <= 0:
            return audio_data

        fade_out = np.sqrt(np.linspace(1.0, 0.0, cross_len, dtype=np.float32))
        fade_in  = np.sqrt(np.linspace(0.0, 1.0, cross_len, dtype=np.float32))

        audio_data = audio_data.copy()
        overlap_region = self.full_audio_buffer[-cross_len:]

        audio_data[:cross_len] = overlap_region * fade_out + audio_data[:cross_len] * fade_in
        return audio_data

    def _mix_with_background(
        self,
        bg_path: str,
        start_time: float,
        duration: float,
        audio_data: np.ndarray
    ) -> np.ndarray:
        """
        从 bg_path 读取背景音乐，在 [start_time, start_time+duration] 区间截取，
        与 audio_data (人声) 混合。
        """
        background_audio, sr = sf.read(bg_path)
        background_audio = np.asarray(background_audio, dtype=np.float32)
        if sr != self.sample_rate:
            logger.warning(
                f"背景音采样率={sr} 与目标={self.sample_rate}不匹配, 未做重采样, 可能有问题."
            )

        target_length = int(duration * self.sample_rate)
        start_sample = int(start_time * self.sample_rate)
        end_sample   = start_sample + target_length

        if end_sample <= len(background_audio):
            bg_segment = background_audio[start_sample:end_sample]
        else:
            bg_segment = background_audio[start_sample:]

        result = np.zeros(target_length, dtype=np.float32)
        audio_len = min(len(audio_data), target_length)
        bg_len    = min(len(bg_segment), target_length)

        # 混合人声 & 背景
        if audio_len > 0:
            result[:audio_len] = audio_data[:audio_len] * self.vocals_volume
        if bg_len > 0:
            result[:bg_len] += bg_segment[:bg_len] * self.background_volume

        return result

    def _normalize_audio(self, audio_data: np.ndarray) -> np.ndarray:
        """对音频做简单归一化"""
        if len(audio_data) == 0:
            return audio_data
        max_val = np.max(np.abs(audio_data))
        if max_val > self.max_val:
            audio_data = audio_data * (self.max_val / max_val)
        return audio_data

    @handle_errors(logger)
    async def _add_video_segment(
        self,
        video_path: str,
        start_time: float,
        duration: float,
        audio_data: np.ndarray,
        output_path: str,
        sentences: List[Any],
        generate_subtitle: bool,
        task_state: TaskState
    ):
        """
        从原视频里截取 [start_time, start_time + duration] 的视频段(无声)，
        与合成音频合并。
        若 generate_subtitle=True, 则生成 .ass 字幕并在 ffmpeg 工具中进行"烧制"。
        """
        if not os.path.exists(video_path):
            raise FileNotFoundError(f"_add_video_segment: 视频文件不存在: {video_path}")
        if len(audio_data) == 0:
            raise ValueError("_add_video_segment: 无音频数据")
        if duration <= 0:
            raise ValueError("_add_video_segment: 无效时长 <=0")

        with ExitStack() as stack:
            temp_video = stack.enter_context(NamedTemporaryFile(suffix='.mp4'))
            temp_audio = stack.enter_context(NamedTemporaryFile(suffix='.wav'))

            end_time = start_time + duration

            # 1) 截取视频 (无音轨)
            await self.ffmpeg_tool.cut_video_track(
                input_path=video_path,
                output_path=temp_video.name,
                start=start_time,
                end=end_time
            )

            # 2) 写合成音频到临时文件
            await asyncio.to_thread(sf.write, temp_audio.name, audio_data, self.sample_rate)

            # 3) 如果需要字幕，则构建 .ass 并用 ffmpeg "烧"进去
            if generate_subtitle:
                temp_ass = stack.enter_context(NamedTemporaryFile(suffix='.ass'))
                await asyncio.to_thread(
                    self._generate_subtitles_for_segment,
                    sentences,
                    start_time * 1000,   # segment_start_ms
                    temp_ass.name,
                    task_state.target_language
                )

                # 生成带字幕的视频
                await self.ffmpeg_tool.cut_video_with_subtitles_and_audio(
                    input_video_path=temp_video.name,
                    input_audio_path=temp_audio.name,
                    subtitles_path=temp_ass.name,
                    output_path=output_path
                )
            else:
                # 不加字幕，仅合并音频
                await self.ffmpeg_tool.cut_video_with_audio(
                    input_video_path=temp_video.name,
                    input_audio_path=temp_audio.name,
                    output_path=output_path
                )

    def _generate_subtitles_for_segment(
        self,
        sentences: List[Any],
        segment_start_ms: float,
        output_sub_path: str,
        target_language: str = "en"
    ):
        """生成 ASS 字幕文件"""
        subs = pysubs2.SSAFile()

        for s in sentences:
            # 计算相对时间
            start_local = s.adjusted_start - segment_start_ms - s.segment_start * 1000

            sub_text = (s.trans_text or s.raw_text or "").strip()
            if not sub_text:
                continue

            # 直接使用adjusted_duration作为duration_ms
            duration_ms = s.duration / s.speed
            if duration_ms <= 0:
                continue

            # 如果 Sentence 本身带 lang, 就优先使用 s.lang, 否则用 target_language
            lang = target_language or "en"

            # 拆分长句子 -> 多段 sequential
            blocks = self._split_long_text_to_sub_blocks(
                text=sub_text,
                start_ms=start_local,
                duration_ms=duration_ms,
                lang=lang
            )

            for block in blocks:
                evt = pysubs2.SSAEvent(
                    start=int(block["start"]),
                    end=int(block["end"]),
                    text=block["text"]
                )
                subs.append(evt)

        # 设置"类YouTube"的默认样式
        style = subs.styles.get("Default", pysubs2.SSAStyle())

        style.fontname = "Arial"             # 常见无衬线
        style.fontsize = 22
        style.bold = True
        style.italic = False
        style.underline = False

        # 颜色 (R, G, B, A=0 => 不透明)
        style.primarycolor = pysubs2.Color(255, 255, 255, 0)
        style.outlinecolor = pysubs2.Color(0, 0, 0, 100)  # 半透明黑
        style.borderstyle = 3  # 3 => 有背景块
        style.shadow = 0
        style.alignment = pysubs2.Alignment.BOTTOM_CENTER
        style.marginv = 20    # 离底部像素

        # 更新回 default
        subs.styles["Default"] = style

        # 写入文件
        subs.save(output_sub_path, format="ass")
        logger.debug(f"_generate_subtitles_for_segment: 已写入字幕 => {output_sub_path}")

    def _split_long_text_to_sub_blocks(
        self,
        text: str,
        start_ms: float,
        duration_ms: float,
        lang: str = "en"
    ) -> List[dict]:
        """将文本拆分成多块字幕"""
        recommended_max_chars = {
            "zh": 20,
            "ja": 20,
            "ko": 20,
            "en": 40
        }
        if lang not in recommended_max_chars:
            lang = "en"
        max_chars = recommended_max_chars[lang]

        if len(text) <= max_chars:
            return [{
                "start": start_ms,
                "end":   start_ms + duration_ms,
                "text":  text
            }]

        chunks = self._chunk_text_by_language(text, lang, max_chars)

        sub_blocks = []
        total_chars = sum(len(c) for c in chunks)
        current_start = start_ms

        for c in chunks:
            chunk_len = len(c)
            chunk_dur = duration_ms * (chunk_len / total_chars) if total_chars > 0 else 0
            block_start = current_start
            block_end   = current_start + chunk_dur

            sub_blocks.append({
                "start": block_start,
                "end":   block_end,
                "text":  c
            })
            current_start += chunk_dur

        if sub_blocks:
            sub_blocks[-1]["end"] = start_ms + duration_ms
        else:
            sub_blocks.append({
                "start": start_ms,
                "end":   start_ms + duration_ms,
                "text":  text
            })

        return sub_blocks

    def _chunk_text_by_language(self, text: str, lang: str, max_chars: int) -> List[str]:
        """根据语言拆分文本"""
        cjk_puncts = set("，,。.!！？?；;：:、…~— ")
        eng_puncts = set(".,!?;: ")

        if lang == "en":
            return self._chunk_english_text(text, max_chars, eng_puncts)
        else:
            return self._chunk_cjk_text(text, max_chars, cjk_puncts)

    def _chunk_english_text(self, text: str, max_chars: int, puncts: set) -> List[str]:
        """英文文本拆分"""
        words = text.split()
        chunks = []
        current_line = []

        for w in words:
            line_len = sum(len(x) for x in current_line) + len(current_line)
            if line_len + len(w) > max_chars:
                if current_line:
                    chunks.append(" ".join(current_line))
                    current_line = []
            current_line.append(w)

        if current_line:
            chunks.append(" ".join(current_line))

        return chunks

    def _chunk_cjk_text(self, text: str, max_chars: int, puncts: set) -> List[str]:
        """中日韩文本拆分"""
        chunks = []
        total_length = len(text)
        start_idx = 0

        while start_idx < total_length:
            end_idx = start_idx + max_chars
            
            if end_idx < total_length and text[end_idx] in puncts:
                end_idx += 1

            end_idx = min(end_idx, total_length)
            chunk = text[start_idx:end_idx]
            chunks.append(chunk)
            start_idx = end_idx

        return chunks

    async def reset(self):
        """重置 mixer 状态"""
        self.full_audio_buffer = np.array([], dtype=np.float32)
        logger.debug("MediaMixer 已重置 full_audio_buffer")

================
File: workers/mixer_worker/worker.py
================
import logging
from typing import List, Any
from pathlib import Path
from utils.decorators import worker_decorator
from utils.task_state import TaskState
from .media_mixer import MediaMixer
from services.hls import HLSClient

logger = logging.getLogger(__name__)

class MixerWorker:
    """
    混音 Worker：调用 MediaMixer 将生成的音频与视频混合，生成最终输出段视频。
    """

    def __init__(self, config, hls_service: HLSClient):
        """初始化 MixerWorker"""
        self.config = config
        self.hls_service = hls_service
        self.logger = logger
        
        # 直接实例化 MediaMixer
        self.mixer = MediaMixer(config=config)

    @worker_decorator(
        input_queue_attr='mixing_queue',
        worker_name='混音 Worker'
    )
    async def run(self, sentences_batch: List[Any], task_state: TaskState):
        if not sentences_batch:
            return
        seg_index = sentences_batch[0].segment_index
        self.logger.debug(f"[混音 Worker] 收到 {len(sentences_batch)} 句, segment={seg_index}, TaskID={task_state.task_id}")

        output_path = task_state.task_paths.segments_dir / f"segment_{task_state.batch_counter}.mp4"

        success = await self.mixer.mixed_media_maker(
            sentences=sentences_batch,
            task_state=task_state,
            output_path=str(output_path),
            generate_subtitle=task_state.generate_subtitle
        )

        if success:
            # 使用 HLS 服务
            added = await self.hls_service.add_segment(
                task_state.task_id,
                output_path,
                task_state.batch_counter
            )
            if added:
                self.logger.info(f"[混音 Worker] 分段 {task_state.batch_counter} 已加入 HLS, TaskID={task_state.task_id}")
                task_state.merged_segments.append(str(output_path))
            else:
                self.logger.error(f"[混音 Worker] 分段 {task_state.batch_counter} 添加到 HLS 流失败, TaskID={task_state.task_id}")

        task_state.batch_counter += 1
        return None

if __name__ == '__main__':
    print("Mixer Worker 模块加载成功")

================
File: workers/modelin_worker/model_in.py
================
import logging
import asyncio
import torch
import numpy as np
import librosa
from typing import List, Dict
import os
import uuid
import soundfile as sf

from services.cosyvoice.client import CosyVoiceClient

class ModelIn:
    def __init__(self, cosyvoice_client: CosyVoiceClient, max_concurrent_tasks: int = 4):
        self.cosyvoice_client = cosyvoice_client
        self.logger = logging.getLogger(__name__)
        self.semaphore = asyncio.Semaphore(max_concurrent_tasks)
        self.speaker_cache: Dict[str, asyncio.Task] = {}  # speaker_id -> asyncio.Task
        self.max_val = 0.8
        self.cosy_sample_rate = 24000

    def postprocess(self, speech, top_db=60, hop_length=220, win_length=440):
        """
        对音频进行 trim、幅度归一化，并在结尾加 0.2s 静音
        """
        speech, _ = librosa.effects.trim(
            speech,
            top_db=top_db,
            frame_length=win_length,
            hop_length=hop_length
        )
        if np.abs(speech).max() > self.max_val:
            speech = speech / np.abs(speech).max() * self.max_val

        pad_samples = int(self.cosy_sample_rate * 0.2)
        speech = np.concatenate([speech, np.zeros(pad_samples, dtype=speech.dtype)])
        return speech

    async def get_speaker_features(self, speaker_id: str, sentence) -> str:
        """
        异步提取说话人特征，返回说话人UUID
        """
        if speaker_id in self.speaker_cache:
            return await self.speaker_cache[speaker_id]

        async def extract_features() -> str:
            # 1. 确保是 numpy array
            audio_np = sentence.audio.squeeze(0).cpu().numpy() if isinstance(sentence.audio, torch.Tensor) else sentence.audio
            
            # 2. 处理音频，得到numpy array
            postprocessed_audio = self.postprocess(audio_np)

            speaker_uuid = str(uuid.uuid4())
            success = await asyncio.to_thread(
                self.cosyvoice_client.extract_speaker_features,
                speaker_uuid,
                postprocessed_audio,  # 直接传递numpy array
                self.cosy_sample_rate
            )
            if not success:
                raise Exception("提取说话人特征失败")
            return speaker_uuid

        task = asyncio.create_task(extract_features())
        self.speaker_cache[speaker_id] = task
        return await task

    async def _process_one_sentence_async(self, sentence, reuse_speaker: bool):
        async with self.semaphore:
            speaker_id = getattr(sentence, 'speaker_id', None)
            
            # 处理文本UUID
            text_uuid = await asyncio.to_thread(
                self.cosyvoice_client.normalize_text,
                sentence.trans_text or ""
            )
            sentence.model_input['text_uuid'] = text_uuid
            
            # 处理说话人UUID
            if speaker_id is not None and not reuse_speaker:
                speaker_uuid = await self.get_speaker_features(speaker_id, sentence)
                sentence.model_input['speaker_uuid'] = speaker_uuid
            elif 'speaker_uuid' not in sentence.model_input:
                raise ValueError("缺少说话人UUID且未提取新特征")

            return sentence

    async def modelin_maker(self, sentences: List, reuse_speaker: bool = False, batch_size: int = 3):
        """
        对一批 sentence 进行文本与说话人特征提取，按 batch_size 分批 yield 结果
        """
        if not sentences:
            self.logger.warning("modelin_maker: 收到空句子列表")
            return

        tasks = [
            asyncio.create_task(self._process_one_sentence_async(s, reuse_speaker))
            for s in sentences
        ]

        results_batch = []
        try:
            for i, task in enumerate(tasks, start=1):
                updated = await task
                if updated is not None:
                    results_batch.append(updated)
                if i % batch_size == 0:
                    yield results_batch
                    results_batch = []
            if results_batch:
                yield results_batch
        except Exception as e:
            self.logger.error(f"modelin_maker处理失败: {e}")
            raise
        finally:
            if not reuse_speaker:
                self.speaker_cache.clear()

================
File: workers/modelin_worker/worker.py
================
import logging
from typing import List, Any
from utils.decorators import worker_decorator
from utils.task_state import TaskState
from .model_in import ModelIn
from services.cosyvoice.client import CosyVoiceClient

logger = logging.getLogger(__name__)

class ModelInWorker:
    """
    模型输入 Worker：调用 ModelIn 对句子进行 speaker 特征更新、文本处理等。
    """

    def __init__(self, config):
        """初始化 ModelInWorker"""
        self.config = config
        self.logger = logger
        
        # 初始化 CosyVoiceClient
        cosyvoice_address = f"{config.COSYVOICE_SERVICE_HOST}:{config.COSYVOICE_SERVICE_PORT}"
        cosyvoice_client = CosyVoiceClient(address=cosyvoice_address)
        self.model_in = ModelIn(cosyvoice_client=cosyvoice_client)

    @worker_decorator(
        input_queue_attr='modelin_queue',
        next_queue_attr='tts_token_queue',
        worker_name='模型输入 Worker',
        mode='stream'
    )
    async def run(self, sentences_batch: List[Any], task_state: TaskState):
        if not sentences_batch:
            return
        self.logger.debug(f"[模型输入 Worker] 收到 {len(sentences_batch)} 句子, TaskID={task_state.task_id}")

        async for updated_batch in self.model_in.modelin_maker(
            sentences_batch,
            reuse_speaker=False,
            batch_size=self.config.MODELIN_BATCH_SIZE
        ):
            yield updated_batch

if __name__ == '__main__':
    print("ModelIn Worker 模块加载成功")

================
File: workers/segment_worker/audio_separator.py
================
from abc import ABC, abstractmethod
from typing import Tuple
import numpy as np

from models.ClearerVoice.clearvoice import ClearVoice

class AudioSeparator(ABC):
    """音频分离器接口"""
    @abstractmethod
    def separate_audio(self, input_path: str, **kwargs) -> Tuple[np.ndarray, np.ndarray]:
        pass

class ClearVoiceSeparator(AudioSeparator):
    """使用 ClearVoice 实现的音频分离器"""
    def __init__(self, model_name: str = 'MossFormer2_SE_48K'):
        self.model_name = model_name
        self.clearvoice = ClearVoice(
            task='speech_enhancement',
            model_names=[model_name]
        )
    
    def separate_audio(self, input_path: str) -> Tuple[np.ndarray, np.ndarray, int]:
        enhanced_audio, background_audio = self.clearvoice(
            input_path=input_path,
            online_write=False,
            extract_noise=True
        )
        
        if self.model_name.endswith('16K'):
            sr = 16000
        elif self.model_name.endswith('48K'):
            sr = 48000
        else:
            sr = 48000
        
        return enhanced_audio, background_audio, sr

================
File: workers/segment_worker/video_segmenter.py
================
import logging
from typing import List, Tuple
from utils.ffmpeg_utils import FFmpegTool

logger = logging.getLogger(__name__)

class VideoSegmenter:
    """视频分段计算器，负责计算视频应该如何分段"""
    
    def __init__(self, config, ffmpeg_tool: FFmpegTool):
        self.config = config
        self.ffmpeg_tool = ffmpeg_tool
        self.logger = logger
        
    async def get_video_duration(self, video_path: str) -> float:
        """获取视频时长"""
        return await self.ffmpeg_tool.get_duration(video_path)
        
    async def get_audio_segments(self, duration: float) -> List[Tuple[float, float]]:
        """
        按配置分割时间片：
        segment_length: 每段的理想长度
        min_length: 最小允许的分段时长，若最后一个分段不足该时长则与前一段合并
        """
        segment_length = self.config.SEGMENT_MINUTES * 60
        min_length = self.config.MIN_SEGMENT_MINUTES * 60

        if duration <= min_length:
            return [(0, duration)]

        segments = []
        current_pos = 0.0

        while current_pos < duration:
            remaining_duration = duration - current_pos
            
            if remaining_duration <= segment_length:
                if remaining_duration < min_length and segments:
                    # 合并到上一段
                    start = segments[-1][0]
                    new_duration = duration - start
                    segments[-1] = (start, new_duration)
                else:
                    segments.append((current_pos, remaining_duration))
                break

            segments.append((current_pos, segment_length))
            current_pos += segment_length

        return segments

================
File: workers/segment_worker/worker.py
================
import logging
import asyncio
import numpy as np
import torch
import torchaudio
import soundfile as sf
from pathlib import Path
from typing import Dict, Any, Optional
from utils.ffmpeg_utils import FFmpegTool
from utils.task_state import TaskState
from utils.decorators import worker_decorator
from utils import concurrency
from .audio_separator import ClearVoiceSeparator
from .video_segmenter import VideoSegmenter

logger = logging.getLogger(__name__)

class SegmentWorker:
    """
    视频分段处理 Worker：负责分段初始化以及媒体提取（音频、视频和后续音频分离）。
    """

    def __init__(self, config):
        """初始化 SegmentWorker 及其依赖"""
        self.config = config
        self.target_sr = 24000  # 从config获取
        
        # 初始化依赖
        self.audio_separator = ClearVoiceSeparator(model_name='MossFormer2_SE_48K')
        self.ffmpeg_tool = FFmpegTool()
        self.video_segmenter = VideoSegmenter(config=config, ffmpeg_tool=self.ffmpeg_tool)
        self.logger = logger

    @worker_decorator(
        input_queue_attr='segment_init_queue',
        next_queue_attr='segment_queue',
        worker_name='分段初始化 Worker',
        mode='stream'
    )
    async def run_init(self, item, task_state: TaskState):
        """处理视频分段初始化任务"""
        try:
            video_path = item['video_path']
            duration = await self.video_segmenter.get_video_duration(video_path)
            if duration <= 0:
                raise ValueError(f"无效的视频时长: {duration}s")
            segments = await self.video_segmenter.get_audio_segments(duration)
            if not segments:
                raise ValueError("无法获取有效分段")
            self.logger.info(
                f"[分段初始化 Worker] 视频总长={duration:.2f}s, 分段数={len(segments)}, TaskID={task_state.task_id}"
            )
            for i, (start, seg_duration) in enumerate(segments):
                yield {
                    'index': i,
                    'start': start,
                    'duration': seg_duration,
                    'video_path': video_path  # 传递视频路径给下游
                }
        except Exception as e:
            self.logger.error(f"[分段初始化 Worker] 处理失败: {e} -> TaskID={task_state.task_id}", exc_info=True)
            task_state.errors.append({
                'stage': 'segment_initialization',
                'error': str(e)
            })

    @worker_decorator(
        input_queue_attr='segment_queue',
        next_queue_attr='asr_queue',
        worker_name='分段提取 Worker'
    )
    async def run_extract(self, item, task_state: TaskState) -> Dict[str, Any]:
        """
        处理单个视频分段，执行：
          1. 并发提取音频与视频；
          2. 分离人声和背景音；
          3. 重采样与写文件；
          4. 清理临时文件，并返回提取信息。
        """
        try:
            if item is None:
                return None

            index = item['index']
            start = item['start']
            duration = item['duration']
            video_path = item['video_path']  # 从队列消息中获取视频路径

            self.logger.debug(
                f"[分段提取 Worker] 开始处理分段 {index}, start={start:.2f}s, duration={duration:.2f}s -> TaskID={task_state.task_id}"
            )
            silent_video = str(task_state.task_paths.processing_dir / f"video_silent_{index}.mp4")
            full_audio = str(task_state.task_paths.processing_dir / f"audio_full_{index}.wav")
            vocals_audio = str(task_state.task_paths.processing_dir / f"vocals_{index}.wav")
            background_audio = str(task_state.task_paths.processing_dir / f"background_{index}.wav")

            # 并发提取音频与视频
            await asyncio.gather(
                self.ffmpeg_tool.extract_audio(video_path, full_audio, start, duration),
                self.ffmpeg_tool.extract_video(video_path, silent_video, start, duration)
            )

            # 分离人声与背景（同步调用用 asyncio.to_thread 包装）
            vocals, background, sr = await asyncio.to_thread(self.audio_separator.separate_audio, full_audio)
            background = await asyncio.to_thread(self._resample_audio_sync, sr, background, self.target_sr)

            await asyncio.gather(
                asyncio.to_thread(sf.write, vocals_audio, vocals, sr, subtype='FLOAT'),
                asyncio.to_thread(sf.write, background_audio, background, self.target_sr, subtype='FLOAT')
            )

            Path(full_audio).unlink(missing_ok=True)

            media_files = {
                'video': silent_video,
                'vocals': vocals_audio,
                'background': background_audio,
                'duration': len(vocals) / sr
            }
            task_state.segment_media_files[index] = media_files

            return {
                'segment_index': index,
                'vocals_path': vocals_audio,
                'start': start,
                'duration': media_files['duration']
            }
        except Exception as e:
            self.logger.error(
                f"[分段提取 Worker] 分段处理失败: {e} -> TaskID={task_state.task_id}",
                exc_info=True
            )
            task_state.errors.append({
                'stage': 'segment_extraction',
                'error': str(e)
            })
            return None

    def _resample_audio_sync(self, fs: int, audio: np.ndarray, target_sr: int) -> np.ndarray:
        """
        同步方式进行音频归一化和重采样
          - audio: 待处理音频数据
          - fs: 源采样率
          - target_sr: 目标采样率
        """
        audio = audio.astype(np.float32)
        max_val = np.abs(audio).max()
        if max_val > 0:
            audio = audio / max_val

        if len(audio.shape) > 1:
            audio = audio.mean(axis=-1)

        if fs != target_sr:
            audio = np.ascontiguousarray(audio)
            resampler = torchaudio.transforms.Resample(
                orig_freq=fs,
                new_freq=target_sr,
                dtype=torch.float32
            )
            audio = resampler(torch.from_numpy(audio)[None, :])[0].numpy()

        return audio

if __name__ == '__main__':
    print("Segment Worker 模块加载成功")

================
File: workers/translation_worker/translation/__init__.py
================
# 空文件，用于标识 translation 目录为一个 Python 包

================
File: workers/translation_worker/translation/deepseek_client.py
================
# =========================== deepseek_client.py ===========================
import json
import logging
from openai import OpenAI
from typing import Dict
from json_repair import loads

# [MODIFIED] 引入统一线程管理
from utils import concurrency

logger = logging.getLogger(__name__)

class DeepSeekClient:
    def __init__(self, api_key: str):
        """初始化 DeepSeek 客户端"""
        if not api_key:
            raise ValueError("DeepSeek API key must be provided")
            
        self.client = OpenAI(
            api_key=api_key,
            base_url="https://api.deepseek.com"
        )
        logger.info("DeepSeek 客户端初始化成功")

    async def translate(
        self,
        system_prompt: str,
        user_prompt: str
    ) -> Dict[str, str]:
        """
        直接调用 DeepSeek API，要求返回 JSON 格式的内容。
        """
        try:
            response = await concurrency.run_sync(
                self.client.chat.completions.create,
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=1.3
            )
            result = response.choices[0].message.content
            logger.info(f"DeepSeek 原文请求内容:\n{user_prompt}")
            logger.info(f"DeepSeek 原始返回内容 (长度: {len(result)}):\n{result!r}")
            
            if not result or not result.strip():
                logger.error("DeepSeek 返回了空响应")
                raise ValueError("Empty response from DeepSeek")
                
            # 尝试修复和解析 JSON
            try:
                parsed_result = loads(result)
                logger.debug("DeepSeek 请求成功，JSON 解析完成")
                return parsed_result
            except Exception as json_error:
                logger.error(f"JSON 解析失败，原始内容: {result!r}")
                logger.error(f"JSON 解析错误详情: {str(json_error)}")
                raise
            
        except Exception as e:
            logger.error(f"DeepSeek 请求失败: {str(e)}")
            if "503" in str(e):
                logger.error("连接错误：无法连接到 DeepSeek API，可能是代理或网络问题")
            raise

================
File: workers/translation_worker/translation/gemini_client.py
================
import logging
from typing import Dict

import google.generativeai as genai
from google.generativeai.types import GenerationConfig

from json_repair import loads

# 引入统一线程管理，与 deepseek_client 用法一致
from utils import concurrency

logger = logging.getLogger(__name__)

class GeminiClient:
    def __init__(self, api_key: str):
        """初始化 Gemini 客户端"""
        if not api_key:
            raise ValueError("Gemini API key must be provided")
        # 配置 Gemini
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('gemini-1.5-flash')  # 或 'gemini-2.0-flash-exp'
        logger.info("Gemini 客户端初始化成功")
    
    async def translate(
        self,
        system_prompt: str,
        user_prompt: str
    ) -> Dict[str, str]:
        """
        直接调用 Gemini API，要求返回 JSON 格式的内容。
        """
        try:
            response = await concurrency.run_sync(
                self.model.generate_content,
                [system_prompt, user_prompt],
                generation_config=GenerationConfig(temperature=0.3)
            )
            logger.info(f"Gemini 原文请求内容:\n{user_prompt}")
            result_text = response.text
            logger.info(f"Gemini 原始返回内容 (长度: {len(result_text)}):\n{result_text!r}")
            
            if not result_text or not result_text.strip():
                logger.error("Gemini 返回了空响应")
                raise ValueError("Empty response from Gemini")
                
            # 尝试修复和解析 JSON
            try:
                parsed_result = loads(result_text)
                logger.debug("Gemini 请求成功，JSON 解析完成")
                return parsed_result
            except Exception as json_error:
                logger.error(f"JSON 解析失败，原始内容: {result_text!r}")
                logger.error(f"JSON 解析错误详情: {str(json_error)}")
                raise
            
        except Exception as e:
            logger.error(f"Gemini 请求失败: {str(e)}")
            raise

================
File: workers/translation_worker/translation/glm4_client.py
================
import json
import logging
from zhipuai import ZhipuAI
from .prompt import GLM4_TRANSLATION_PROMPT, GLM4_SYSTEM_PROMPT

logger = logging.getLogger(__name__)

class GLM4Client:
    def __init__(self, api_key: str):
        """初始化 GLM-4 客户端"""
        if not api_key:
            raise ValueError("API key must be provided")
        self.client = ZhipuAI(api_key=api_key)
        logger.info("GLM-4 客户端初始化成功")

    async def translate(self, texts: dict) -> str:
        """调用 GLM-4 模型进行翻译，返回 JSON 字符串"""
        prompt = GLM4_TRANSLATION_PROMPT.format(json_content=json.dumps(texts, ensure_ascii=False, indent=2))
        try:
            logger.debug(f"需要翻译的JSON: {json.dumps(texts, ensure_ascii=False, indent=2)}")
            response = self.client.chat.completions.create(
                model="glm-4-flash",
                messages=[
                    {"role": "system", "content": GLM4_SYSTEM_PROMPT},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                top_p=0.8
            )
            content = response.choices[0].message.content
            logger.debug(f"翻译结果: {content}")
            return content
        except Exception as e:
            logger.error(f"GLM-4 翻译请求失败: {str(e)}")
            raise

================
File: workers/translation_worker/translation/prompt.py
================
# 支持的语言映射
LANGUAGE_MAP = {
    "zh": "中文",
    "en": "英文",
    "ja": "日文",
    "ko": "韩文"
}

TRANSLATION_USER_PROMPT = """
**角色设定：**
- **经历：** 游学四方、博览群书、翻译官、外交官
- **性格：**严谨、好奇、坦率、睿智、求真、惜墨如金
- **技能：**精通{target_language}、博古通今、斟字酌句、精确传达
- **表达方式：** 精炼、简洁、最优化、避免冗余
**执行规则：**
1.无论何时，只翻译JSON格式中的value，*严格保持原 JSON 结构与各层级字段key数量完全一致*
2.value中出现的数字，翻译成*{target_language}数字*，而非阿拉伯数字。
3.仔细阅读原文，结合上下文，深思熟虑，并将你的思考过程和分析放入 "thinking" 字段中。
4.确保译文语气、风格、表达的意思与原文保持一致。
5.将翻译后的JSON文本放入 "output" 字段中。

最终请返回一个JSON对象，结构如下：
{{
    "thinking": "...",
    "output": {{ ... 翻译后的JSON ... }}
}}

原文JSON如下：
{json_content}
"""

TRANSLATION_SYSTEM_PROMPT = """你将扮演久经历练的翻译官，致力于将提供的JSON格式原文翻译成地道的{target_language}。"""

SIMPLIFICATION_USER_PROMPT = """
**角色设定：**
- **性格：**严谨克制、精确表达、追求简约
- **技能：**咬文嚼字、斟字酌句、去芜存菁
- **表达方式：** 精炼、清晰、最优化、避免冗余
**执行规则：**
1.在"thinking" 字段中提醒自己：
-原始文本是什么语言，你的任务不是翻译，而是精简，*切勿改变语言*。
-原始文本中出现的数字，保留当前语言的数字形式，而非阿拉伯数字。
-无论何时，只精简JSON格式中的value，*保持key不变，不要进行合并*。
2.首先对value内容进行深度分析，进行3种不同层次的精简：
- "slight": 轻微精简，去除冗余和重复，忠实保持原意。
- "moderate": 中度精简，进一步去除冗余，用精简的表达取代复杂的表达，保持原意不变。
- "extreme": 极度精简，保持句子的基本结构和意思不变。

最终请返回一个JSON对象，结构如下：
{{
    "thinking": "...",
    "slight": {{ ... 轻微精简后的JSON ... }},
    "moderate": {{ ... 中度精简后的JSON ... }},
    "extreme": {{ ... 极度精简后的JSON ... }}
}}

原文JSON如下：
{json_content}
"""

SIMPLIFICATION_SYSTEM_PROMPT = """你将扮演克制严谨的语言专家，致力于将提供的JSON格式文本进行精简，不要对文本进行翻译。"""

================
File: workers/translation_worker/translation/translator.py
================
import asyncio
import logging
from typing import Dict, List, AsyncGenerator, Protocol, Optional, TypeVar
from dataclasses import dataclass
from .prompt import (
    TRANSLATION_SYSTEM_PROMPT,
    TRANSLATION_USER_PROMPT,
    SIMPLIFICATION_SYSTEM_PROMPT,
    SIMPLIFICATION_USER_PROMPT,
    LANGUAGE_MAP
)

logger = logging.getLogger(__name__)

class TranslationClient(Protocol):
    async def translate(
        self,
        texts: Dict[str, str],
        system_prompt: str,
        user_prompt: str
    ) -> Dict[str, str]:
        ...

@dataclass
class BatchConfig:
    """批处理配置"""
    initial_size: int = 100
    min_size: int = 1
    required_successes: int = 2
    retry_delay: float = 0.1

T = TypeVar('T')

class Translator:
    def __init__(self, translation_client: TranslationClient):
        self.translation_client = translation_client
        self.logger = logging.getLogger(__name__)

    async def translate(self, texts: Dict[str, str], target_language: str = "zh") -> Dict[str, str]:
        """执行翻译并返回包含 "thinking" 与 "output" 字段的 JSON"""
        try:
            system_prompt = TRANSLATION_SYSTEM_PROMPT.format(
                target_language=LANGUAGE_MAP.get(target_language, target_language)
            )
            user_prompt = TRANSLATION_USER_PROMPT.format(
                target_language=LANGUAGE_MAP.get(target_language, target_language),
                json_content=texts
            )
            return await self.translation_client.translate(
                system_prompt=system_prompt,
                user_prompt=user_prompt
            )
        except Exception as e:
            self.logger.error(f"翻译失败: {str(e)}")
            raise

    async def simplify(self, texts: Dict[str, str]) -> Dict[str, str]:
        """执行简化并返回包含 "thinking"、"slight"、"moderate"、"extreme" 字段的 JSON"""
        try:
            system_prompt = SIMPLIFICATION_SYSTEM_PROMPT
            user_prompt = SIMPLIFICATION_USER_PROMPT.format(json_content=texts)
            return await self.translation_client.translate(
                system_prompt=system_prompt,
                user_prompt=user_prompt
            )
        except Exception as e:
            self.logger.error(f"简化失败: {str(e)}")
            raise

    async def _process_batch(
        self,
        items: List[T],
        process_func: callable,
        config: BatchConfig,
        error_handler: Optional[callable] = None,
        reduce_batch_on_error: bool = True
    ) -> AsyncGenerator[List[T], None]:
        if not items:
            return

        i = 0
        batch_size = config.initial_size
        success_count = 0

        while i < len(items):
            try:
                batch = items[i:i+batch_size]
                if not batch:
                    break

                results = await process_func(batch)
                if results:
                    success_count += 1
                    yield results
                    i += len(batch)
                    if reduce_batch_on_error and batch_size < config.initial_size and success_count >= config.required_successes:
                        self.logger.debug(f"连续成功{success_count}次，恢复到初始批次大小: {config.initial_size}")
                        batch_size = config.initial_size
                        success_count = 0

                    if i < len(items):
                        await asyncio.sleep(config.retry_delay)

            except Exception as e:
                self.logger.error(f"批处理失败: {str(e)}")
                if reduce_batch_on_error and batch_size > config.min_size:
                    batch_size = max(batch_size // 2, config.min_size)
                    success_count = 0
                    self.logger.debug(f"出错后减小批次大小到: {batch_size}")
                    continue
                else:
                    if error_handler:
                        yield error_handler(batch)
                    i += len(batch)

    async def translate_sentences(
        self,
        sentences: List,
        batch_size: int = 100,
        target_language: str = "zh"
    ) -> AsyncGenerator[List, None]:
        """
        批量翻译处理，将每个句子的原始文本翻译后赋值给 sentence.trans_text。
        """
        if not sentences:
            self.logger.warning("收到空的句子列表")
            return

        config = BatchConfig(initial_size=batch_size)

        async def process_batch(batch: List) -> Optional[List]:
            texts = {str(j): s.raw_text for j, s in enumerate(batch)}
            self.logger.debug(f"翻译批次: {len(texts)}条文本")
            translated = await self.translate(texts, target_language)
            if "output" not in translated:
                self.logger.error("翻译结果中缺少 output 字段")
                return None
            translated_texts = translated["output"]
            if len(translated_texts) == len(texts):
                for j, sentence in enumerate(batch):
                    sentence.trans_text = translated_texts[str(j)]
                return batch
            return None

        def handle_error(batch: List) -> List:
            for sentence in batch:
                sentence.trans_text = sentence.raw_text
            return batch

        async for batch_result in self._process_batch(
            sentences,
            process_batch,
            config,
            error_handler=handle_error,
            reduce_batch_on_error=True
        ):
            yield batch_result

    async def simplify_sentences(
        self,
        sentences: List,
        batch_size: int = 4,
        target_speed: float = 1.1  # 目标语速设定为 max_speed，此处默认值 1.1
    ) -> AsyncGenerator[List, None]:
        """
        批量精简处理，对于语速过快的句子（由 DurationAligner 筛选），
        根据原文本与各精简版本的长度比较，计算理想文本长度后选择最佳候选版本。

        理想文本长度计算公式：
            ideal_length = len(old_text) * (target_speed / s.speed)
        当 target_speed = max_speed 时，可确保精简后的文本达到预期的语速要求。
        """
        if not sentences:
            self.logger.warning("收到空的句子列表")
            return

        config = BatchConfig(initial_size=batch_size, min_size=1, required_successes=2)

        async def process_batch(batch: List) -> Optional[List]:
            texts = {str(i): s.trans_text for i, s in enumerate(batch)}
            self.logger.debug(f"简化批次: {len(texts)}条文本")
            batch_result = await self.simplify(texts)
            
            if "thinking" not in batch_result or not any(key in batch_result for key in ["slight", "moderate", "extreme"]):
                self.logger.error("简化结果格式不正确，缺少必要字段")
                return None
                
            for i, s in enumerate(batch):
                old_text = s.trans_text
                str_i = str(i)
                
                # 确保每个句子的简化结果都存在
                if not any(str_i in batch_result.get(key, {}) for key in ["slight", "moderate", "extreme"]):
                    self.logger.error(f"句子 {i} 的简化结果不完整")
                    continue

                # 根据原文本长度和当前语速计算理想文本长度
                ideal_length = len(old_text) * (target_speed / s.speed) if s.speed > 0 else len(old_text)
                
                acceptable_candidates = {}
                non_acceptable_candidates = {}
                
                for key in ["slight", "moderate", "extreme"]:
                    if key in batch_result and str_i in batch_result[key]:
                        candidate_text = batch_result[key][str_i]
                        if candidate_text:
                            candidate_length = len(candidate_text)
                            if candidate_length <= ideal_length:
                                acceptable_candidates[key] = candidate_text
                            else:
                                non_acceptable_candidates[key] = candidate_text
                
                if acceptable_candidates:
                    # 在满足候选长度不超过理想长度的版本中，选择文本最长的版本
                    chosen_key, chosen_text = max(acceptable_candidates.items(), key=lambda item: len(item[1]))
                elif non_acceptable_candidates:
                    # 若所有候选均超过理想长度，则选择与理想长度差值最小的版本
                    chosen_key, chosen_text = min(non_acceptable_candidates.items(), key=lambda item: abs(len(item[1]) - ideal_length))
                else:
                    chosen_text = old_text

                s.trans_text = chosen_text
                self.logger.info(
                    f"精简: {old_text} -> {chosen_text} (理想长度: {ideal_length}, s.speed: {s.speed})"
                )
            return batch

        def handle_error(batch: List) -> List:
            # 出错时原样返回
            return batch

        async for batch_result in self._process_batch(
            sentences,
            process_batch,
            config,
            error_handler=handle_error,
            reduce_batch_on_error=False
        ):
            yield batch_result

================
File: workers/translation_worker/worker.py
================
import logging
from typing import List, Any
from utils.decorators import worker_decorator
from utils.task_state import TaskState
from .translation.translator import Translator
from .translation.deepseek_client import DeepSeekClient
from .translation.gemini_client import GeminiClient

logger = logging.getLogger(__name__)

class TranslationWorker:
    """
    翻译 Worker：调用 Translator 对句子进行批量翻译。
    """

    def __init__(self, config):
        """初始化 TranslationWorker"""
        self.config = config
        self.logger = logger
        
        # 根据配置选择翻译客户端
        translation_model = config.TRANSLATION_MODEL.lower()
        if translation_model == "deepseek":
            client = DeepSeekClient(api_key=config.DEEPSEEK_API_KEY)
        elif translation_model == "gemini":
            client = GeminiClient(api_key=config.GEMINI_API_KEY)
        else:
            raise ValueError(f"不支持的翻译模型: {translation_model}")
        
        # 直接初始化 Translator
        self.translator = Translator(translation_client=client)

    @worker_decorator(
        input_queue_attr='translation_queue',
        next_queue_attr='modelin_queue',
        worker_name='翻译 Worker',
        mode='stream'
    )
    async def run(self, sentences_list: List[Any], task_state: TaskState):
        if not sentences_list:
            return
        self.logger.debug(f"[翻译 Worker] 收到 {len(sentences_list)} 句子, TaskID={task_state.task_id}")

        async for translated_batch in self.translator.translate_sentences(
            sentences_list,
            batch_size=self.config.TRANSLATION_BATCH_SIZE,
            target_language=task_state.target_language
        ):
            yield translated_batch

if __name__ == '__main__':
    print("Translation Worker 模块加载成功")

================
File: workers/tts_worker/tts_token_gener.py
================
# core/tts_token_gener.py

import logging
import asyncio
from typing import List
from services.cosyvoice.client import CosyVoiceClient
from utils import concurrency

class TTSTokenGenerator:
    def __init__(self, cosyvoice_client: CosyVoiceClient):
        self.cosyvoice_client = cosyvoice_client
        self.logger = logging.getLogger(__name__)

    async def tts_token_maker(self, sentences: List):
        """
        并发生成TTS tokens，并存储预估时长
        """
        if not sentences:
            return []

        tasks = []
        for s in sentences:
            text_uuid = s.model_input.get('text_uuid')
            speaker_uuid = s.model_input.get('speaker_uuid')
            if not text_uuid or not speaker_uuid:
                self.logger.warning("缺少text_uuid或speaker_uuid，无法生成TTS tokens")
                continue

            tasks.append(asyncio.create_task(self._generate_tts_single_async(s, text_uuid, speaker_uuid)))

        processed = await asyncio.gather(*tasks)
        return processed

    async def _generate_tts_single_async(self, sentence, text_uuid: str, speaker_uuid: str):
        return await concurrency.run_sync(
            self._generate_tts_single, sentence, text_uuid, speaker_uuid
        )

    def _generate_tts_single(self, sentence, text_uuid: str, speaker_uuid: str):
        duration_ms, success = self.cosyvoice_client.generate_tts_tokens(text_uuid, speaker_uuid)
        if not success:
            self.logger.error(f"生成TTS tokens失败 (text_uuid={text_uuid}, speaker_uuid={speaker_uuid})")
            return sentence

        sentence.duration = duration_ms
        self.logger.debug(f"[TTS Token] (text_uuid={text_uuid}, speaker_uuid={speaker_uuid}) 生成完毕 => 估计时长 {duration_ms}ms")
        return sentence

================
File: workers/tts_worker/worker.py
================
import logging
from typing import List, Any
from utils.decorators import worker_decorator
from utils.task_state import TaskState
from .tts_token_gener import TTSTokenGenerator
from services.cosyvoice.client import CosyVoiceClient

logger = logging.getLogger(__name__)

class TTSTokenWorker:
    """
    TTS Token 生成 Worker：调用 TTSTokenGenerator 为句子生成 TTS token。
    """

    def __init__(self, config):
        """初始化 TTSTokenWorker"""
        self.config = config
        self.logger = logger
        
        # 初始化 CosyVoiceClient
        cosyvoice_address = f"{config.COSYVOICE_SERVICE_HOST}:{config.COSYVOICE_SERVICE_PORT}"
        cosyvoice_client = CosyVoiceClient(address=cosyvoice_address)
        self.tts_token_generator = TTSTokenGenerator(cosyvoice_client=cosyvoice_client)

    @worker_decorator(
        input_queue_attr='tts_token_queue',
        next_queue_attr='duration_align_queue',
        worker_name='TTS Token生成 Worker'
    )
    async def run(self, sentences_batch: List[Any], task_state: TaskState):
        if not sentences_batch:
            return
        self.logger.debug(f"[TTS Token生成 Worker] 收到 {len(sentences_batch)} 句子, TaskID={task_state.task_id}")

        await self.tts_token_generator.tts_token_maker(sentences_batch)
        return sentences_batch

if __name__ == '__main__':
    print("TTS Token Worker 模块加载成功")

================
File: __init__.py
================
# 空文件即可，标识这是一个 Python 包

================
File: .cursorignore
================
# 忽略模型文件夹，因为包含大量模型文件和第三方代码
models/

================
File: .env.example
================
# ================================
# .env.example
# ================================

# 【可选】修改 FLASK_ENV，默认使用 development 方便调试
FLASK_ENV=development

# 翻译模型选择 (可选值: deepseek, glm4, gemini)
TRANSLATION_MODEL=deepseek

# API Keys (请替换为实际的密钥)
ZHIPUAI_API_KEY=your_zhipuai_key_here
GEMINI_API_KEY=your_gemini_key_here
DEEPSEEK_API_KEY=your_deepseek_key_here

================
File: api.py
================
# ------------------------------
# backend/api.py  (完整可复制版本)
# ------------------------------
import sys
from pathlib import Path
import logging
import uuid
import asyncio
from typing import Dict

import uvicorn
from fastapi import FastAPI, File, UploadFile, HTTPException, Request, Form
from fastapi.responses import JSONResponse, FileResponse
from fastapi.templating import Jinja2Templates
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
import aiofiles

from config import Config
config = Config()
config.init_directories()

sys.path.extend(config.SYSTEM_PATHS)

logging.basicConfig(
    level=logging.INFO,
    format="%(levelname)s | %(asctime)s | %(name)s | L%(lineno)d | %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)
logger = logging.getLogger(__name__)

from video_translator import ViTranslator
from utils.task_storage import TaskPaths
from fastapi import BackgroundTasks

app = FastAPI(debug=True)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

current_dir = Path(__file__).parent
templates = Jinja2Templates(directory=str(current_dir / "templates"))

# 初始化服务
vi_translator = ViTranslator(config=config)

task_results: Dict[str, dict] = {}

@app.get("/")
async def index(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

@app.post("/upload")
async def upload_video(
    video: UploadFile = File(...),
    target_language: str = Form("zh"),
    generate_subtitle: bool = Form(False),
):
    try:
        if not video:
            raise HTTPException(status_code=400, detail="没有文件上传")
        
        if not video.content_type.startswith('video/'):
            raise HTTPException(status_code=400, detail="只支持视频文件")
            
        if target_language not in ["zh", "en", "ja", "ko"]:
            raise HTTPException(status_code=400, detail=f"不支持的目标语言: {target_language}")
        
        task_id = str(uuid.uuid4())
        task_paths = TaskPaths(config, task_id)
        task_paths.create_directories()
        
        video_path = task_paths.input_dir / f"original_{video.filename}"
        try:
            async with aiofiles.open(video_path, "wb") as f:
                content = await video.read()
                await f.write(content)
        except Exception as e:
            logger.error(f"保存文件失败: {str(e)}")
            raise HTTPException(status_code=500, detail="文件保存失败")

        # 创建新的事件循环来处理视频转换
        loop = asyncio.get_event_loop()
        task = loop.create_task(vi_translator.trans_video(
            video_path=str(video_path),
            task_id=task_id,
            task_paths=task_paths,
            target_language=target_language,
            generate_subtitle=generate_subtitle,
        ))
        
        task_results[task_id] = {
            "status": "processing",
            "message": "视频处理中",
            "progress": 0
        }
        
        async def on_task_complete(t):
            try:
                result = await t
                if result.get('status') == 'success':
                    task_results[task_id].update({
                        "status": "success",
                        "message": "处理完成",
                        "progress": 100
                    })
                else:
                    task_results[task_id].update({
                        "status": "error",
                        "message": result.get('message', '处理失败'),
                        "progress": 0
                    })
            except Exception as e:
                logger.error(f"任务处理失败: {str(e)}")
                task_results[task_id].update({
                    "status": "error",
                    "message": str(e),
                    "progress": 0
                })
        
        task.add_done_callback(lambda t: asyncio.create_task(on_task_complete(t)))
        
        return {
            'status': 'processing',
            'task_id': task_id,
            'message': '视频上传成功，开始处理'
        }
    except HTTPException as e:
        raise e
    except Exception as e:
        logger.error(f"上传处理失败: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/task/{task_id}")
async def get_task_status(task_id: str):
    result = task_results.get(task_id)
    if not result:
        return {
            "status": "error",
            "message": "任务不存在",
            "progress": 0
        }
    return result

# 挂载静态文件服务
app.mount("/playlists", StaticFiles(directory=str(config.PUBLIC_DIR / "playlists")), name="playlists")
app.mount("/segments", StaticFiles(directory=str(config.PUBLIC_DIR / "segments")), name="segments")

@app.get("/download/{task_id}")
async def download_translated_video(task_id: str):
    final_video_path = config.TASKS_DIR / task_id / "output" / f"final_{task_id}.mp4"
    if not final_video_path.exists():
        raise HTTPException(status_code=404, detail="最终视频文件尚未生成或已被删除")
    return FileResponse(
        str(final_video_path),
        media_type='video/mp4',
        filename=f"final_{task_id}.mp4",
    )

if __name__ == "__main__":
    uvicorn.run(
        app,
        host=config.SERVER_HOST,
        port=config.SERVER_PORT,
        log_level="info"
    )

================
File: config.py
================
import os
from pathlib import Path
from dotenv import load_dotenv
import torch

current_dir = Path(__file__).parent
env_path = current_dir / '.env'
load_dotenv(env_path)

project_dir = current_dir.parent
storage_dir = project_dir / 'storage'

class Config:
    SERVER_HOST = "0.0.0.0"
    SERVER_PORT = 8000
    LOG_LEVEL = "DEBUG"

    BASE_DIR = storage_dir
    TASKS_DIR = BASE_DIR / "tasks"
    PUBLIC_DIR = BASE_DIR / "public"

    BATCH_SIZE = 6
    TARGET_SPEAKER_AUDIO_DURATION = 8
    VAD_SR = 16000
    VOCALS_VOLUME = 0.7
    BACKGROUND_VOLUME = 0.3
    AUDIO_OVERLAP = 1024
    NORMALIZATION_THRESHOLD = 0.9

    SEGMENT_MINUTES = 5
    MIN_SEGMENT_MINUTES = 3

    TRANSLATION_MODEL = os.getenv("TRANSLATION_MODEL", "deepseek")
    ZHIPUAI_API_KEY = os.getenv("ZHIPUAI_API_KEY", "")
    GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "")
    DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY", "")

    SYSTEM_PATHS = [
        str(current_dir / 'models' / 'CosyVoice'),
        str(current_dir / 'models' / 'ClearVoice'),
        str(current_dir / 'models' / 'CosyVoice' / 'third_party' / 'Matcha-TTS')
    ]

    MODEL_DIR = project_dir / "models"

    # =========== 全局音频配置 ===========
    SAMPLE_RATE = 24000  # 全局采样率，从 CosyVoice 模型加载后会被更新

    # =========== ASR 模型相关配置 ===========
    ASR_MODEL_DIR = MODEL_DIR / "SenseVoice"
    ASR_MODEL_NAME = "iic/SenseVoiceSmall"
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    
    # ASR 模型的其他参数
    ASR_MODEL_KWARGS = {
        "model": "iic/SenseVoiceSmall",
        "remote_code": "./models/SenseVoice/model.py",
        "vad_model": "iic/speech_fsmn_vad_zh-cn-16k-common-pytorch",
        "vad_kwargs": {"max_single_segment_time": 30000},
        "spk_model": "cam++",
        "trust_remote_code": True,
        "disable_update": True,
        "device": "cuda" if torch.cuda.is_available() else "cpu"
    }

    @property
    def MODEL_PATH(self) -> Path:
        return Path(self.MODEL_DIR)

    @property
    def BASE_PATH(self) -> Path:
        return self.BASE_DIR

    @property
    def TASKS_PATH(self) -> Path:
        return self.TASKS_DIR

    @property
    def PUBLIC_PATH(self) -> Path:
        return self.PUBLIC_DIR

    @classmethod
    def init_directories(cls):
        directories = [
            cls.BASE_DIR,
            cls.TASKS_DIR,
            cls.PUBLIC_DIR,
            cls.PUBLIC_DIR / "playlists",
            cls.PUBLIC_DIR / "segments"
        ]
        for dir_path in directories:
            dir_path.mkdir(parents=True, exist_ok=True)
            os.chmod(str(dir_path), 0o755)

    MAX_GAP_MS = 2000
    SHORT_SENTENCE_MERGE_THRESHOLD_MS = 1000
    MAX_TOKENS_PER_SENTENCE = 80
    MIN_SENTENCE_LENGTH = 4
    SENTENCE_END_TOKENS = {9686, 9688, 9676, 9705, 9728, 9729, 20046, 24883, 24879}
    STRONG_END_TOKENS = {9688, 9676, 9705, 9729, 20046, 24883}
    WEAK_END_TOKENS = {9686, 9728, 24879}
    SPEAKER_AUDIO_TARGET_DURATION = 8.0
    TRANSLATION_BATCH_SIZE = 50
    MODELIN_BATCH_SIZE = 3
    # 控制同时处理多少个视频分段
    MAX_PARALLEL_SEGMENTS = 2

    # 存储服务配置
    STORAGE_TYPE = "local"  # 'local' 或 's3'
    
    # HLS 配置
    HLS_SEGMENT_DURATION = 10  # 分片时长(秒)
    HLS_LIST_SIZE = 6         # 播放列表保留的分片数
    HLS_SEGMENT_FORMAT = "ts"  # 分片格式
    HLS_TIME = 10             # 每个分片的目标时长
    HLS_FLAGS = "independent_segments"  # HLS 标志

    # HLS gRPC服务配置
    HLS_GRPC_HOST = "0.0.0.0"
    HLS_GRPC_PORT = 50051
    
    # HLS服务地址(供客户端使用)
    HLS_SERVICE_HOST = os.getenv("HLS_SERVICE_HOST", "localhost")
    HLS_SERVICE_PORT = int(os.getenv("HLS_SERVICE_PORT", "50051"))

    # CosyVoice 服务配置
    COSYVOICE_SERVICE_HOST = os.getenv("COSYVOICE_SERVICE_HOST", "localhost")
    COSYVOICE_SERVICE_PORT = int(os.getenv("COSYVOICE_SERVICE_PORT", "50052"))

================
File: pipeline_scheduler.py
================
# -----------------------------------------
# backend/pipeline_scheduler.py (节选完整示例)
# -----------------------------------------
import asyncio
import logging
from utils.task_state import TaskState

logger = logging.getLogger(__name__)

class PipelineScheduler:
    """
    流水线调度器：负责协调各个 Worker 阶段，通过各个队列传递数据，
    启动所有 worker 长循环任务，然后在停止时发送终止信号。
    """

    def __init__(
        self,
        segment_worker,    # workers.segment_worker.Worker 实例
        asr_worker,        # workers.asr_worker.Worker 实例
        translation_worker,# workers.translation_worker.Worker 实例
        modelin_worker,    # workers.modelin_worker.Worker 实例
        tts_token_worker,  # workers.tts_worker.Worker 实例
        duration_worker,   # workers.duration_worker.Worker 实例
        audio_gen_worker,  # workers.audio_gen_worker.Worker 实例
        mixer_worker,      # workers.mixer_worker.Worker 实例
        config
    ):
        self.logger = logging.getLogger(__name__)
        self.segment_worker = segment_worker
        self.asr_worker = asr_worker
        self.translation_worker = translation_worker
        self.modelin_worker = modelin_worker
        self.tts_token_worker = tts_token_worker
        self.duration_worker = duration_worker
        self.audio_gen_worker = audio_gen_worker
        self.mixer_worker = mixer_worker
        self.config = config
        self._workers = []

    async def start_workers(self, task_state: TaskState):
        self.logger.info(f"[PipelineScheduler] 启动所有 Worker -> TaskID={task_state.task_id}")
        self._workers = [
            asyncio.create_task(self.segment_worker.run_init(task_state)),
            asyncio.create_task(self.segment_worker.run_extract(task_state)),
            asyncio.create_task(self.asr_worker.run(task_state)),
            asyncio.create_task(self.translation_worker.run(task_state)),
            asyncio.create_task(self.modelin_worker.run(task_state)),
            asyncio.create_task(self.tts_token_worker.run(task_state)),
            asyncio.create_task(self.duration_worker.run(task_state)),
            asyncio.create_task(self.audio_gen_worker.run(task_state)),
            asyncio.create_task(self.mixer_worker.run(task_state))
        ]

    async def stop_workers(self, task_state: TaskState):
        self.logger.info(f"[PipelineScheduler] 停止所有 Worker -> TaskID={task_state.task_id}")
        # 发送终止信号（在最上游队列中发送 None）
        await task_state.segment_init_queue.put(None)
        await asyncio.gather(*self._workers, return_exceptions=True)
        self.logger.info(f"[PipelineScheduler] 所有 Worker 已结束 -> TaskID={task_state.task_id}")

================
File: postcss.config.json
================
{ "plugins": { "postcss-preset-env": {}, "autoprefixer": {} } }

================
File: run_cosyvoice_service.py
================
import logging
import sys
import os
from pathlib import Path
import argparse

current_dir = Path(__file__).parent

def setup_system_paths():
    system_paths = [
        str(current_dir / 'models' / 'CosyVoice'),
        str(current_dir / 'models' / 'CosyVoice' / 'third_party' / 'Matcha-TTS')
    ]
    for path in system_paths:
        if path not in sys.path and os.path.exists(path):
            sys.path.append(path)
            logging.info(f"Added {path} to system path")

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    parser = argparse.ArgumentParser()
    parser.add_argument('--model_dir', type=str, default='models/CosyVoice/pretrained_models/CosyVoice2-0.5B', help='base directory containing model folders')
    parser.add_argument('--host', type=str, default='0.0.0.0', help='service host')
    parser.add_argument('--port', type=int, default=50052, help='service port')
    args = parser.parse_args()
    
    # 设置系统路径
    setup_system_paths()

    # 必须在导入任何使用顶层 "cosyvoice" 模块之前注册别名
    import models.CosyVoice.cosyvoice
    sys.modules["cosyvoice"] = models.CosyVoice.cosyvoice

    # 现在再导入服务模块，确保其内部使用 "cosyvoice" 时能正确找到
    from services.cosyvoice.service import serve
    serve(args)

================
File: run_hls_service.py
================
import asyncio
import logging
from config import Config
from services.hls import serve

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

async def main():
    config = Config()
    server = serve(config)
    
    try:
        logger.info(f"启动HLS gRPC服务 - {config.HLS_GRPC_HOST}:{config.HLS_GRPC_PORT}")
        await server.start()
        await server.wait_for_termination()
    except KeyboardInterrupt:
        logger.info("正在关闭服务器...")
        await server.stop(0)
        
if __name__ == "__main__":
    asyncio.run(main())

================
File: video_translator.py
================
import logging
from typing import Dict, Any
from pathlib import Path

from pipeline_scheduler import PipelineScheduler
from utils.task_storage import TaskPaths
from config import Config
from utils.task_state import TaskState
from utils.ffmpeg_utils import FFmpegTool
from services.hls import HLSClient

# 从各 worker 文件夹导入 Worker 类
from workers.asr_worker.worker import ASRWorker
from workers.segment_worker.worker import SegmentWorker
from workers.audio_gen_worker.worker import AudioGenWorker
from workers.mixer_worker.worker import MixerWorker
from workers.translation_worker.worker import TranslationWorker
from workers.modelin_worker.worker import ModelInWorker
from workers.tts_worker.worker import TTSTokenWorker
from workers.duration_worker.worker import DurationWorker

logger = logging.getLogger(__name__)

class ViTranslator:
    """
    视频翻译器：初始化所有 Worker，通过 PipelineScheduler 协调整个处理流程。
    """

    def __init__(self, config: Config):
        self.config = config
        self.hls_client = None  # 将在 trans_video 中异步初始化
        self.logger = logger
        self._init_workers()

    def _init_workers(self):
        """初始化所有 Worker"""
        self.logger.info("开始初始化 Worker...")
        
        self.segment_worker = SegmentWorker(config=self.config)
        self.asr_worker = ASRWorker(config=self.config)
        self.translation_worker = TranslationWorker(config=self.config)
        self.modelin_worker = ModelInWorker(config=self.config)
        self.tts_token_worker = TTSTokenWorker(config=self.config)
        self.duration_worker = DurationWorker(config=self.config)
        self.audio_gen_worker = AudioGenWorker(config=self.config)
        self.mixer_worker = None  # 将在 trans_video 中初始化，依赖 HLSClient
        
        self.logger.info("Worker 初始化完成")

    async def trans_video(
        self,
        video_path: str,
        task_id: str,
        task_paths: TaskPaths,
        target_language: str = "zh",
        generate_subtitle: bool = False,
    ) -> Dict[str, Any]:
        """
        入口：对整段视频进行处理。包括分段、ASR、翻译、TTS、混音、生成 HLS 等。
        """
        self.logger.info(
            f"[trans_video] 开始处理视频: {video_path}, task_id={task_id}, "
            f"target_language={target_language}, generate_subtitle={generate_subtitle}"
        )

        try:
            # 异步初始化 HLS 客户端
            self.hls_client = await HLSClient.create(self.config)
            # 初始化依赖 HLS 客户端的 mixer_worker
            self.mixer_worker = MixerWorker(config=self.config, hls_service=self.hls_client)

            # 初始化任务状态
            task_state = TaskState(
                task_id=task_id,
                task_paths=task_paths,
                target_language=target_language,
                generate_subtitle=generate_subtitle
            )

            # 初始化流水线
            pipeline = PipelineScheduler(
                segment_worker=self.segment_worker,
                asr_worker=self.asr_worker,
                translation_worker=self.translation_worker,
                modelin_worker=self.modelin_worker,
                tts_token_worker=self.tts_token_worker,
                duration_worker=self.duration_worker,
                audio_gen_worker=self.audio_gen_worker,
                mixer_worker=self.mixer_worker,
                config=self.config
            )

            # 初始化HLS
            if not await self.hls_client.init_task(task_id):
                raise RuntimeError("HLS任务初始化失败")
            
            # 1. 启动所有worker
            await pipeline.start_workers(task_state)

            # 2. 触发视频分段初始化
            await task_state.segment_init_queue.put({
                'video_path': video_path,
                'task_id': task_id
            })

            # 3. 等待所有worker完成
            await pipeline.stop_workers(task_state)

            # 4. 检查是否有错误
            if task_state.errors:
                # 如果有错误，清理HLS资源
                await self.hls_client.cleanup_task(task_id)
                error_msg = f"处理过程中发生 {len(task_state.errors)} 个错误"
                self.logger.error(f"{error_msg} -> TaskID={task_id}")
                return {"status": "error", "message": error_msg, "errors": task_state.errors}

            # 5. 完成HLS流
            if not await self.hls_client.finalize_task(task_id):
                raise RuntimeError("HLS任务完成失败")
            self.logger.info(f"[trans_video] HLS生成完成 -> TaskID={task_id}")

            # 6. 合并所有segment MP4s
            final_video_path = await self._concat_segment_mp4s(task_state)
            if final_video_path is not None and final_video_path.exists():
                self.logger.info(f"翻译后的完整视频已生成: {final_video_path}")

                # 清理资源
                import torch
                torch.cuda.empty_cache()
                self.logger.info("已释放未使用的GPU显存")

                return {
                    "status": "success",
                    "message": "视频翻译完成",
                    "final_video_path": str(final_video_path)
                }
            else:
                return {
                    "status": "error",
                    "message": "无法合并生成最终MP4文件"
                }

        except Exception as e:
            self.logger.exception(f"[trans_video] 处理失败: {e} -> TaskID={task_id}")
            # 确保清理HLS资源
            if self.hls_client:
                await self.hls_client.cleanup_task(task_id)
            return {"status": "error", "message": str(e)}

        finally:
            # 确保清理资源
            if 'pipeline' in locals():
                await pipeline.stop_workers(task_state)
            if self.hls_client:
                await self.hls_client.close()

    async def _concat_segment_mp4s(self, task_state: TaskState) -> Path:
        """
        把 pipeline_scheduler _mixing_worker 产出的所有 segment_xxx.mp4
        用 ffmpeg concat 合并成 final_{task_state.task_id}.mp4
        如果成功再删除这些小片段。
        """
        if not task_state.merged_segments:
            self.logger.warning("无可合并的 segment MP4，可能任务中断或没有生成混音段.")
            return None

        final_path = task_state.task_paths.output_dir / f"final_{task_state.task_id}.mp4"
        final_path.parent.mkdir(parents=True, exist_ok=True)

        list_txt = final_path.parent / f"concat_{task_state.task_id}.txt"
        with open(list_txt, 'w', encoding='utf-8') as f:
            for seg_mp4 in task_state.merged_segments:
                abs_path = Path(seg_mp4).resolve()
                f.write(f"file '{abs_path}'\n")

        cmd = [
            "ffmpeg", "-y",
            "-f", "concat",
            "-safe", "0",
            "-i", str(list_txt),
            "-c", "copy",
            str(final_path)
        ]
        try:
            self.logger.info(f"开始合并 {len(task_state.merged_segments)} 个 MP4 -> {final_path}")
            # 使用 FFmpegTool 实例
            ffmpeg_tool = FFmpegTool()
            await ffmpeg_tool.run_command(cmd)
            self.logger.info(f"合并完成: {final_path}")

            for seg_mp4 in task_state.merged_segments:
                try:
                    Path(seg_mp4).unlink(missing_ok=True)
                    self.logger.debug(f"已删除分段文件: {seg_mp4}")
                except Exception as ex:
                    self.logger.warning(f"删除分段文件 {seg_mp4} 失败: {ex}")

            return final_path
        except Exception as e:
            self.logger.error(f"ffmpeg concat 失败: {e}")
            return None
        finally:
            if list_txt.exists():
                list_txt.unlink()

if __name__ == '__main__':
    print("ViTranslator 模块加载成功")



================================================================
End of Codebase
================================================================
