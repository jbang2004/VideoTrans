This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: models/
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
core/
  timeadjust/
    duration_aligner.py
    timestamp_adjuster.py
  translation/
    __init__.py
    deepseek_client.py
    gemini_client.py
    glm4_client.py
    prompt.py
    translator.py
  __init__.py
  asr_model_actor.py
  audio_gener.py
  audio_separator.py
  auto_sense.py
  cosyvoice_model_actor.py
  hls_manager.py
  media_mixer.py
  model_in.py
  sentence_tools.py
  tts_token_gener.py
utils/
  concurrency.py
  decorators.py
  ffmpeg_utils.py
  gpu_manager.py
  media_utils.py
  sentence_logger.py
  task_state.py
  task_storage.py
  temp_file_manager.py
__init__.py
.cursorignore
.env.example
api.py
config.py
pipeline_scheduler.py
postcss.config.json
video_translator.py

================================================================
Files
================================================================

================
File: core/timeadjust/duration_aligner.py
================
import logging

class DurationAligner:
    def __init__(self, model_in=None, simplifier=None, tts_token_gener=None, max_speed=1.1):
        """
        model_in：生成模型接口，用于更新文本特征  
        simplifier：简化处理接口（Translator）  
        tts_token_gener：TTS token 生成接口  
        max_speed：语速阈值，超过该速率的句子需要进行简化
        """
        self.model_in = model_in
        self.simplifier = simplifier
        self.tts_token_gener = tts_token_gener
        self.max_speed = max_speed
        self.logger = logging.getLogger(__name__)

    async def align_durations(self, sentences):
        """
        对整批句子进行时长对齐，并检查是否需要精简（语速过快）。
        """
        if not sentences:
            return

        # 第一次对齐
        self._align_batch(sentences)

        # 查找语速过快的句子（speed > max_speed）
        retry_sentences = [s for s in sentences if s.speed > self.max_speed]
        if retry_sentences:
            self.logger.info(f"{len(retry_sentences)} 个句子语速过快, 正在精简...")
            success = await self._retry_sentences_batch(retry_sentences)
            if success:
                # 若精简文本成功，再次对齐
                self._align_batch(sentences)
            else:
                self.logger.warning("精简过程失败, 保持原结果")

    def _align_batch(self, sentences):
        """
        同批次句子进行时长对齐。
        """
        if not sentences:
            return

        # 计算每句需要调整的时间差
        for s in sentences:
            s.diff = s.duration - s.target_duration

        total_diff_to_adjust = sum(s.diff for s in sentences)
        positive_diff_sum = sum(x.diff for x in sentences if x.diff > 0)
        negative_diff_sum_abs = sum(abs(x.diff) for x in sentences if x.diff < 0)
        current_time = sentences[0].start

        for s in sentences:
            s.adjusted_start = current_time
            diff = s.diff
            s.speed = 1.0
            s.silence_duration = 0.0
            s.adjusted_duration = s.duration

            if total_diff_to_adjust != 0:
                if total_diff_to_adjust > 0 and diff > 0:
                    if positive_diff_sum > 0:
                        proportion = diff / positive_diff_sum
                        adjustment = total_diff_to_adjust * proportion
                        s.adjusted_duration = s.duration - adjustment
                        s.speed = s.duration / max(s.adjusted_duration, 0.001)
                elif total_diff_to_adjust < 0 and diff < 0:
                    if negative_diff_sum_abs > 0:
                        proportion = abs(diff) / negative_diff_sum_abs
                        total_needed = abs(total_diff_to_adjust) * proportion
                        max_slowdown = s.duration * 0.07
                        slowdown = min(total_needed, max_slowdown)
                        s.adjusted_duration = s.duration + slowdown
                        s.speed = s.duration / max(s.adjusted_duration, 0.001)
                        s.silence_duration = total_needed - slowdown
                        if s.silence_duration > 0:
                            s.adjusted_duration += s.silence_duration

            s.diff = s.duration - s.adjusted_duration
            current_time += s.adjusted_duration

            self.logger.info(
                f"对齐后: {s.trans_text}, duration: {s.duration}, "
                f"target_duration: {s.target_duration}, diff: {s.diff}, "
                f"speed: {s.speed}, silence_duration: {s.silence_duration}"
            )

    async def _retry_sentences_batch(self, sentences):
        """
        对语速过快的句子执行精简 + 更新 TTS token。
        """
        try:
            # 1. 分批对语速过快的句子进行精简
            async for _ in self.simplifier.simplify_sentences(sentences, target_speed=self.max_speed):
                pass
            # 2. 批量更新文本特征（复用 speaker 与 uuid）
            async for batch in self.model_in.modelin_maker(
                sentences,
                reuse_speaker=True,
                reuse_uuid=True,
                batch_size=3
            ):
                # 3. 再生成 token（复用 uuid）
                updated_batch = await self.tts_token_gener.tts_token_maker(batch, reuse_uuid=True)
            return True
        except Exception as e:
            self.logger.error(f"_retry_sentences_batch 出错: {e}")
            return False

================
File: core/timeadjust/timestamp_adjuster.py
================
import logging
from typing import List, Optional
from dataclasses import dataclass

class TimestampAdjuster:
    """句子时间戳调整器"""
    
    def __init__(self, sample_rate: int):
        self.logger = logging.getLogger(__name__)
        self.sample_rate = sample_rate
        
    def update_timestamps(self, sentences: List, start_time: float = None) -> float:
        """更新句子的时间戳信息
        
        Args:
            sentences: 要更新的句子列表
            start_time: 起始时间（毫秒），如果为 None 则使用第一个句子的开始时间
            
        Returns:
            float: 最后一个句子结束的时间点（毫秒）
        """
        if not sentences:
            return start_time if start_time is not None else 0
            
        # 使用传入的起始时间或第一个句子的开始时间
        current_time = start_time if start_time is not None else sentences[0].start
        
        for i, sentence in enumerate(sentences):
            # 计算实际音频长度（毫秒）
            if sentence.generated_audio is not None:
                actual_duration = (len(sentence.generated_audio) / self.sample_rate) * 1000
            else:
                actual_duration = 0
                self.logger.warning(f"句子 {sentence.sentence_id} 没有生成音频")
            
            # 更新时间戳
            sentence.adjusted_start = current_time
            sentence.adjusted_duration = actual_duration
            
            # 更新差异值
            sentence.diff = sentence.duration - actual_duration
            
            # 更新下一个句子的开始时间
            current_time += actual_duration
            
        return current_time
        
    def validate_timestamps(self, sentences: List) -> bool:
        """验证时间戳的连续性和有效性
        
        Args:
            sentences: 要验证的句子列表
            
        Returns:
            bool: 时间戳是否有效
        """
        if not sentences:
            return True
            
        for i in range(len(sentences) - 1):
            current = sentences[i]
            next_sentence = sentences[i + 1]
            
            # 验证时间连续性
            expected_next_start = current.adjusted_start + current.adjusted_duration
            if abs(next_sentence.adjusted_start - expected_next_start) > 1:  # 允许1毫秒的误差
                self.logger.error(
                    f"时间戳不连续 - 句子 {current.sentence_id} 结束时间: {expected_next_start:.2f}ms, "
                    f"句子 {next_sentence.sentence_id} 开始时间: {next_sentence.adjusted_start:.2f}ms"
                )
                return False
                
            # 验证时长有效性
            if current.adjusted_duration <= 0:
                self.logger.error(f"句子 {current.sentence_id} 的时长无效: {current.adjusted_duration:.2f}ms")
                return False
                
        return True

================
File: core/translation/__init__.py
================
# 空文件，用于标识 translation 目录为一个 Python 包

================
File: core/translation/deepseek_client.py
================
# =========================== deepseek_client.py ===========================
import json
import logging
from openai import OpenAI
from typing import Dict
from json_repair import loads

# [MODIFIED] 引入统一线程管理
from utils import concurrency

logger = logging.getLogger(__name__)

class DeepSeekClient:
    def __init__(self, api_key: str):
        """初始化 DeepSeek 客户端"""
        if not api_key:
            raise ValueError("DeepSeek API key must be provided")
            
        self.client = OpenAI(
            api_key=api_key,
            base_url="https://api.deepseek.com"
        )
        logger.info("DeepSeek 客户端初始化成功")

    async def translate(
        self,
        system_prompt: str,
        user_prompt: str
    ) -> Dict[str, str]:
        """
        直接调用 DeepSeek API，要求返回 JSON 格式的内容。
        """
        try:
            response = await concurrency.run_sync(
                self.client.chat.completions.create,
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=1.3
            )
            result = response.choices[0].message.content
            logger.info(f"DeepSeek 原文请求内容:\n{user_prompt}")
            logger.info(f"DeepSeek 原始返回内容 (长度: {len(result)}):\n{result!r}")
            
            if not result or not result.strip():
                logger.error("DeepSeek 返回了空响应")
                raise ValueError("Empty response from DeepSeek")
                
            # 尝试修复和解析 JSON
            try:
                parsed_result = loads(result)
                logger.debug("DeepSeek 请求成功，JSON 解析完成")
                return parsed_result
            except Exception as json_error:
                logger.error(f"JSON 解析失败，原始内容: {result!r}")
                logger.error(f"JSON 解析错误详情: {str(json_error)}")
                raise
            
        except Exception as e:
            logger.error(f"DeepSeek 请求失败: {str(e)}")
            if "503" in str(e):
                logger.error("连接错误：无法连接到 DeepSeek API，可能是代理或网络问题")
            raise

================
File: core/translation/gemini_client.py
================
import logging
from typing import Dict

import google.generativeai as genai
from google.generativeai.types import GenerationConfig

from json_repair import loads

# 引入统一线程管理，与 deepseek_client 用法一致
from utils import concurrency

logger = logging.getLogger(__name__)

class GeminiClient:
    def __init__(self, api_key: str):
        """初始化 Gemini 客户端"""
        if not api_key:
            raise ValueError("Gemini API key must be provided")
        # 配置 Gemini
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('gemini-1.5-flash')  # 或 'gemini-2.0-flash-exp'
        logger.info("Gemini 客户端初始化成功")
    
    async def translate(
        self,
        system_prompt: str,
        user_prompt: str
    ) -> Dict[str, str]:
        """
        直接调用 Gemini API，要求返回 JSON 格式的内容。
        """
        try:
            response = await concurrency.run_sync(
                self.model.generate_content,
                [system_prompt, user_prompt],
                generation_config=GenerationConfig(temperature=0.3)
            )
            logger.info(f"Gemini 原文请求内容:\n{user_prompt}")
            result_text = response.text
            logger.info(f"Gemini 原始返回内容 (长度: {len(result_text)}):\n{result_text!r}")
            
            if not result_text or not result_text.strip():
                logger.error("Gemini 返回了空响应")
                raise ValueError("Empty response from Gemini")
                
            # 尝试修复和解析 JSON
            try:
                parsed_result = loads(result_text)
                logger.debug("Gemini 请求成功，JSON 解析完成")
                return parsed_result
            except Exception as json_error:
                logger.error(f"JSON 解析失败，原始内容: {result_text!r}")
                logger.error(f"JSON 解析错误详情: {str(json_error)}")
                raise
            
        except Exception as e:
            logger.error(f"Gemini 请求失败: {str(e)}")
            raise

================
File: core/translation/glm4_client.py
================
import json
import logging
from zhipuai import ZhipuAI
from .prompt import GLM4_TRANSLATION_PROMPT, GLM4_SYSTEM_PROMPT

logger = logging.getLogger(__name__)

class GLM4Client:
    def __init__(self, api_key: str):
        """初始化 GLM-4 客户端"""
        if not api_key:
            raise ValueError("API key must be provided")
        self.client = ZhipuAI(api_key=api_key)
        logger.info("GLM-4 客户端初始化成功")

    async def translate(self, texts: dict) -> str:
        """调用 GLM-4 模型进行翻译，返回 JSON 字符串"""
        prompt = GLM4_TRANSLATION_PROMPT.format(json_content=json.dumps(texts, ensure_ascii=False, indent=2))
        try:
            logger.debug(f"需要翻译的JSON: {json.dumps(texts, ensure_ascii=False, indent=2)}")
            response = self.client.chat.completions.create(
                model="glm-4-flash",
                messages=[
                    {"role": "system", "content": GLM4_SYSTEM_PROMPT},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                top_p=0.8
            )
            content = response.choices[0].message.content
            logger.debug(f"翻译结果: {content}")
            return content
        except Exception as e:
            logger.error(f"GLM-4 翻译请求失败: {str(e)}")
            raise

================
File: core/translation/prompt.py
================
# 支持的语言映射
LANGUAGE_MAP = {
    "zh": "中文",
    "en": "英文",
    "ja": "日文",
    "ko": "韩文"
}

TRANSLATION_USER_PROMPT = """
**角色设定：**
- **经历：** 游学四方、博览群书、翻译官、外交官
- **性格：**严谨、好奇、坦率、睿智、求真、惜墨如金
- **技能：**精通{target_language}、博古通今、斟字酌句、精确传达
- **表达方式：** 精炼、简洁、最优化、避免冗余
**执行规则：**
1.无论何时，只翻译JSON格式中的value，*严格保持原 JSON 结构与各层级字段key数量完全一致*
2.value中出现的数字，翻译成*{target_language}数字*，而非阿拉伯数字。
3.仔细阅读原文，结合上下文，深思熟虑，并将你的思考过程和分析放入 "thinking" 字段中。
4.确保译文语气、风格、表达的意思与原文保持一致。
5.将翻译后的JSON文本放入 "output" 字段中。

最终请返回一个JSON对象，结构如下：
{{
    "thinking": "...",
    "output": {{ ... 翻译后的JSON ... }}
}}

原文JSON如下：
{json_content}
"""

TRANSLATION_SYSTEM_PROMPT = """你将扮演久经历练的翻译官，致力于将提供的JSON格式原文翻译成地道的{target_language}。"""

SIMPLIFICATION_USER_PROMPT = """
**角色设定：**
- **性格：**严谨克制、精确表达、追求简约
- **技能：**咬文嚼字、斟字酌句、去芜存菁
- **表达方式：** 精炼、清晰、最优化、避免冗余
**执行规则：**
1.在"thinking" 字段中提醒自己：
-原始文本是什么语言，你的任务不是翻译，而是精简，*切勿改变语言*。
-原始文本中出现的数字，保留当前语言的数字形式，而非阿拉伯数字。
-无论何时，只精简JSON格式中的value，*保持key不变，不要进行合并*。
2.首先对value内容进行深度分析，进行3种不同层次的精简：
- "slight": 轻微精简，去除冗余和重复，忠实保持原意。
- "moderate": 中度精简，进一步去除冗余，用精简的表达取代复杂的表达，保持原意不变。
- "extreme": 极度精简，保持句子的基本结构和意思不变。

最终请返回一个JSON对象，结构如下：
{{
    "thinking": "...",
    "slight": {{ ... 轻微精简后的JSON ... }},
    "moderate": {{ ... 中度精简后的JSON ... }},
    "extreme": {{ ... 极度精简后的JSON ... }}
}}

原文JSON如下：
{json_content}
"""

SIMPLIFICATION_SYSTEM_PROMPT = """你将扮演克制严谨的语言专家，致力于将提供的JSON格式文本进行精简，不要对文本进行翻译。"""

================
File: core/translation/translator.py
================
import asyncio
import logging
from typing import Dict, List, AsyncGenerator, Protocol, Optional, TypeVar
from dataclasses import dataclass
from .prompt import (
    TRANSLATION_SYSTEM_PROMPT,
    TRANSLATION_USER_PROMPT,
    SIMPLIFICATION_SYSTEM_PROMPT,
    SIMPLIFICATION_USER_PROMPT,
    LANGUAGE_MAP
)

logger = logging.getLogger(__name__)

class TranslationClient(Protocol):
    async def translate(
        self,
        texts: Dict[str, str],
        system_prompt: str,
        user_prompt: str
    ) -> Dict[str, str]:
        ...

@dataclass
class BatchConfig:
    """批处理配置"""
    initial_size: int = 100
    min_size: int = 1
    required_successes: int = 2
    retry_delay: float = 0.1

T = TypeVar('T')

class Translator:
    def __init__(self, translation_client: TranslationClient):
        self.translation_client = translation_client
        self.logger = logging.getLogger(__name__)

    async def translate(self, texts: Dict[str, str], target_language: str = "zh") -> Dict[str, str]:
        """执行翻译并返回包含 "thinking" 与 "output" 字段的 JSON"""
        try:
            system_prompt = TRANSLATION_SYSTEM_PROMPT.format(
                target_language=LANGUAGE_MAP.get(target_language, target_language)
            )
            user_prompt = TRANSLATION_USER_PROMPT.format(
                target_language=LANGUAGE_MAP.get(target_language, target_language),
                json_content=texts
            )
            return await self.translation_client.translate(
                system_prompt=system_prompt,
                user_prompt=user_prompt
            )
        except Exception as e:
            self.logger.error(f"翻译失败: {str(e)}")
            raise

    async def simplify(self, texts: Dict[str, str]) -> Dict[str, str]:
        """执行简化并返回包含 "thinking"、"slight"、"moderate"、"extreme" 字段的 JSON"""
        try:
            system_prompt = SIMPLIFICATION_SYSTEM_PROMPT
            user_prompt = SIMPLIFICATION_USER_PROMPT.format(json_content=texts)
            return await self.translation_client.translate(
                system_prompt=system_prompt,
                user_prompt=user_prompt
            )
        except Exception as e:
            self.logger.error(f"简化失败: {str(e)}")
            raise

    async def _process_batch(
        self,
        items: List[T],
        process_func: callable,
        config: BatchConfig,
        error_handler: Optional[callable] = None,
        reduce_batch_on_error: bool = True
    ) -> AsyncGenerator[List[T], None]:
        if not items:
            return

        i = 0
        batch_size = config.initial_size
        success_count = 0

        while i < len(items):
            try:
                batch = items[i:i+batch_size]
                if not batch:
                    break

                results = await process_func(batch)
                if results:
                    success_count += 1
                    yield results
                    i += len(batch)
                    if reduce_batch_on_error and batch_size < config.initial_size and success_count >= config.required_successes:
                        self.logger.debug(f"连续成功{success_count}次，恢复到初始批次大小: {config.initial_size}")
                        batch_size = config.initial_size
                        success_count = 0

                    if i < len(items):
                        await asyncio.sleep(config.retry_delay)

            except Exception as e:
                self.logger.error(f"批处理失败: {str(e)}")
                if reduce_batch_on_error and batch_size > config.min_size:
                    batch_size = max(batch_size // 2, config.min_size)
                    success_count = 0
                    self.logger.debug(f"出错后减小批次大小到: {batch_size}")
                    continue
                else:
                    if error_handler:
                        yield error_handler(batch)
                    i += len(batch)

    async def translate_sentences(
        self,
        sentences: List,
        batch_size: int = 100,
        target_language: str = "zh"
    ) -> AsyncGenerator[List, None]:
        """
        批量翻译处理，将每个句子的原始文本翻译后赋值给 sentence.trans_text。
        """
        if not sentences:
            self.logger.warning("收到空的句子列表")
            return

        config = BatchConfig(initial_size=batch_size)

        async def process_batch(batch: List) -> Optional[List]:
            texts = {str(j): s.raw_text for j, s in enumerate(batch)}
            self.logger.debug(f"翻译批次: {len(texts)}条文本")
            translated = await self.translate(texts, target_language)
            if "output" not in translated:
                self.logger.error("翻译结果中缺少 output 字段")
                return None
            translated_texts = translated["output"]
            if len(translated_texts) == len(texts):
                for j, sentence in enumerate(batch):
                    sentence.trans_text = translated_texts[str(j)]
                return batch
            return None

        def handle_error(batch: List) -> List:
            for sentence in batch:
                sentence.trans_text = sentence.raw_text
            return batch

        async for batch_result in self._process_batch(
            sentences,
            process_batch,
            config,
            error_handler=handle_error,
            reduce_batch_on_error=True
        ):
            yield batch_result

    async def simplify_sentences(
        self,
        sentences: List,
        batch_size: int = 4,
        target_speed: float = 1.1  # 目标语速设定为 max_speed，此处默认值 1.1
    ) -> AsyncGenerator[List, None]:
        """
        批量精简处理，对于语速过快的句子（由 DurationAligner 筛选），
        根据原文本与各精简版本的长度比较，计算理想文本长度后选择最佳候选版本。

        理想文本长度计算公式：
            ideal_length = len(old_text) * (target_speed / s.speed)
        当 target_speed = max_speed 时，可确保精简后的文本达到预期的语速要求。
        """
        if not sentences:
            self.logger.warning("收到空的句子列表")
            return

        config = BatchConfig(initial_size=batch_size, min_size=1, required_successes=2)

        async def process_batch(batch: List) -> Optional[List]:
            texts = {str(i): s.trans_text for i, s in enumerate(batch)}
            self.logger.debug(f"简化批次: {len(texts)}条文本")
            batch_result = await self.simplify(texts)
            
            if "thinking" not in batch_result or not any(key in batch_result for key in ["slight", "moderate", "extreme"]):
                self.logger.error("简化结果格式不正确，缺少必要字段")
                return None
                
            for i, s in enumerate(batch):
                old_text = s.trans_text
                str_i = str(i)
                
                # 确保每个句子的简化结果都存在
                if not any(str_i in batch_result.get(key, {}) for key in ["slight", "moderate", "extreme"]):
                    self.logger.error(f"句子 {i} 的简化结果不完整")
                    continue

                # 根据原文本长度和当前语速计算理想文本长度
                ideal_length = len(old_text) * (target_speed / s.speed) if s.speed > 0 else len(old_text)
                
                acceptable_candidates = {}
                non_acceptable_candidates = {}
                
                for key in ["slight", "moderate", "extreme"]:
                    if key in batch_result and str_i in batch_result[key]:
                        candidate_text = batch_result[key][str_i]
                        if candidate_text:
                            candidate_length = len(candidate_text)
                            if candidate_length <= ideal_length:
                                acceptable_candidates[key] = candidate_text
                            else:
                                non_acceptable_candidates[key] = candidate_text
                
                if acceptable_candidates:
                    # 在满足候选长度不超过理想长度的版本中，选择文本最长的版本
                    chosen_key, chosen_text = max(acceptable_candidates.items(), key=lambda item: len(item[1]))
                elif non_acceptable_candidates:
                    # 若所有候选均超过理想长度，则选择与理想长度差值最小的版本
                    chosen_key, chosen_text = min(non_acceptable_candidates.items(), key=lambda item: abs(len(item[1]) - ideal_length))
                else:
                    chosen_text = old_text

                s.trans_text = chosen_text
                self.logger.info(
                    f"精简: {old_text} -> {chosen_text} (理想长度: {ideal_length}, s.speed: {s.speed})"
                )
            return batch

        def handle_error(batch: List) -> List:
            # 出错时原样返回
            return batch

        async for batch_result in self._process_batch(
            sentences,
            process_batch,
            config,
            error_handler=handle_error,
            reduce_batch_on_error=False
        ):
            yield batch_result

================
File: core/__init__.py
================
# 空文件即可，标识这是一个 Python 包

================
File: core/asr_model_actor.py
================
import ray
import sys
import logging

@ray.remote(num_gpus=0.3)  # 直接指定GPU资源
class SenseAutoModelActor:
    """ASR模型Actor封装 - 为原始SenseAutoModel提供Ray远程接口"""
    
    def __init__(self):
        """初始化ASR模型Actor"""
        self.logger = logging.getLogger(__name__)
        self.logger.info("初始化ASR模型Actor")
        
        # 导入Config并设置系统路径
        from config import Config
        self.config = Config()
        
        # 添加系统路径
        for path in self.config.SYSTEM_PATHS:
            if path not in sys.path:
                sys.path.append(path)
                self.logger.info(f"添加系统路径: {path}")
        
        try:
            # 导入并初始化ASR模型
            from core.auto_sense import SenseAutoModel
            
            # 直接使用硬编码参数，与原始代码保持一致
            self.model = SenseAutoModel(
                config=self.config,
                model="iic/SenseVoiceSmall",
                remote_code="./models/SenseVoice/model.py",
                vad_model="iic/speech_fsmn_vad_zh-cn-16k-common-pytorch",
                vad_kwargs={"max_single_segment_time": 30000},
                spk_model="cam++",
                trust_remote_code=True,
                disable_update=True,
                device="cuda"
            )
            
            self.logger.info("ASR模型加载完成")
        except Exception as e:
            self.logger.error(f"ASR模型加载失败: {str(e)}")
            raise
    
    async def generate_async(self, input, **kwargs):
        """
        执行与原始SenseAutoModel.generate_async相同的方法
        这是ASR的主要接口方法
        """
        try:
            self.logger.info(f"开始ASR识别音频: {input if isinstance(input, str) else '(已加载音频)'}")
            # 直接调用model的原始方法
            result = await self.model.generate_async(input, **kwargs)
            self.logger.info(f"ASR识别完成，获得 {len(result)} 个句子")
            return result
        except Exception as e:
            self.logger.error(f"ASR识别失败: {str(e)}")
            raise

================
File: core/audio_gener.py
================
import logging
import asyncio
import torch
import numpy as np
import ray

from utils import concurrency

class AudioGenerator:
    def __init__(self, cosyvoice_model_actor, sample_rate: int = None):
        """
        Args:
            cosyvoice_model_actor: CosyVoice模型Actor引用
            sample_rate: 采样率，如果为None则使用Actor的采样率
        """
        self.cosyvoice_actor = cosyvoice_model_actor
        # 获取采样率（如果未指定）
        self.sample_rate = sample_rate or ray.get(cosyvoice_model_actor.get_sample_rate.remote())
        self.logger = logging.getLogger(__name__)

    async def vocal_audio_maker(self, batch_sentences):
        """异步批量生成音频"""
        tasks = []
        for s in batch_sentences:
            tasks.append(self._generate_single_async(s))

        try:
            await asyncio.gather(*tasks)
        except Exception as e:
            self.logger.error(f"音频生成失败: {str(e)}")
            raise

    async def _generate_single_async(self, sentence):
        """异步生成单个音频（调用Actor）"""
        try:
            audio_np = await concurrency.run_sync(
                self._generate_audio_single, sentence
            )
            sentence.generated_audio = audio_np
        except Exception as e:
            self.logger.error(f"音频生成失败 (UUID: {sentence.model_input.get('uuid', 'unknown')}): {str(e)}")
            sentence.generated_audio = None

    def _generate_audio_single(self, sentence):
        """调用Actor生成音频"""
        model_input = sentence.model_input
        self.logger.debug(f"开始生成音频 (主UUID: {model_input.get('uuid', 'unknown')})")

        try:
            segment_audio_list = []
            
            tokens_list = model_input.get('segment_speech_tokens', [])
            uuids_list = model_input.get('segment_uuids', [])

            if not tokens_list or not uuids_list:
                self.logger.debug(f"空的语音标记, 仅生成空波形 (UUID: {model_input.get('uuid', 'unknown')})")
                segment_audio_list.append(np.zeros(0, dtype=np.float32))
            else:
                for i, (tokens, segment_uuid) in enumerate(zip(tokens_list, uuids_list)):
                    if not tokens:
                        segment_audio_list.append(np.zeros(0, dtype=np.float32))
                        continue

                    # 准备参数
                    token_tensor = torch.tensor(tokens).unsqueeze(dim=0)
                    prompt_token = model_input.get('flow_prompt_speech_token', torch.zeros(1, 0, dtype=torch.int32))
                    prompt_feat = model_input.get('prompt_speech_feat', torch.zeros(1, 0, 80))
                    embedding = model_input.get('flow_embedding', torch.zeros(0))
                    speed = sentence.speed if sentence.speed else 1.0
                    
                    # 调用Actor生成音频
                    segment_audio = ray.get(self.cosyvoice_actor.generate_audio.remote(
                        token_tensor, 0, segment_uuid, prompt_token, prompt_feat, embedding, speed
                    ))
                    
                    # 如果是多通道，转单通道
                    if segment_audio.ndim > 1:
                        segment_audio = segment_audio.mean(axis=0)
                    
                    segment_audio_list.append(segment_audio)
                    self.logger.debug(
                        f"段落 {i+1}/{len(uuids_list)} 生成完成，"
                        f"时长: {len(segment_audio)/self.sample_rate:.2f}秒"
                    )

            # 拼接音频和静音（与原代码相同）
            if segment_audio_list:
                final_audio = np.concatenate(segment_audio_list)
            else:
                final_audio = np.zeros(0, dtype=np.float32)

            # 添加首句静音
            if sentence.is_first and sentence.start > 0:
                silence_samples = int(sentence.start * self.sample_rate / 1000)
                final_audio = np.concatenate([np.zeros(silence_samples, dtype=np.float32), final_audio])

            # 添加尾部静音
            if sentence.silence_duration > 0:
                silence_samples = int(sentence.silence_duration * self.sample_rate / 1000)
                final_audio = np.concatenate([final_audio, np.zeros(silence_samples, dtype=np.float32)])

            return final_audio

        except Exception as e:
            self.logger.error(f"音频生成失败 (UUID: {model_input.get('uuid', 'unknown')}): {str(e)}")
            raise

================
File: core/audio_separator.py
================
from abc import ABC, abstractmethod
from typing import Tuple
import numpy as np

from models.ClearerVoice.clearvoice import ClearVoice

class AudioSeparator(ABC):
    """音频分离器接口"""
    @abstractmethod
    def separate_audio(self, input_path: str, **kwargs) -> Tuple[np.ndarray, np.ndarray]:
        pass

class ClearVoiceSeparator(AudioSeparator):
    """使用 ClearVoice 实现的音频分离器"""
    def __init__(self, model_name: str = 'MossFormer2_SE_48K'):
        self.model_name = model_name
        self.clearvoice = ClearVoice(
            task='speech_enhancement',
            model_names=[model_name]
        )
    
    def separate_audio(self, input_path: str) -> Tuple[np.ndarray, np.ndarray, int]:
        enhanced_audio, background_audio = self.clearvoice(
            input_path=input_path,
            online_write=False,
            extract_noise=True
        )
        
        if self.model_name.endswith('16K'):
            sr = 16000
        elif self.model_name.endswith('48K'):
            sr = 48000
        else:
            sr = 48000
        
        return enhanced_audio, background_audio, sr

================
File: core/auto_sense.py
================
import logging
import os
import importlib.util
import sys
import torch
import numpy as np
from tqdm import tqdm
from pathlib import Path
import time
from funasr.register import tables
from funasr.auto.auto_model import AutoModel as BaseAutoModel
from funasr.auto.auto_model import prepare_data_iterator
from funasr.utils.misc import deep_update
from funasr.models.campplus.utils import sv_chunk, postprocess
from funasr.models.campplus.cluster_backend import ClusterBackend
from .sentence_tools import get_sentences
from funasr.utils.vad_utils import slice_padding_audio_samples
from funasr.utils.load_utils import load_audio_text_image_video

# [MODIFIED] 新增以下导入，用于在 async 函数中包装同步调用
from utils import concurrency
from functools import partial

class SenseAutoModel(BaseAutoModel):
    def __init__(self, config, **kwargs):
        super().__init__(**kwargs)
        self.logger = logging.getLogger(__name__)
        self.config = config
        
        if self.spk_model is not None:
            self.cb_model = ClusterBackend().to(kwargs["device"])
            spk_mode = kwargs.get("spk_mode", "punc_segment")
            if spk_mode not in ["default", "vad_segment", "punc_segment"]:
                self.logger.error("spk_mode 应该是 'default', 'vad_segment' 或 'punc_segment' 之一。")
            self.spk_mode = spk_mode

    def inference_with_vad(self, input, input_len=None, **cfg):
        kwargs = self.kwargs
        self.tokenizer = kwargs.get("tokenizer")
        deep_update(self.vad_kwargs, cfg)
        
        res = self.inference(input, input_len=input_len, model=self.vad_model, kwargs=self.vad_kwargs, **cfg)
        model = self.model
        deep_update(kwargs, cfg)
        kwargs["batch_size"] = max(int(kwargs.get("batch_size_s", 300)) * 1000, 1)
        batch_size_threshold_ms = int(kwargs.get("batch_size_threshold_s", 60)) * 1000

        key_list, data_list = prepare_data_iterator(input, input_len=input_len, data_type=kwargs.get("data_type", None))
        results_ret_list = []

        pbar_total = tqdm(total=len(res), dynamic_ncols=True, disable=kwargs.get("disable_pbar", False))

        for i, item in enumerate(res):
            key, vadsegments = item["key"], item["value"]
            input_i = data_list[i]
            fs = kwargs["frontend"].fs if hasattr(kwargs["frontend"], "fs") else 16000
            speech = load_audio_text_image_video(input_i, fs=fs, audio_fs=kwargs.get("fs", 16000))
            speech_lengths = len(speech)
            self.logger.debug(f"音频长度: {speech_lengths} 样本")

            if speech_lengths < 400:
                self.logger.warning(f"音频太短（{speech_lengths} 样本），可能导致处理错误")

            sorted_data = sorted([(seg, idx) for idx, seg in enumerate(vadsegments)], key=lambda x: x[0][1] - x[0][0])
            if not sorted_data:
                self.logger.info(f"解码, utt: {key}, 空语音")
                continue

            results_sorted = []
            all_segments = []
            beg_idx, end_idx = 0, 1
            max_len_in_batch = 0

            for j in range(len(sorted_data)):
                sample_length = sorted_data[j][0][1] - sorted_data[j][0][0]
                potential_batch_length = max(max_len_in_batch, sample_length) * (j + 1 - beg_idx)

                if (j < len(sorted_data) - 1 and 
                    sample_length < batch_size_threshold_ms and 
                    potential_batch_length < kwargs["batch_size"]):
                    max_len_in_batch = max(max_len_in_batch, sample_length)
                    end_idx += 1
                    continue

                speech_j, _ = slice_padding_audio_samples(speech, speech_lengths, sorted_data[beg_idx:end_idx])
                results = self.inference(speech_j, input_len=None, model=model, kwargs=kwargs, **cfg)

                if self.spk_model is not None:
                    for _b, speech_segment in enumerate(speech_j):
                        vad_segment = sorted_data[beg_idx:end_idx][_b][0]
                        segments = sv_chunk([[vad_segment[0] / 1000.0, vad_segment[1] / 1000.0, np.array(speech_segment)]])
                        all_segments.extend(segments)
                        speech_b = [seg[2] for seg in segments]
                        spk_res = self.inference(speech_b, input_len=None, model=self.spk_model, kwargs=kwargs, **cfg)
                        results[_b]["spk_embedding"] = spk_res[0]["spk_embedding"]
                beg_idx, end_idx = end_idx, end_idx + 1
                max_len_in_batch = sample_length
                results_sorted.extend(results)

            if len(results_sorted) != len(sorted_data):
                self.logger.info(f"解码，utt: {key}，空结果")
                continue

            restored_data = [0] * len(sorted_data)
            for j, (_, idx) in enumerate(sorted_data):
                restored_data[idx] = results_sorted[j]

            result = self.combine_results(restored_data, vadsegments)

            if self.spk_model is not None and kwargs.get("return_spk_res", True):
                all_segments.sort(key=lambda x: x[0])
                spk_embedding = result["spk_embedding"]
                labels = self.cb_model(spk_embedding.cpu(), oracle_num=kwargs.get("preset_spk_num", None))
                sv_output = postprocess(all_segments, None, labels, spk_embedding.cpu())

                if "timestamp" not in result:
                    self.logger.error(f"speaker diarization 依赖于时间戳对于 utt: {key}")
                    sentence_list = []
                else:
                    sentence_list = get_sentences(
                        tokens=result["token"],
                        timestamps=result["timestamp"],
                        tokenizer=self.tokenizer,
                        speech=speech,
                        sd_time_list=sv_output,
                        sample_rate=fs,
                        config=self.config
                    )
                    results_ret_list = sentence_list
            else:
                sentence_list = []
            pbar_total.update(1)

        pbar_total.close()
        return results_ret_list

    def combine_results(self, restored_data, vadsegments):
        result = {}
        for j, data in enumerate(restored_data):
            for k, v in data.items():
                if k.startswith("timestamp"):
                    if k not in result:
                        result[k] = []
                    for t in v:
                        t[0] += vadsegments[j][0]
                        t[1] += vadsegments[j][0]
                    result[k].extend(v)
                elif k == "spk_embedding":
                    if k not in result:
                        result[k] = v
                    else:
                        result[k] = torch.cat([result[k], v], dim=0)
                elif "token" in k:
                    if k not in result:
                        result[k] = v
                    else:
                        result[k].extend(v)
                else:
                    if k not in result:
                        result[k] = v
                    else:
                        result[k] += v
        return result

    # [MODIFIED] 统一使用 concurrency.run_sync 来执行 self.generate
    async def generate_async(self, input, input_len=None, **cfg):
        func = partial(self.generate, input, input_len, **cfg)
        return await concurrency.run_sync(func)

================
File: core/cosyvoice_model_actor.py
================
import ray
import torch
import logging
import threading
import sys

@ray.remote(num_gpus=0.9)
class CosyVoiceModelActor:
    """
    将CosyVoice模型封装为Ray Actor，提供所有与模型相关的操作
    """
    def __init__(self, model_path):
        """初始化CosyVoice模型服务"""
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"初始化CosyVoice模型Actor: {model_path}")
        
        # 导入Config并设置系统路径
        from config import Config
        config = Config()
        
        # 添加系统路径，与api.py中相同
        for path in config.SYSTEM_PATHS:
            if path not in sys.path:
                sys.path.append(path)
                self.logger.info(f"添加系统路径: {path}")
        
        # 导入并加载模型
        try:
            from models.CosyVoice.cosyvoice.cli.cosyvoice import CosyVoice2
            self.cosyvoice = CosyVoice2(model_path)
            self.model = self.cosyvoice.model
            self.sample_rate = self.cosyvoice.sample_rate
            self.frontend = self.cosyvoice.frontend
            
            self.logger.info("CosyVoice模型加载完成")
        except Exception as e:
            self.logger.error(f"CosyVoice模型加载失败: {str(e)}")
            self.logger.error(f"当前系统路径: {sys.path}")
            raise
        
    def get_model_info(self):
        """获取模型基本信息"""
        return {
            "sample_rate": self.sample_rate,
        }
        
    def get_sample_rate(self):
        """返回模型采样率"""
        return self.sample_rate
        
    def get_frontend(self):
        """获取模型frontend（用于ModelIn）"""
        return self.frontend
        
    def postprocess_audio(self, speech_tensor, top_db=60, hop_length=220, win_length=440, max_val=0.8):
        """处理音频"""
        import librosa
        speech, _ = librosa.effects.trim(
            speech_tensor, 
            top_db=top_db,
            frame_length=win_length,
            hop_length=hop_length
        )
        if speech.abs().max() > max_val:
            speech = speech / speech.abs().max() * max_val
        
        speech = torch.concat([speech, torch.zeros(1, int(self.sample_rate * 0.2))], dim=1)
        return speech
        
    def extract_speaker_features(self, processed_audio):
        """提取说话人特征"""
        return self.frontend.frontend_cross_lingual(
            "",
            processed_audio,
            self.sample_rate
        )
        
    def normalize_text(self, text, split=True):
        """文本正则化"""
        return self.frontend.text_normalize(text, split=split)
        
    def extract_text_tokens(self, text):
        """提取文本token"""
        return self.frontend._extract_text_token(text)
    
    # ==== TTS Token生成功能 ====
    def generate_tts_tokens(self, text, prompt_text, llm_prompt_speech_token, llm_embedding, seg_uuid):
        """生成TTS Tokens"""
        with self.model.lock:
            self.model.tts_speech_token_dict[seg_uuid] = []
            self.model.llm_end_dict[seg_uuid] = False
            if hasattr(self.model, 'mel_overlap_dict'):
                self.model.mel_overlap_dict[seg_uuid] = None
            self.model.hift_cache_dict[seg_uuid] = None
            
        # 调用模型生成token
        self.model.llm_job(
            text,
            prompt_text,
            llm_prompt_speech_token,
            llm_embedding,
            seg_uuid
        )
        
        # 返回生成的tokens
        return self.model.tts_speech_token_dict[seg_uuid]
    
    def cleanup_tts_tokens(self, seg_uuid):
        """清理TTS token缓存"""
        with self.model.lock:
            self.model.tts_speech_token_dict.pop(seg_uuid, None)
            self.model.llm_end_dict.pop(seg_uuid, None)
            self.model.hift_cache_dict.pop(seg_uuid, None)
            if hasattr(self.model, 'mel_overlap_dict'):
                self.model.mel_overlap_dict.pop(seg_uuid, None)
    
    # ==== 音频生成功能 ====
    def generate_audio(self, tokens, token_offset, uuid, prompt_token, prompt_feat, embedding, speed=1.0):
        """生成音频"""
        token2wav_kwargs = {
            'token': tokens,
            'token_offset': token_offset,
            'finalize': True,
            'prompt_token': prompt_token,
            'prompt_feat': prompt_feat,
            'embedding': embedding,
            'uuid': uuid,
            'speed': speed
        }
        
        segment_output = self.model.token2wav(**token2wav_kwargs)
        return segment_output.cpu().numpy()

================
File: core/hls_manager.py
================
# core/hls_manager.py
import logging
import asyncio
from pathlib import Path
import m3u8
from typing import List, Union
from utils.decorators import handle_errors
from utils.task_storage import TaskPaths
import os.path
import shutil

# 引入统一的 FFmpegTool
from utils.ffmpeg_utils import FFmpegTool

logger = logging.getLogger(__name__)

class HLSManager:
    """处理 HLS 流媒体相关的功能"""
    def __init__(self, config, task_id: str, task_paths: TaskPaths):
        self.config = config
        self.task_id = task_id
        self.task_paths = task_paths
        self.logger = logging.getLogger(__name__)

        self.playlist_path = Path(task_paths.playlist_path)
        self.segments_dir = Path(task_paths.segments_dir)
        self.sequence_number = 0
        self._lock = asyncio.Lock()

        self.playlist = m3u8.M3U8()
        self.playlist.version = 3
        self.playlist.target_duration = 20
        self.playlist.media_sequence = 0
        self.playlist.playlist_type = 'VOD'
        self.playlist.is_endlist = False

        # 引入统一 ffmpeg 工具
        self.ffmpeg_tool = FFmpegTool()

        self.has_segments = False

        self._save_playlist()

    def _save_playlist(self) -> None:
        """保存播放列表到文件"""
        try:
            for segment in self.playlist.segments:
                # 确保 URI 带有斜杠
                if segment.uri is not None and not segment.uri.startswith('/'):
                    segment.uri = '/' + segment.uri

            with open(self.playlist_path, 'w', encoding='utf-8') as f:
                f.write(self.playlist.dumps())
            self.logger.info("播放列表已更新")
        except Exception as e:
            self.logger.error(f"保存播放列表失败: {e}")
            raise

    @handle_errors(None)
    async def add_segment(self, video_path: Union[str, Path], part_index: int) -> None:
        """添加新的视频片段到播放列表"""
        async with self._lock:
            try:
                self.segments_dir.mkdir(parents=True, exist_ok=True)

                segment_filename = f'segment_{self.sequence_number:04d}_%03d.ts'
                segment_pattern = str(self.segments_dir / segment_filename)
                temp_playlist_path = self.task_paths.processing_dir / f'temp_{part_index}.m3u8'

                # 统一使用 ffmpeg_tool.hls_segment
                await self.ffmpeg_tool.hls_segment(
                    input_path=str(video_path),
                    segment_pattern=segment_pattern,
                    playlist_path=str(temp_playlist_path),
                    hls_time=10
                )

                # 加入分段
                temp_m3u8 = m3u8.load(str(temp_playlist_path))
                discontinuity_segment = m3u8.Segment(discontinuity=True)
                self.playlist.add_segment(discontinuity_segment)

                for segment in temp_m3u8.segments:
                    segment.uri = f"segments/{self.task_id}/{Path(segment.uri).name}"
                    self.playlist.segments.append(segment)

                self.sequence_number += len(temp_m3u8.segments)
                self.has_segments = True
                self._save_playlist()

            finally:
                if os.path.exists(str(temp_playlist_path)):
                    try:
                        os.unlink(str(temp_playlist_path))
                    except Exception as e:
                        self.logger.warning(f"清理临时文件失败: {e}")

    async def finalize_playlist(self) -> None:
        """标记播放列表为完成状态"""
        if self.has_segments:
            self.playlist.is_endlist = True
            self._save_playlist()
            self.logger.info("播放列表已保存，并标记为完成状态")
        else:
            self.logger.warning("播放列表为空，不标记为结束状态")

================
File: core/media_mixer.py
================
# ---------------------------------------------------
# backend/core/media_mixer.py (最终改进版)
# ---------------------------------------------------
import numpy as np
import logging
import soundfile as sf
import os
import asyncio
from contextlib import ExitStack
from tempfile import NamedTemporaryFile
from typing import List

import pysubs2  # 用于简化字幕处理

from utils.decorators import handle_errors
from utils.ffmpeg_utils import FFmpegTool
from config import Config
from core.sentence_tools import Sentence
from utils.task_state import TaskState

logger = logging.getLogger(__name__)

class MediaMixer:
    """
    用于将多段句子的合成音频与原视频片段混合，并可生成带字幕的视频。
    支持:
      - 音频淡入淡出
      - 背景音乐混合
      - 基于 pysubs2 生成 .ass 字幕(“YouTube风格”)
      - 按语言自动决定单行最大长度
    """
    def __init__(self, config: Config, sample_rate: int):
        self.config = config
        self.sample_rate = sample_rate

        # 音量相关
        self.max_val = 1.0
        self.overlap = self.config.AUDIO_OVERLAP
        self.vocals_volume = self.config.VOCALS_VOLUME
        self.background_volume = self.config.BACKGROUND_VOLUME

        # 全局缓存, 可按需使用
        self.full_audio_buffer = np.array([], dtype=np.float32)

        self.ffmpeg_tool = FFmpegTool()

    @handle_errors(logger)
    async def mixed_media_maker(
        self,
        sentences: List[Sentence],
        task_state: TaskState,
        output_path: str,
        generate_subtitle: bool = False
    ) -> bool:
        """
        主入口: 处理一批句子的音频与视频，输出一段带音频的 MP4。
        根据 generate_subtitle 决定是否烧制字幕。

        Args:
            sentences: 本片段内的所有句子对象
            task_state: 任务状态，内部包含 target_language 等
            output_path: 生成的 MP4 文件路径
            generate_subtitle: 是否在最终视频里烧制字幕

        Returns:
            True / False 表示成功或失败
        """
        if not sentences:
            logger.warning("mixed_media_maker: 收到空的句子列表")
            return False

        segment_index = sentences[0].segment_index
        segment_files = task_state.segment_media_files.get(segment_index)
        if not segment_files:
            logger.error(f"找不到分段 {segment_index} 对应的媒体文件信息")
            return False

        # =========== (1) 拼接所有句子的合成音频 =============
        full_audio = np.array([], dtype=np.float32)
        for sentence in sentences:
            if sentence.generated_audio is not None:
                audio_data = np.asarray(sentence.generated_audio, dtype=np.float32)
                # 如果已经有前面累积的音频，做淡入淡出衔接
                if len(full_audio) > 0:
                    audio_data = self._apply_fade_effect(audio_data)
                full_audio = np.concatenate((full_audio, audio_data))
            else:
                logger.warning(
                    "句子音频生成失败: text=%r, UUID=%s",
                    sentence.raw_text,  # 或 sentence.trans_text
                    sentence.model_input.get("uuid", "unknown")
                )

        if len(full_audio) == 0:
            logger.error("mixed_media_maker: 没有有效的合成音频数据")
            return False

        # 计算当前片段的起始时间和时长(秒)
        start_time = 0.0
        if not sentences[0].is_first:
            start_time = (sentences[0].adjusted_start - sentences[0].segment_start * 1000) / 1000.0

        duration = sum(s.adjusted_duration for s in sentences) / 1000.0

        # =========== (2) 背景音乐混合 (可选) =============
        background_audio_path = segment_files['background']
        if background_audio_path is not None:
            full_audio = self._mix_with_background(
                bg_path=background_audio_path,
                start_time=start_time,
                duration=duration,
                audio_data=full_audio
            )
            full_audio = self._normalize_audio(full_audio)

        # (可按需储存到全局 mixer 缓存)
        self.full_audio_buffer = np.concatenate((self.full_audio_buffer, full_audio))

        # =========== (3) 如果有视频，就把音频合并到视频里 ============
        video_path = segment_files['video']
        if video_path:
            await self._add_video_segment(
                video_path=video_path,
                start_time=start_time,
                duration=duration,
                audio_data=full_audio,
                output_path=output_path,
                sentences=sentences,
                generate_subtitle=generate_subtitle,
                task_state=task_state  # 传入以获取 target_language
            )
            return True

        logger.warning("mixed_media_maker: 本片段无video_path可用")
        return False

    # -------------------------------------------------------------------------
    # 辅助函数: 做音频淡入淡出
    # -------------------------------------------------------------------------
    def _apply_fade_effect(self, audio_data: np.ndarray) -> np.ndarray:
        """
        在语音片段衔接处做 overlap 长度的淡入淡出衔接。
        """
        if audio_data is None or len(audio_data) == 0:
            return np.array([], dtype=np.float32)

        cross_len = min(self.overlap, len(self.full_audio_buffer), len(audio_data))
        if cross_len <= 0:
            return audio_data

        fade_out = np.sqrt(np.linspace(1.0, 0.0, cross_len, dtype=np.float32))
        fade_in  = np.sqrt(np.linspace(0.0, 1.0, cross_len, dtype=np.float32))

        audio_data = audio_data.copy()
        overlap_region = self.full_audio_buffer[-cross_len:]

        audio_data[:cross_len] = overlap_region * fade_out + audio_data[:cross_len] * fade_in
        return audio_data

    # -------------------------------------------------------------------------
    # 辅助函数: 将合成音频与背景音乐混合
    # -------------------------------------------------------------------------
    def _mix_with_background(
        self,
        bg_path: str,
        start_time: float,
        duration: float,
        audio_data: np.ndarray
    ) -> np.ndarray:
        """
        从 bg_path 读取背景音乐，在 [start_time, start_time+duration] 区间截取，
        与 audio_data (人声) 混合。
        """
        background_audio, sr = sf.read(bg_path)
        background_audio = np.asarray(background_audio, dtype=np.float32)
        if sr != self.sample_rate:
            logger.warning(
                f"背景音采样率={sr} 与目标={self.sample_rate}不匹配, 未做重采样, 可能有问题."
            )

        target_length = int(duration * self.sample_rate)
        start_sample = int(start_time * self.sample_rate)
        end_sample   = start_sample + target_length

        if end_sample <= len(background_audio):
            bg_segment = background_audio[start_sample:end_sample]
        else:
            bg_segment = background_audio[start_sample:]

        result = np.zeros(target_length, dtype=np.float32)
        audio_len = min(len(audio_data), target_length)
        bg_len    = min(len(bg_segment), target_length)

        # 混合人声 & 背景
        if audio_len > 0:
            result[:audio_len] = audio_data[:audio_len] * self.vocals_volume
        if bg_len > 0:
            result[:bg_len] += bg_segment[:bg_len] * self.background_volume

        return result

    # -------------------------------------------------------------------------
    # 辅助函数: 对音频做简单归一化
    # -------------------------------------------------------------------------
    def _normalize_audio(self, audio_data: np.ndarray) -> np.ndarray:
        if len(audio_data) == 0:
            return audio_data
        max_val = np.max(np.abs(audio_data))
        if max_val > self.max_val:
            audio_data = audio_data * (self.max_val / max_val)
        return audio_data

    # -------------------------------------------------------------------------
    # 核心函数: 将 audio_data 与截取后的视频段合并
    # -------------------------------------------------------------------------
    @handle_errors(logger)
    async def _add_video_segment(
        self,
        video_path: str,
        start_time: float,
        duration: float,
        audio_data: np.ndarray,
        output_path: str,
        sentences: List[Sentence],
        generate_subtitle: bool,
        task_state: TaskState
    ):
        """
        从原视频里截取 [start_time, start_time + duration] 的视频段(无声)，
        与合成音频合并。
        若 generate_subtitle=True, 则生成 .ass 字幕并在 ffmpeg 工具中进行"烧制"。
        """
        if not os.path.exists(video_path):
            raise FileNotFoundError(f"_add_video_segment: 视频文件不存在: {video_path}")
        if len(audio_data) == 0:
            raise ValueError("_add_video_segment: 无音频数据")
        if duration <= 0:
            raise ValueError("_add_video_segment: 无效时长 <=0")

        with ExitStack() as stack:
            temp_video = stack.enter_context(NamedTemporaryFile(suffix='.mp4'))
            temp_audio = stack.enter_context(NamedTemporaryFile(suffix='.wav'))

            end_time = start_time + duration

            # 1) 截取视频 (无音轨)
            await self.ffmpeg_tool.cut_video_track(
                input_path=video_path,
                output_path=temp_video.name,
                start=start_time,
                end=end_time
            )

            # 2) 写合成音频到临时文件
            await asyncio.to_thread(sf.write, temp_audio.name, audio_data, self.sample_rate)

            # 3) 如果需要字幕，则构建 .ass 并用 ffmpeg "烧"进去
            if generate_subtitle:
                temp_ass = stack.enter_context(NamedTemporaryFile(suffix='.ass'))
                # 调用生成字幕的函数
                # (这里把 task_state.target_language 传给它)
                await asyncio.to_thread(
                    self._generate_subtitles_for_segment,
                    sentences,
                    start_time * 1000,   # segment_start_ms
                    temp_ass.name,
                    task_state.target_language  # 新增：把目标语言传进去
                )

                # 生成带字幕的视频
                await self.ffmpeg_tool.cut_video_with_subtitles_and_audio(
                    input_video_path=temp_video.name,
                    input_audio_path=temp_audio.name,
                    subtitles_path=temp_ass.name,
                    output_path=output_path
                )
            else:
                # 不加字幕，仅合并音频
                await self.ffmpeg_tool.cut_video_with_audio(
                    input_video_path=temp_video.name,
                    input_audio_path=temp_audio.name,
                    output_path=output_path
                )

    # -------------------------------------------------------------------------
    # 核心字幕生成函数: 生成 .ass, 带"YouTube风格" + 多语言拆分
    # -------------------------------------------------------------------------
    def _generate_subtitles_for_segment(
        self,
        sentences: List[Sentence],
        segment_start_ms: float,
        output_sub_path: str,
        target_language: str = "en"
    ):
        """
        使用 pysubs2 生成 ASS 字幕文件.
        1. 遍历每条 Sentence, 将其文本拆分成若干块(避免过长字幕).
        2. 向 pysubs2 中写入事件, 并设置"类YouTube"的默认样式.
        3. 最后保存为 .ass 文件.

        Args:
            sentences: 本段的句子列表
            segment_start_ms: 当前段在全局中起始毫秒 (用来计算相对时间)
            output_sub_path: 存放字幕的 .ass 路径
            target_language: 用来确定拆分逻辑(中文/英文/日文/韩文)
        """
        subs = pysubs2.SSAFile()

        for s in sentences:
            # 计算相对时间
            start_local = s.adjusted_start - segment_start_ms - s.segment_start * 1000

            sub_text = (s.trans_text or s.raw_text or "").strip()
            if not sub_text:
                continue

            # 直接使用adjusted_duration作为duration_ms
            duration_ms = s.duration / s.speed
            if duration_ms <= 0:
                continue

            # 如果 Sentence 本身带 lang, 就优先使用 s.lang, 否则用 target_language
            lang = target_language or "en"

            # 拆分长句子 -> 多段 sequential
            blocks = self._split_long_text_to_sub_blocks(
                text=sub_text,
                start_ms=start_local,
                duration_ms=duration_ms,
                lang=lang
            )

            for block in blocks:
                evt = pysubs2.SSAEvent(
                    start=int(block["start"]),
                    end=int(block["end"]),
                    text=block["text"]
                )
                subs.append(evt)

        # 设置"类YouTube"的默认样式
        # 若 "Default" 不存在则创建
        style = subs.styles.get("Default", pysubs2.SSAStyle())

        style.fontname = "Arial"             # 常见无衬线
        style.fontsize = 22
        style.bold = True
        style.italic = False
        style.underline = False

        # 颜色 (R, G, B, A=0 => 不透明)
        # 文字: 白色,  描边/背景: 黑色
        style.primarycolor = pysubs2.Color(255, 255, 255, 0)
        style.outlinecolor = pysubs2.Color(0, 0, 0, 100)  # 半透明黑
        style.borderstyle = 3  # 3 => 有背景块
        # style.outline = 4      # 背景矩形厚度
        style.shadow = 0
        style.alignment = pysubs2.Alignment.BOTTOM_CENTER
        style.marginv = 20    # 离底部像素
        # style.marginl = 30      # 左边距，根据需要调整
        # style.marginr = 30      # 右边距，根据需要调整

        # 更新回 default
        subs.styles["Default"] = style

        # 写入文件
        subs.save(output_sub_path, format="ass")
        logger.debug(f"_generate_subtitles_for_segment: 已写入字幕 => {output_sub_path}")

    # -------------------------------------------------------------------------
    # 字幕拆分逻辑: 根据语言不同设定 max_chars, 尝试在标点/空格/单词边界处断行
    # -------------------------------------------------------------------------
    def _split_long_text_to_sub_blocks(
        self,
        text: str,
        start_ms: float,
        duration_ms: float,
        lang: str = "en"
    ) -> List[dict]:
        """
        将文本在 [start_ms, start_ms+duration_ms] 区间内拆分成多块 sequential 字幕，
        并按照每块的字符数在总时长内做比例分配。

        lang: "zh"/"ja"/"ko"/"en", 若无匹配则默认英文
        """
        # 1) 确定每行最大字符数
        recommended_max_chars = {
            "zh": 20,
            "ja": 20,
            "ko": 20,
            "en": 40
        }
        if lang not in recommended_max_chars:
            lang = "en"
        max_chars = recommended_max_chars[lang]

        if len(text) <= max_chars:
            # 不需要拆分
            return [{
                "start": start_ms,
                "end":   start_ms + duration_ms,
                "text":  text
            }]

        # 2) 根据语言拆分成若干行
        chunks = self._chunk_text_by_language(text, lang, max_chars)

        # 3) 根据各行的字符数在总时长内进行时间分配
        sub_blocks = []
        total_chars = sum(len(c) for c in chunks)
        current_start = start_ms

        for c in chunks:
            chunk_len = len(c)
            chunk_dur = duration_ms * (chunk_len / total_chars) if total_chars > 0 else 0
            block_start = current_start
            block_end   = current_start + chunk_dur

            sub_blocks.append({
                "start": block_start,
                "end":   block_end,
                "text":  c
            })
            current_start += chunk_dur

        # 修正最后一块结束
        if sub_blocks:
            sub_blocks[-1]["end"] = start_ms + duration_ms
        else:
            # 理论上不会发生
            sub_blocks.append({
                "start": start_ms,
                "end":   start_ms + duration_ms,
                "text":  text
            })

        return sub_blocks

    def _chunk_text_by_language(self, text: str, lang: str, max_chars: int) -> List[str]:
        """
        根据语言做拆分:
         - 英文: 按单词拆, 避免截断单词
         - 中/日/韩: 按字符拆, 尝试在标点附近断行
        """
        cjk_puncts = set("，,。.!！？?；;：:、…~— ")
        eng_puncts = set(".,!?;: ")

        if lang == "en":
            return self._chunk_english_text(text, max_chars, eng_puncts)
        else:
            return self._chunk_cjk_text(text, max_chars, cjk_puncts)

    def _chunk_english_text(self, text: str, max_chars: int, puncts: set) -> List[str]:
        words = text.split()
        chunks = []
        current_line = []

        for w in words:
            # 计算本行加上下一个单词后长多少
            line_len = sum(len(x) for x in current_line) + len(current_line)  # 单词总长 + 空格数
            if line_len + len(w) > max_chars:
                if current_line:
                    chunks.append(" ".join(current_line))
                    current_line = []
            current_line.append(w)

        # 收尾
        if current_line:
            chunks.append(" ".join(current_line))

        return chunks

    def _chunk_cjk_text(self, text: str, max_chars: int, puncts: set) -> List[str]:
        chunks = []
        total_length = len(text)
        start_idx = 0

        while start_idx < total_length:
            # 基础结束位置
            end_idx = start_idx + max_chars
            
            # 如果下一个字符是标点，则包含到当前块
            if end_idx < total_length and text[end_idx] in puncts:
                end_idx += 1  # 包含标点符号

            # 确保不越界
            end_idx = min(end_idx, total_length)
            
            # 截取当前块
            chunk = text[start_idx:end_idx]
            chunks.append(chunk)
            
            # 移动起始位置
            start_idx = end_idx

        return chunks

    # -------------------------------------------------------------------------
    # 重置mixer状态(可选调用)
    # -------------------------------------------------------------------------
    async def reset(self):
        """
        重置 full_audio_buffer, 适合在一次任务结束后做清理。
        """
        self.full_audio_buffer = np.array([], dtype=np.float32)
        logger.debug("MediaMixer 已重置 full_audio_buffer")

================
File: core/model_in.py
================
import os
import logging
import torch
import numpy as np
import librosa
from typing import List, Optional
import asyncio
import ray

# [NEW] 统一使用 concurrency.run_sync
from utils import concurrency

class ModelIn:
    def __init__(self, cosyvoice_model_actor):
        """
        Args:
            cosyvoice_model_actor: CosyVoice模型Actor引用
        """
        self.cosyvoice_actor = cosyvoice_model_actor
        self.logger = logging.getLogger(__name__)

        # 获取采样率（同步调用，只在初始化时执行一次）
        self.cosy_sample_rate = ray.get(self.cosyvoice_actor.get_sample_rate.remote())
        self.speaker_cache = {}
        self.max_val = 0.8

        self.semaphore = asyncio.Semaphore(4)  # max_concurrent_tasks=4
        
        self.logger.info(f"ModelIn initialized with CosyVoice Actor")

    def postprocess(self, speech, top_db=60, hop_length=220, win_length=440):
        speech, _ = librosa.effects.trim(
            speech, top_db=top_db,
            frame_length=win_length,
            hop_length=hop_length
        )
        if speech.abs().max() > self.max_val:
            speech = speech / speech.abs().max() * self.max_val
        
        speech = torch.concat([speech, torch.zeros(1, int(self.cosy_sample_rate * 0.2))], dim=1)
        return speech

    async def _update_text_features_sync(self, sentence):
        """
        更新文本特征（现在调用Actor）
        """
        try:
            tts_text = sentence.trans_text
            
            # 调用Actor的normalize_text方法
            normalized_segments = await concurrency.run_sync(
                lambda: ray.get(self.cosyvoice_actor.normalize_text.remote(tts_text, split=True))
            )
            
            segment_tokens = []
            segment_token_lens = []

            for seg in normalized_segments:
                # 调用Actor提取文本token
                txt, txt_len = await concurrency.run_sync(
                    lambda s=seg: ray.get(self.cosyvoice_actor.extract_text_tokens.remote(s))
                )
                segment_tokens.append(txt)
                segment_token_lens.append(txt_len)

            sentence.model_input['text'] = segment_tokens
            sentence.model_input['text_len'] = segment_token_lens
            sentence.model_input['normalized_text_segments'] = normalized_segments

            self.logger.debug(f"成功更新文本特征: {normalized_segments}")
            return sentence
        except Exception as e:
            self.logger.error(f"更新文本特征失败: {str(e)}")
            raise

    async def _process_sentence_sync(self, sentence, reuse_speaker=False, reuse_uuid=False):
        """
        处理单个句子（现在调用Actor）
        """
        speaker_id = sentence.speaker_id

        # 1) Speaker处理
        if not reuse_speaker:
            if speaker_id not in self.speaker_cache:
                # 调用Actor处理音频
                processed_audio = await concurrency.run_sync(
                    lambda: ray.get(self.cosyvoice_actor.postprocess_audio.remote(
                        sentence.audio, max_val=self.max_val
                    ))
                )
                
                # 调用Actor提取说话人特征
                self.speaker_cache[speaker_id] = await concurrency.run_sync(
                    lambda: ray.get(self.cosyvoice_actor.extract_speaker_features.remote(processed_audio))
                )
                
            speaker_features = self.speaker_cache[speaker_id].copy()
            sentence.model_input = speaker_features

        # 2) UUID处理
        if not reuse_uuid:
            sentence.model_input['uuid'] = ""

        # 3) 文本特征更新
        await self._update_text_features_sync(sentence)
        return sentence

    async def _process_sentence_async(self, sentence, reuse_speaker=False, reuse_uuid=False):
        """在异步方法中，对单个 sentence 做同步处理"""
        async with self.semaphore:
            # 确保sentence是真实对象，不是协程
            if asyncio.iscoroutine(sentence):
                sentence = await sentence  # 等待协程完成
            
            # 然后执行处理
            return await self._process_sentence_sync(sentence, reuse_speaker, reuse_uuid)

    async def modelin_maker(self,
                            sentences,
                            reuse_speaker=False,
                            reuse_uuid=False,
                            batch_size=3):
        """
        对一批 sentences 做 model_in 处理，分批 yield
        """
        if not sentences:
            self.logger.warning("modelin_maker: 收到空的句子列表")
            return

        tasks = []
        for s in sentences:
            tasks.append(
                asyncio.create_task(
                    self._process_sentence_async(s, reuse_speaker, reuse_uuid)
                )
            )

        try:
            results = []
            for i, task in enumerate(tasks, start=1):
                updated_sentence = await task
                results.append(updated_sentence)

                if i % batch_size == 0:
                    yield results
                    results = []

            if results:
                yield results

        except Exception as e:
            self.logger.error(f"modelin_maker处理失败: {str(e)}")
            raise

        finally:
            # 不复用speaker时，清空cache
            if not reuse_speaker:
                self.speaker_cache.clear()
                self.logger.debug("modelin_maker: 已清理 speaker_cache")

================
File: core/sentence_tools.py
================
import os
import torch
import torchaudio
import numpy as np
from typing import List, Tuple, Dict
from dataclasses import dataclass, field
from pathlib import Path
from config import Config

Token = int
Timestamp = Tuple[float, float]
SpeakerSegment = Tuple[float, float, int]

@dataclass
class Sentence:
    raw_text: str
    start: float
    end: float
    speaker_id: int
    trans_text: str = field(default="")
    sentence_id: int = field(default=-1)
    audio: torch.Tensor = field(default=None)
    target_duration: float = field(default=None)
    duration: float = field(default=0.0)
    diff: float = field(default=0.0)
    silence_duration: float = field(default=0.0)
    speed: float = field(default=1.0)
    is_first: bool = field(default=False)
    is_last: bool = field(default=False)
    model_input: Dict = field(default_factory=dict)
    generated_audio: np.ndarray = field(default=None)
    adjusted_start: float = field(default=0.0)
    adjusted_duration: float = field(default=0.0)
    segment_index: int = field(default=-1)
    segment_start: float = field(default=0.0)
    task_id: str = field(default="")

def tokens_timestamp_sentence(tokens: List[Token], timestamps: List[Timestamp], speaker_segments: List[SpeakerSegment], tokenizer, config: Config) -> List[Tuple[List[Token], List[Timestamp], int]]:
    sentences = []
    current_tokens = []
    current_timestamps = []
    token_index = 0

    for segment in speaker_segments:
        seg_start_ms = int(segment[0] * 1000)
        seg_end_ms = int(segment[1] * 1000)
        speaker_id = segment[2]

        while token_index < len(tokens):
            token = tokens[token_index]
            token_start, token_end = timestamps[token_index]

            if token_start >= seg_end_ms:
                break
            if token_end <= seg_start_ms:
                token_index += 1
                continue

            current_tokens.append(token)
            current_timestamps.append(timestamps[token_index])
            token_index += 1

            if token in config.STRONG_END_TOKENS and len(current_tokens) <= config.MIN_SENTENCE_LENGTH:
                if sentences:
                    previous_end_time = sentences[-1][1][-1][1]
                    current_start_time = current_timestamps[0][0]
                    time_gap = current_start_time - previous_end_time

                    if time_gap > config.SHORT_SENTENCE_MERGE_THRESHOLD_MS:
                        continue

                    sentences[-1] = (
                        sentences[-1][0] + current_tokens[:],
                        sentences[-1][1] + current_timestamps[:],
                        sentences[-1][2]
                    )
                    current_tokens.clear()
                    current_timestamps.clear()
                continue

            if (token in config.STRONG_END_TOKENS or len(current_tokens) > config.MAX_TOKENS_PER_SENTENCE):
                sentences.append((current_tokens[:], current_timestamps[:], speaker_id))
                current_tokens.clear()
                current_timestamps.clear()

        if current_tokens:
            if len(current_tokens) >= config.MIN_SENTENCE_LENGTH or not sentences:
                sentences.append((current_tokens[:], current_timestamps[:], speaker_id))
                current_tokens.clear()
                current_timestamps.clear()
            else:
                continue

    if current_tokens:
        if len(current_tokens) >= config.MIN_SENTENCE_LENGTH or not sentences:
            sentences.append((current_tokens[:], current_timestamps[:], speaker_id))
            current_tokens.clear()
            current_timestamps.clear()
        else:
            sentences[-1] = (
                sentences[-1][0] + current_tokens[:],
                sentences[-1][1] + current_timestamps[:],
                sentences[-1][2]
            )
            current_tokens.clear()
            current_timestamps.clear()

    return sentences

def merge_sentences(raw_sentences: List[Tuple[List[Token], List[Timestamp], int]], 
                   tokenizer,
                   input_duration: float,
                   config: Config) -> List[Sentence]:
    merged_sentences = []
    current = None
    current_tokens_count = 0

    for tokens, timestamps, speaker_id in raw_sentences:
        time_gap = timestamps[0][0] - current.end if current else float('inf')
        
        if (current and 
            current.speaker_id == speaker_id and 
            current_tokens_count + len(tokens) <= config.MAX_TOKENS_PER_SENTENCE and
            time_gap <= config.MAX_GAP_MS):
            current.raw_text += tokenizer.decode(tokens)
            current.end = timestamps[-1][1]
            current_tokens_count += len(tokens)
        else:
            if current:
                current.target_duration = timestamps[0][0] - current.start
                merged_sentences.append(current)
            
            text = tokenizer.decode(tokens)
            current = Sentence(
                raw_text=text, 
                start=timestamps[0][0], 
                end=timestamps[-1][1], 
                speaker_id=speaker_id,
            )
            current_tokens_count = len(tokens)

    if current:
        current.target_duration = input_duration - current.start
        merged_sentences.append(current)

    if merged_sentences:
        merged_sentences[0].is_first = True
        merged_sentences[-1].is_last = True

    return merged_sentences

def extract_audio(sentences: List[Sentence], speech: torch.Tensor, sr: int, config: Config) -> List[Sentence]:
    target_samples = int(config.SPEAKER_AUDIO_TARGET_DURATION * sr)
    speech = speech.unsqueeze(0) if speech.dim() == 1 else speech

    speaker_segments: Dict[int, List[Tuple[int, int, int]]] = {}
    for idx, s in enumerate(sentences):
        start_sample = int(s.start * sr / 1000)
        end_sample = int(s.end * sr / 1000)
        speaker_segments.setdefault(s.speaker_id, []).append((start_sample, end_sample, idx))

    speaker_audio_cache: Dict[int, torch.Tensor] = {}

    for speaker_id, segments in speaker_segments.items():
        segments.sort(key=lambda x: x[1] - x[0], reverse=True)
        longest_start, longest_end, _ = segments[0]

        ignore_samples = int(0.5 * sr)
        adjusted_start = longest_start + ignore_samples
        available_length_adjusted = longest_end - adjusted_start

        if available_length_adjusted > 0:
            audio_length = min(target_samples, available_length_adjusted)
            speaker_audio = speech[:, adjusted_start:adjusted_start + audio_length]
        else:
            available_length_original = longest_end - longest_start
            audio_length = min(target_samples, available_length_original)
            speaker_audio = speech[:, longest_start:longest_start + audio_length]

        speaker_audio_cache[speaker_id] = speaker_audio

    for sentence in sentences:
        sentence.audio = speaker_audio_cache.get(sentence.speaker_id)

    output_dir = Path(config.TASKS_DIR) / sentences[0].task_id / 'speakers'
    output_dir.mkdir(parents=True, exist_ok=True)

    for speaker_id, audio in speaker_audio_cache.items():
        if audio is not None:
            output_path = output_dir / f'speaker_{speaker_id}.wav'
            torchaudio.save(str(output_path), audio, sr)

    return sentences

def get_sentences(tokens: List[Token],
                  timestamps: List[Timestamp],
                  speech: torch.Tensor,
                  tokenizer,
                  sd_time_list: List[SpeakerSegment],
                  sample_rate: int = 16000,
                  config: Config = None) -> List[Sentence]:
    if config is None:
        config = Config()

    input_duration = (speech.shape[-1] / sample_rate) * 1000

    raw_sentences = tokens_timestamp_sentence(tokens, timestamps, sd_time_list, tokenizer, config)
    merged_sentences = merge_sentences(raw_sentences, tokenizer, input_duration, config)
    sentences_with_audio = extract_audio(merged_sentences, speech, sample_rate, config)

    return sentences_with_audio

================
File: core/tts_token_gener.py
================
import logging
import asyncio
import uuid
import torch
import ray

from utils import concurrency

class TTSTokenGenerator:
    def __init__(self, cosyvoice_model_actor, Hz=25):
        """
        Args:
            cosyvoice_model_actor: CosyVoice模型Actor引用
            Hz: token频率
        """
        self.cosyvoice_actor = cosyvoice_model_actor
        self.Hz = Hz
        self.logger = logging.getLogger(__name__)

    async def tts_token_maker(self, sentences, reuse_uuid=False):
        """生成TTS tokens（调用Actor）"""
        try:
            # 确保sentences不是协程
            if asyncio.iscoroutine(sentences):
                self.logger.warning("收到协程对象而非句子列表，尝试等待...")
                sentences = await sentences
            
            tasks = []
            for s in sentences:
                # 检查单个句子是否为协程
                if asyncio.iscoroutine(s):
                    self.logger.warning(f"句子{id(s)}是协程，等待...")
                    s = await s
                
                current_uuid = (
                    s.model_input.get('uuid') if reuse_uuid and s.model_input.get('uuid')
                    else str(uuid.uuid1())
                )
                # 创建异步任务
                tasks.append(asyncio.create_task(
                    self._generate_tts_single_async(s, current_uuid)
                ))

            processed = await asyncio.gather(*tasks)

            for sen in processed:
                if not sen.model_input.get('segment_speech_tokens'):
                    self.logger.error(f"TTS token 生成失败: {sen.trans_text}")

            return processed

        except Exception as e:
            self.logger.error(f"TTS token 生成失败: {e}")
            raise

    async def _generate_tts_single_async(self, sentence, main_uuid):
        """异步调用Actor生成TTS tokens"""
        # 检查sentence是否为协程
        if asyncio.iscoroutine(sentence):
            self.logger.warning(f"在_generate_tts_single_async中接收到协程，等待...")
            sentence = await sentence
        
        # 使用直接方式而非run_sync
        try:
            # 使用内部同步实现直接生成
            return self._generate_tts_single(sentence, main_uuid)
        except Exception as e:
            self.logger.error(f"处理失败 (UUID={main_uuid}): {e}")
            raise

    def _generate_tts_single(self, sentence, main_uuid):
        """同步调用Actor生成TTS tokens"""
        model_input = sentence.model_input
        segment_tokens_list = []
        segment_uuids = []
        total_token_count = 0

        try:
            for i, (text, text_len) in enumerate(zip(model_input['text'], model_input['text_len'])):
                seg_uuid = f"{main_uuid}_seg_{i}"
                
                # 调用Actor生成TTS tokens
                prompt_text = model_input.get('prompt_text', torch.zeros(1, 0, dtype=torch.int32))
                llm_prompt_speech_token = model_input.get('llm_prompt_speech_token', torch.zeros(1, 0, dtype=torch.int32))
                llm_embedding = model_input.get('llm_embedding', torch.zeros(0, 192))
                
                seg_tokens = ray.get(self.cosyvoice_actor.generate_tts_tokens.remote(
                    text, prompt_text, llm_prompt_speech_token, llm_embedding, seg_uuid
                ))

                segment_tokens_list.append(seg_tokens)
                segment_uuids.append(seg_uuid)
                total_token_count += len(seg_tokens)

            total_duration_s = total_token_count / self.Hz
            sentence.duration = total_duration_s * 1000

            model_input['segment_speech_tokens'] = segment_tokens_list
            model_input['segment_uuids'] = segment_uuids
            model_input['uuid'] = main_uuid

            self.logger.debug(
                f"TTS token 生成完成 (UUID={main_uuid}, 时长={total_duration_s:.2f}s, "
                f"段数={len(segment_uuids)})"
            )
            return sentence

        except Exception as e:
            self.logger.error(f"生成失败 (UUID={main_uuid}): {e}")
            # 调用Actor清理
            for seg_uuid in segment_uuids:
                ray.get(self.cosyvoice_actor.cleanup_tts_tokens.remote(seg_uuid))
            raise

================
File: utils/concurrency.py
================
# utils/concurrency.py
import functools
import asyncio
import os
from concurrent.futures import ThreadPoolExecutor

CPU_COUNT = os.cpu_count() or 1
GLOBAL_EXECUTOR = ThreadPoolExecutor(max_workers=8)

async def run_sync(func, *args, **kwargs):
    """在线程池中运行同步函数"""
    loop = asyncio.get_running_loop()
    partial_func = functools.partial(func, *args, **kwargs)
    return await loop.run_in_executor(GLOBAL_EXECUTOR, partial_func)

================
File: utils/decorators.py
================
import logging
import functools
from typing import Callable, Any, Optional, AsyncGenerator, TypeVar, Union, Literal
import asyncio
import time

logger = logging.getLogger(__name__)
T = TypeVar('T')
WorkerResult = Union[T, AsyncGenerator[T, None]]
WorkerMode = Literal['base', 'stream']

def handle_errors(custom_logger: Optional[logging.Logger] = None) -> Callable:
    """错误处理装饰器。可应用于需要统一捕获日志的异步函数。"""
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        async def wrapper(*args, **kwargs) -> Any:
            # 如果当前对象有 logger 属性则使用，否则用传入的或全局 logger
            actual_logger = custom_logger if custom_logger else (getattr(args[0], 'logger', logger) if args else logger)
            start_time = time.time()
            try:
                result = await func(*args, **kwargs)
                elapsed = time.time() - start_time
                actual_logger.debug(f"{func.__name__} 正常结束，耗时 {elapsed:.2f}s")
                return result
            except Exception as e:
                elapsed = time.time() - start_time
                actual_logger.error(f"{func.__name__} 执行出错，耗时 {elapsed:.2f}s, 错误: {e}", exc_info=True)
                raise
        return wrapper
    return decorator

def worker_decorator(
    input_queue_attr: str,
    next_queue_attr: Optional[str] = None,
    worker_name: Optional[str] = None,
    mode: WorkerMode = 'base'
) -> Callable:
    """通用 Worker 装饰器"""
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        async def wrapper(self, task_state, *args, **kwargs):
            worker_display_name = worker_name or func.__name__
            wlogger = getattr(self, 'logger', logger)

            input_queue = getattr(task_state, input_queue_attr)
            next_queue = getattr(task_state, next_queue_attr) if next_queue_attr else None

            wlogger.info(f"[{worker_display_name}] 启动 (TaskID={task_state.task_id}). "
                         f"输入队列: {input_queue_attr}, 下游队列: {next_queue_attr if next_queue_attr else '无'}")

            processed_count = 0
            try:
                while True:
                    try:
                        queue_size_before = input_queue.qsize()
                        item = await input_queue.get()
                        if item is None:
                            if next_queue:
                                await next_queue.put(None)
                            wlogger.info(f"[{worker_display_name}] 收到停止信号。已处理 {processed_count} 个item。")
                            break

                        wlogger.debug(f"[{worker_display_name}] 从 {input_queue_attr} 取出一个item. 队列剩余: {queue_size_before}")

                        start_time = time.time()
                        if mode == 'stream':
                            async for result in func(self, item, task_state, *args, **kwargs):
                                if result is not None and next_queue:
                                    await next_queue.put(result)
                        else:
                            result = await func(self, item, task_state, *args, **kwargs)
                            if result is not None and next_queue:
                                await next_queue.put(result)

                        processed_count += 1
                        elapsed = time.time() - start_time
                        wlogger.debug(f"[{worker_display_name}] item处理完成，耗时 {elapsed:.2f}s. "
                                      f"TaskID={task_state.task_id}, 已处理计数: {processed_count}")

                    except asyncio.CancelledError:
                        wlogger.warning(f"[{worker_display_name}] 被取消 (TaskID={task_state.task_id}). "
                                        f"已处理 {processed_count} 个item")
                        if next_queue:
                            await next_queue.put(None)
                        break
                    except Exception as e:
                        wlogger.error(f"[{worker_display_name}] 发生异常: {e} (TaskID={task_state.task_id}). "
                                      f"已处理 {processed_count} 个item", exc_info=True)
                        if next_queue:
                            await next_queue.put(None)
                        break
            finally:
                wlogger.info(f"[{worker_display_name}] 结束 (TaskID={task_state.task_id}). 共处理 {processed_count} 个item.")

        return wrapper
    return decorator

================
File: utils/ffmpeg_utils.py
================
# --------------------------------------
# utils/ffmpeg_utils.py
# 彻底移除 force_style, 仅使用 .ass 内部样式
# --------------------------------------
import asyncio
import logging
from pathlib import Path
from typing import List, Tuple, Optional

logger = logging.getLogger(__name__)

class FFmpegTool:
    """
    统一封装 FFmpeg 常见用法的工具类。
    通过异步方式执行 ffmpeg 命令，并在出错时抛出异常。
    """

    async def run_command(self, cmd: List[str]) -> Tuple[bytes, bytes]:
        """
        运行 ffmpeg 命令，返回 (stdout, stderr)。
        若命令返回码非 0，则抛出 RuntimeError。
        """
        logger.debug(f"[FFmpegTool] Running command: {' '.join(cmd)}")
        process = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        stdout, stderr = await process.communicate()

        if process.returncode != 0:
            error_msg = stderr.decode() or "Unknown error"
            logger.error(f"[FFmpegTool] Command failed with error: {error_msg}")
            raise RuntimeError(f"FFmpeg command failed: {error_msg}")

        return stdout, stderr

    async def extract_audio(
        self,
        input_path: str,
        output_path: str,
        start: float = 0.0,
        duration: Optional[float] = None
    ) -> None:
        """
        提取音频，可选指定起始时间与持续时长。
        输出为单声道 PCM float32 (48k/16k 视需求).
        """
        cmd = ["ffmpeg", "-y", "-i", input_path]
        if start > 0:
            cmd += ["-ss", str(start)]
        if duration is not None:
            cmd += ["-t", str(duration)]

        cmd += [
            "-vn",                # 去掉视频
            "-acodec", "pcm_f32le",
            "-ac", "1",
            output_path
        ]
        await self.run_command(cmd)

    async def extract_video(
        self,
        input_path: str,
        output_path: str,
        start: float = 0.0,
        duration: Optional[float] = None
    ) -> None:
        """
        提取纯视频（去掉音轨），可选指定起始时间与持续时长。
        """
        cmd = ["ffmpeg", "-y", "-i", input_path]
        if start > 0:
            cmd += ["-ss", str(start)]
        if duration is not None:
            cmd += ["-t", str(duration)]

        cmd += [
            "-an",                # 去掉音频
            "-c:v", "libx264",
            "-preset", "ultrafast",
            "-crf", "18",
            "-tune", "fastdecode",
            output_path
        ]
        await self.run_command(cmd)

    async def hls_segment(
        self,
        input_path: str,
        segment_pattern: str,
        playlist_path: str,
        hls_time: int = 10
    ) -> None:
        """
        将输入视频切割为 HLS 片段。
        segment_pattern 形如 "out%03d.ts"
        playlist_path   形如 "playlist.m3u8"
        """
        cmd = [
            "ffmpeg", "-y",
            "-i", input_path,
            "-c", "copy",
            "-f", "hls",
            "-hls_time", str(hls_time),
            "-hls_list_size", "0",
            "-hls_segment_type", "mpegts",
            "-hls_segment_filename", segment_pattern,
            playlist_path
        ]
        await self.run_command(cmd)

    async def cut_video_track(
        self,
        input_path: str,
        output_path: str,
        start: float,
        end: float
    ) -> None:
        """
        截取 [start, end] 的无声视频段，end为绝对秒数。
        """
        duration = end - start
        if duration <= 0:
            raise ValueError(f"Invalid duration: {duration}")

        cmd = [
            "ffmpeg", "-y",
            "-i", input_path,
            "-ss", str(start),
            "-t", str(duration),
            "-c:v", "libx264",
            "-preset", "superfast",
            "-an",  # 去除音轨
            "-vsync", "vfr",
            output_path
        ]
        await self.run_command(cmd)

    async def cut_video_with_audio(
        self,
        input_video_path: str,
        input_audio_path: str,
        output_path: str
    ) -> None:
        """
        将无声视频与音频合并 (视频copy，音频AAC)。
        """
        cmd = [
            "ffmpeg", "-y",
            "-i", input_video_path,
            "-i", input_audio_path,
            "-c:v", "copy",
            "-c:a", "aac",
            output_path
        ]
        await self.run_command(cmd)

    async def cut_video_with_subtitles_and_audio(
        self,
        input_video_path: str,
        input_audio_path: str,
        subtitles_path: str,
        output_path: str
    ) -> None:
        """
        将无声视频 + 音频 + .ass字幕 合并输出到 output_path。
        (无 force_style, 由 .ass 内样式全权决定)
        
        若字幕渲染失败，则回退到仅合并音视频。
        """
        # 检查输入文件是否存在
        for file_path in [input_video_path, input_audio_path, subtitles_path]:
            if not Path(file_path).exists():
                raise FileNotFoundError(f"文件不存在: {file_path}")

        # 构建"subtitles"过滤器, 不带 force_style
        escaped_path = subtitles_path.replace(':', r'\\:')

        try:
            # 方式1: subtitles 滤镜
            # 当前设置是合理的，但需要注意：
            cmd = [
                "ffmpeg", "-y",
                "-i", input_video_path,
                "-i", input_audio_path,
                "-filter_complex",
                f"[0:v]scale=1920:-2:flags=lanczos,subtitles='{escaped_path}'[v]",  # 修改点说明：
                # 1. scale=1920:-2 保持宽高比，-2 保证高度为偶数（兼容编码要求）
                # 2. flags=lanczos 使用高质量的缩放算法
                # 3. 滤镜顺序：先缩放视频，再加字幕（确保字幕在缩放后的画面上）
                "-map", "[v]",
                "-map", "1:a",
                "-c:v", "libx264",
                "-preset", "superfast",
                "-crf", "23",  # 建议添加 CRF 参数控制视频质量
                "-c:a", "aac",
                output_path
            ]
            await self.run_command(cmd)

        except RuntimeError as e:
            logger.warning(f"[FFmpegTool] subtitles滤镜方案失败: {str(e)}")
            # 方式2: 最终回退 - 仅合并音视频
            cmd = [
                "ffmpeg", "-y",
                "-i", input_video_path,
                "-i", input_audio_path,
                "-c:v", "copy",
                "-c:a", "aac",
                output_path
            ]
            await self.run_command(cmd)
            logger.warning("[FFmpegTool] 已跳过字幕，仅合并音视频")

    async def get_duration(self, input_path: str) -> float:
        """
        调用 ffprobe 获取输入文件的时长(秒)。
        """
        cmd = [
            "ffprobe",
            "-v", "error",
            "-show_entries", "format=duration",
            "-of", "default=noprint_wrappers=1:nokey=1",
            input_path
        ]
        stdout, stderr = await self.run_command(cmd)
        return float(stdout.decode().strip())

================
File: utils/gpu_manager.py
================
import ray
import logging
import psutil
import time
import numpy as np
from threading import Lock

logger = logging.getLogger(__name__)

class GPUResourceManager:
    """GPU资源自动分配管理器"""
    
    def __init__(self, total_gpus=1.0, min_gpu=0.1, check_interval=5):
        self.total_gpus = total_gpus  # 总GPU资源
        self.min_gpu = min_gpu         # 最小分配单位
        self.check_interval = check_interval  # 检查间隔(秒)
        self.lock = Lock()
        self.last_check = 0
        self.usage_history = []        # 保存历史使用率
        self.history_window = 3        # 历史窗口大小
        
        # 模型优先级和基础资源需求
        self.model_priority = {
            "cosyvoice": 0.7,    # TTS模型优先级高，基础资源需求0.7
            "sense_asr": 0.3,    # ASR模型优先级低，基础资源需求0.3
        }
    
    def _get_current_load(self):
        """获取当前系统负载"""
        try:
            # 1. CPU负载检查
            cpu_percent = psutil.cpu_percent(interval=0.1)
            # 2. 内存使用检查
            memory_percent = psutil.virtual_memory().percent
            # 3. GPU使用率检查 (可选，需要安装额外库如pynvml)
            
            # 综合负载分数 (0-100)
            load_score = (cpu_percent + memory_percent) / 2
            return load_score
        except Exception as e:
            logger.error(f"获取系统负载失败: {e}")
            return 50  # 默认中等负载
    
    def _check_ray_resources(self):
        """检查Ray集群资源状态"""
        try:
            resources = ray.available_resources()
            gpu_available = resources.get("GPU", 0)
            return {
                "gpu_available": gpu_available,
                "total_resources": resources
            }
        except Exception as e:
            logger.error(f"获取Ray资源失败: {e}")
            return {"gpu_available": 0.1, "total_resources": {}}
    
    def get_allocation(self, model_type):
        """根据负载情况为指定模型类型分配GPU资源"""
        with self.lock:  # 防止并发分配冲突
            # 避免频繁检查
            current_time = time.time()
            if current_time - self.last_check < self.check_interval:
                # 使用缓存的最近分配决策
                if len(self.usage_history) > 0:
                    load_score = np.mean(self.usage_history)
                else:
                    load_score = 50  # 默认中等负载
            else:
                # 更新负载并缓存
                load_score = self._get_current_load()
                self.usage_history.append(load_score)
                if len(self.usage_history) > self.history_window:
                    self.usage_history.pop(0)
                self.last_check = current_time
            
            # 检查Ray资源
            ray_resources = self._check_ray_resources()
            gpu_available = ray_resources["gpu_available"]
            
            # 负载级别分类 (0-100)
            if load_score < 30:  # 低负载
                scale_factor = 1.0
            elif load_score < 70:  # 中等负载 
                scale_factor = 0.8
            else:  # 高负载
                scale_factor = 0.6
            
            # 根据模型类型和优先级分配资源
            base_allocation = self.model_priority.get(model_type, 0.1)
            allocation = max(self.min_gpu, base_allocation * scale_factor)
            
            # 检查是否超出可用资源
            if allocation > gpu_available and gpu_available > self.min_gpu:
                logger.warning(f"GPU资源不足，调整分配: {allocation} -> {gpu_available}")
                allocation = gpu_available
            
            logger.info(f"模型 {model_type} 分配GPU: {allocation:.2f} (负载: {load_score:.1f}, 缩放: {scale_factor:.1f})")
            return allocation

================
File: utils/media_utils.py
================
# utils/media_utils.py
import logging
import asyncio
import numpy as np
import torch
import torchaudio
import librosa
import soundfile as sf
from pathlib import Path
from typing import List, Tuple, Dict, Union, Optional
from utils.decorators import handle_errors
from utils import concurrency

# 引入统一的 FFmpegTool
from utils.ffmpeg_utils import FFmpegTool

logger = logging.getLogger(__name__)

class MediaUtils:
    def __init__(self, config, audio_separator, target_sr: int = 24000):
        self.config = config
        self.target_sr = target_sr
        self.logger = logging.getLogger(__name__)
        self.audio_separator = audio_separator

        # 新增 ffmpeg 工具类
        self.ffmpeg_tool = FFmpegTool()

    def normalize_and_resample(
        self,
        audio_input: Union[Tuple[int, np.ndarray], np.ndarray],
        target_sr: int = None
    ) -> np.ndarray:
        """
        同步方式的重采样和归一化。
        若音频比较大，建议用 asyncio.to_thread(...) 包装本函数，以防阻塞事件循环。
        """
        if isinstance(audio_input, tuple):
            fs, audio_input = audio_input
        else:
            fs = target_sr

        audio_input = audio_input.astype(np.float32)

        max_val = np.abs(audio_input).max()
        if max_val > 0:
            audio_input = audio_input / max_val

        # 如果多通道, 转单通道
        if len(audio_input.shape) > 1:
            audio_input = audio_input.mean(axis=-1)

        # 如果源采样率与目标采样率不一致, 用 torchaudio 进行重采样
        if fs != target_sr:
            audio_input = np.ascontiguousarray(audio_input)
            resampler = torchaudio.transforms.Resample(
                orig_freq=fs,
                new_freq=target_sr,
                dtype=torch.float32
            )
            audio_input = resampler(torch.from_numpy(audio_input)[None, :])[0].numpy()

        return audio_input

    @handle_errors(logger)
    async def get_video_duration(self, video_path: str) -> float:
        """
        用 ffprobe 查询视频时长 (异步)，统一改用 FFmpegTool。
        """
        return await self.ffmpeg_tool.get_duration(video_path)

    @handle_errors(logger)
    async def get_audio_segments(self, duration: float) -> List[Tuple[float, float]]:
        """
        按照配置中的 SEGMENT_MINUTES 分割时间片。仅做一些计算，不会阻塞。
        """
        segment_length = self.config.SEGMENT_MINUTES * 60
        min_length = self.config.MIN_SEGMENT_MINUTES * 60

        if duration <= min_length:
            return [(0, duration)]

        segments = []
        current_pos = 0.0

        while current_pos < duration:
            remaining_duration = duration - current_pos

            if remaining_duration <= segment_length:
                # 如果剩余片段过短且已有片段，和前一个合并
                if remaining_duration < min_length and segments:
                    start = segments[-1][0]
                    new_duration = duration - start
                    segments[-1] = (start, new_duration)
                else:
                    segments.append((current_pos, remaining_duration))
                break

            segments.append((current_pos, segment_length))
            current_pos += segment_length

        return segments

    @handle_errors(logger)
    async def extract_segment(
        self,
        video_path: str,
        start: float,
        duration: float,
        output_dir: Path,
        segment_index: int
    ) -> Dict[str, Union[str, float]]:
        """
        1) 提取纯视频 + 音频
        2) 调用 audio_separator 分离人声/背景
        3) 重采样 + 写音频文件
        4) 返回分段文件信息 (video, vocals, background, duration)
        """
        temp_files = {}
        try:
            silent_video = str(output_dir / f"video_silent_{segment_index}.mp4")
            full_audio = str(output_dir / f"audio_full_{segment_index}.wav")
            vocals_audio = str(output_dir / f"vocals_{segment_index}.wav")
            background_audio = str(output_dir / f"background_{segment_index}.wav")

            # (1) 并发提取音频 & 视频
            await asyncio.gather(
                self.ffmpeg_tool.extract_audio(video_path, full_audio, start, duration),
                self.ffmpeg_tool.extract_video(video_path, silent_video, start, duration)
            )

            # (2) 分离人声
            def do_separate():
                return self.audio_separator.separate_audio(full_audio)

            vocals, background, sr = await concurrency.run_sync(do_separate)

            # (3) 重采样
            def do_resample_bg():
                return self.normalize_and_resample((sr, background), self.target_sr)

            background = await concurrency.run_sync(do_resample_bg)

            # 写入人声/背景音频
            def write_vocals():
                sf.write(vocals_audio, vocals, sr, subtype='FLOAT')

            def write_bg():
                sf.write(background_audio, background, self.target_sr, subtype='FLOAT')

            await concurrency.run_sync(write_vocals)
            await concurrency.run_sync(write_bg)

            segment_duration = len(vocals) / sr

            # optional: 删除原始整段音频
            Path(full_audio).unlink(missing_ok=True)

            temp_files = {
                'video': silent_video,
                'vocals': vocals_audio,
                'background': background_audio,
                'duration': segment_duration
            }
            return temp_files

        except Exception as e:
            # 清理已生成的临时文件
            for file_path in temp_files.values():
                if isinstance(file_path, str) and Path(file_path).exists():
                    Path(file_path).unlink()
            raise

================
File: utils/sentence_logger.py
================
import logging
import json
from pathlib import Path
from typing import List, Dict, Any
import asyncio
from utils.decorators import handle_errors

logger = logging.getLogger(__name__)

class SentenceLogger:
    """句子日志记录器"""
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self._lock = asyncio.Lock()
    
    def _format_sentence(self, sentence: Dict[str, Any]) -> Dict[str, Any]:
        return {
            'id': getattr(sentence, 'sentence_id', -1),
            'start_time': getattr(sentence, 'start', 0),
            'end_time': getattr(sentence, 'end', 0),
            'text': getattr(sentence, 'text', ''),
            'translation': getattr(sentence, 'translation', ''),
            'duration': getattr(sentence, 'duration', 0),
            'speaker_id': getattr(sentence, 'speaker_id', 0),
            'speaker_similarity': getattr(sentence, 'speaker_similarity', 0),
            'speaker_embedding': (
                getattr(sentence, 'speaker_embedding', []).tolist() 
                if hasattr(getattr(sentence, 'speaker_embedding', []), 'tolist') 
                else getattr(sentence, 'speaker_embedding', [])
            )
        }
    
    @handle_errors(logger)
    async def save_sentences(self, sentences: List[Dict[str, Any]], output_path: Path, task_id: str) -> None:
        async with self._lock:
            try:
                formatted_sentences = [self._format_sentence(s) for s in sentences]
                output_path.parent.mkdir(parents=True, exist_ok=True)
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(formatted_sentences, f, ensure_ascii=False, indent=2)
                
                self.logger.debug(f"已保存 {len(sentences)} 个句子到 {output_path}")
            except Exception as e:
                self.logger.error(f"保存句子信息失败: {e}")
                raise

================
File: utils/task_state.py
================
# ---------------------------------
# backend/utils/task_state.py (完整可复制版本)
# ---------------------------------
from dataclasses import dataclass, field
from typing import Any, Dict
import asyncio
from utils.task_storage import TaskPaths

@dataclass
class TaskState:
    """
    每个任务的独立状态：包括队列、处理进度、分段信息等
    """
    task_id: str
    video_path: str
    task_paths: TaskPaths
    hls_manager: Any = None
    target_language: str = "zh"

    # 已处理到的句子计数
    sentence_counter: int = 0

    # 时间戳记录
    current_time: float = 0

    # 第几个 HLS 批次 (混音后输出)
    batch_counter: int = 0

    # 每个分段对应的媒体文件信息
    segment_media_files: Dict[int, Dict[str, Any]] = field(default_factory=dict)

    # 各个异步队列
    translation_queue: asyncio.Queue = field(default_factory=asyncio.Queue)
    modelin_queue: asyncio.Queue = field(default_factory=asyncio.Queue)
    tts_token_queue: asyncio.Queue = field(default_factory=asyncio.Queue)
    duration_align_queue: asyncio.Queue = field(default_factory=asyncio.Queue)
    audio_gen_queue: asyncio.Queue = field(default_factory=asyncio.Queue)
    mixing_queue: asyncio.Queue = field(default_factory=asyncio.Queue)

    # 记录 mixing_worker 产出的每个 segment_xxx.mp4
    merged_segments: list = field(default_factory=list)

    # =========== (新增) ===========
    # 用户是否选择烧制字幕
    generate_subtitle: bool = False

================
File: utils/task_storage.py
================
import shutil
import logging
import os
from pathlib import Path
from config import Config

logger = logging.getLogger(__name__)

class TaskPaths:
    def __init__(self, config: Config, task_id: str):
        self.config = config
        self.task_id = task_id

        self.task_dir = config.TASKS_DIR / task_id
        self.input_dir = self.task_dir / "input"
        self.processing_dir = self.task_dir / "processing"
        self.output_dir = self.task_dir / "output"

        self.segments_dir = config.PUBLIC_DIR / "segments" / task_id
        self.playlist_path = config.PUBLIC_DIR / "playlists" / f"playlist_{task_id}.m3u8"

        self.media_dir = self.processing_dir / "media"
        self.processing_segments_dir = self.processing_dir / "segments"

    def create_directories(self):
        dirs = [
            self.task_dir,
            self.input_dir,
            self.processing_dir,
            self.output_dir,
            self.segments_dir,
            self.media_dir,
            self.processing_segments_dir
        ]
        for d in dirs:
            d.mkdir(parents=True, exist_ok=True)
            logger.debug(f"[TaskPaths] 创建目录: {d}")

    async def cleanup(self, keep_output: bool = False):
        try:
            if keep_output:
                logger.info(f"[TaskPaths] 保留输出目录, 即将清理输入/processing/segments")
                dirs_to_clean = [self.input_dir, self.processing_dir, self.segments_dir]
                for d in dirs_to_clean:
                    if d.exists():
                        shutil.rmtree(d)
                        logger.debug(f"[TaskPaths] 已清理: {d}")
            else:
                logger.info(f"[TaskPaths] 全量清理任务目录: {self.task_dir}")
                if self.task_dir.exists():
                    shutil.rmtree(str(self.task_dir))
                    logger.debug(f"[TaskPaths] 已删除: {self.task_dir}")

                if self.segments_dir.exists():
                    shutil.rmtree(str(self.segments_dir))
                    logger.debug(f"[TaskPaths] 已删除: {self.segments_dir}")
        except Exception as e:
            logger.error(f"[TaskPaths] 清理任务目录失败: {e}", exc_info=True)
            raise

================
File: utils/temp_file_manager.py
================
import logging
from pathlib import Path
from typing import Set

logger = logging.getLogger(__name__)

class TempFileManager:
    """临时文件管理器"""
    def __init__(self, base_dir: Path):
        self.base_dir = base_dir
        self.temp_files: Set[Path] = set()
    
    def add_file(self, file_path: Path) -> None:
        self.temp_files.add(Path(file_path))
    
    async def cleanup(self) -> None:
        for file_path in self.temp_files:
            try:
                if file_path.exists():
                    file_path.unlink()
                    logger.debug(f"已删除临时文件: {file_path}")
            except Exception as e:
                logger.warning(f"清理临时文件失败: {file_path}, 错误: {e}")
        self.temp_files.clear()

================
File: __init__.py
================
# 空文件即可，标识这是一个 Python 包

================
File: .cursorignore
================
# 忽略模型文件夹，因为包含大量模型文件和第三方代码
models/

================
File: .env.example
================
# ================================
# .env.example
# ================================

# 【可选】修改 FLASK_ENV，默认使用 development 方便调试
FLASK_ENV=development

# 翻译模型选择 (可选值: deepseek, glm4, gemini)
TRANSLATION_MODEL=deepseek

# API Keys (请替换为实际的密钥)
ZHIPUAI_API_KEY=your_zhipuai_key_here
GEMINI_API_KEY=your_gemini_key_here
DEEPSEEK_API_KEY=your_deepseek_key_here

================
File: api.py
================
# ------------------------------
# backend/api.py  (完整可复制版本)
# ------------------------------
import sys
from pathlib import Path
import logging
import uuid
import asyncio
from typing import Dict

import uvicorn
from fastapi import FastAPI, File, UploadFile, HTTPException, Request, Form
from fastapi.responses import JSONResponse, FileResponse, StreamingResponse, Response
from fastapi.templating import Jinja2Templates
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
import aiofiles

from config import Config
config = Config()
config.init_directories()

sys.path.extend(config.SYSTEM_PATHS)

logging.basicConfig(
    level=logging.INFO,
    format="%(levelname)s | %(asctime)s | %(name)s | L%(lineno)d | %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)
logger = logging.getLogger(__name__)

from video_translator import ViTranslator
from core.hls_manager import HLSManager
from utils.task_storage import TaskPaths
from fastapi import BackgroundTasks

app = FastAPI(debug=True)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

current_dir = Path(__file__).parent
templates = Jinja2Templates(directory=str(current_dir / "templates"))

vi_translator = ViTranslator(config=config)
task_results: Dict[str, dict] = {}

@app.get("/")
async def index(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

@app.post("/upload")
async def upload_video(
    video: UploadFile = File(...),
    target_language: str = Form("zh"),
    # =============== (新增) ================
    generate_subtitle: bool = Form(False),  # 是否烧制字幕
):
    """
    上传视频接口：
    - generate_subtitle: 用户是否选择生成并烧制字幕
    """
    try:
        if not video:
            raise HTTPException(status_code=400, detail="没有文件上传")
        
        if not video.content_type.startswith('video/'):
            raise HTTPException(status_code=400, detail="只支持视频文件")
            
        if target_language not in ["zh", "en", "ja", "ko"]:
            raise HTTPException(status_code=400, detail=f"不支持的目标语言: {target_language}")
        
        task_id = str(uuid.uuid4())
        task_paths = TaskPaths(config, task_id)
        task_paths.create_directories()
        
        video_path = task_paths.input_dir / f"original_{video.filename}"
        try:
            async with aiofiles.open(video_path, "wb") as f:
                content = await video.read()
                await f.write(content)
        except Exception as e:
            logger.error(f"保存文件失败: {str(e)}")
            raise HTTPException(status_code=500, detail="文件保存失败")
        
        hls_manager = HLSManager(config, task_id, task_paths)
        
        # ===================
        # 在这里传递 generate_subtitle 给 translator
        # ===================
        task = asyncio.create_task(vi_translator.trans_video(
            video_path=str(video_path),
            task_id=task_id,
            task_paths=task_paths,
            hls_manager=hls_manager,
            target_language=target_language,
            generate_subtitle=generate_subtitle,
        ))
        
        task_results[task_id] = {
            "status": "processing",
            "message": "视频处理中",
            "progress": 0
        }
        
        async def on_task_complete(t):
            try:
                result = await t
                if result.get('status') == 'success':
                    task_results[task_id].update({
                        "status": "success",
                        "message": "处理完成",
                        "progress": 100
                    })
                else:
                    task_results[task_id].update({
                        "status": "error",
                        "message": result.get('message', '处理失败'),
                        "progress": 0
                    })
            except Exception as e:
                logger.error(f"任务处理失败: {str(e)}")
                task_results[task_id].update({
                    "status": "error",
                    "message": str(e),
                    "progress": 0
                })
        
        task.add_done_callback(lambda t: asyncio.create_task(on_task_complete(t)))
        
        return {
            'status': 'processing',
            'task_id': task_id,
            'message': '视频上传成功，开始处理'
        }
    except HTTPException as e:
        raise e
    except Exception as e:
        logger.error(f"上传处理失败: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/task/{task_id}")
async def get_task_status(task_id: str):
    result = task_results.get(task_id)
    if not result:
        return {
            "status": "error",
            "message": "任务不存在",
            "progress": 0
        }
    return result

app.mount("/playlists", 
    StaticFiles(directory=str(config.PUBLIC_DIR / "playlists"), 
    check_dir=True), 
    name="playlists")

app.mount("/segments", 
    StaticFiles(
        directory=str(config.PUBLIC_DIR / "segments"), 
        check_dir=True
    ), 
    name="segments")

@app.get("/playlists/{task_id}/{filename}")
async def serve_playlist(task_id: str, filename: str):
    try:
        playlist_path = config.PUBLIC_DIR / "playlists" / filename
        if not playlist_path.exists():
            logger.error(f"播放列表未找到: {playlist_path}")
            raise HTTPException(status_code=404, detail="播放列表未找到")
        
        async with aiofiles.open(playlist_path, mode='rb') as f:
            content = await f.read()
            
        return Response(
            content=content,
            media_type='application/vnd.apple.mpegurl',
            headers={
                "Cache-Control": "public, max-age=3600",
                "Access-Control-Allow-Origin": "*"
            }
        )
    except Exception as e:
        logger.error(f"服务播放列表失败: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/segments/{task_id}/{filename}")
async def serve_segments(task_id: str, filename: str):
    try:
        segment_path = config.PUBLIC_DIR / "segments" / task_id / filename
        if not segment_path.exists():
            logger.error(f"片段文件未找到: {segment_path}")
            raise HTTPException(status_code=404, detail="片段文件未找到")
        
        # 使用StreamingResponse而非静态文件
        return StreamingResponse(
            open(segment_path, mode="rb"),
            media_type='video/MP2T',
            headers={
                "Cache-Control": "no-cache, no-store, must-revalidate",
                "Pragma": "no-cache",
                "Expires": "0",
                "Access-Control-Allow-Origin": "*"
            }
        )
    except Exception as e:
        logger.error(f"服务视频片段失败: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/download/{task_id}")
async def download_translated_video(task_id: str):
    final_video_path = config.TASKS_DIR / task_id / "output" / f"final_{task_id}.mp4"
    if not final_video_path.exists():
        raise HTTPException(status_code=404, detail="最终视频文件尚未生成或已被删除")
    return FileResponse(
        str(final_video_path),
        media_type='video/mp4',
        filename=f"final_{task_id}.mp4",
    )

if __name__ == "__main__":
    uvicorn.run(
        app,
        host=config.SERVER_HOST,
        port=config.SERVER_PORT,
        log_level="info"
    )

================
File: config.py
================
import os
from pathlib import Path
from dotenv import load_dotenv

current_dir = Path(__file__).parent
env_path = current_dir / '.env'
load_dotenv(env_path)

project_dir = current_dir.parent
storage_dir = project_dir / 'storage'

class Config:
    SERVER_HOST = "0.0.0.0"
    SERVER_PORT = 8000
    LOG_LEVEL = "DEBUG"

    BASE_DIR = storage_dir
    TASKS_DIR = BASE_DIR / "tasks"
    PUBLIC_DIR = BASE_DIR / "public"

    BATCH_SIZE = 6
    TARGET_SPEAKER_AUDIO_DURATION = 10
    VAD_SR = 16000
    VOCALS_VOLUME = 0.7
    BACKGROUND_VOLUME = 0.3
    AUDIO_OVERLAP = 1024
    NORMALIZATION_THRESHOLD = 0.9

    SEGMENT_MINUTES = 5
    MIN_SEGMENT_MINUTES = 3

    TRANSLATION_MODEL = os.getenv("TRANSLATION_MODEL", "deepseek")
    ZHIPUAI_API_KEY = os.getenv("ZHIPUAI_API_KEY", "")
    GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "")
    DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY", "")

    SYSTEM_PATHS = [
        str(current_dir / 'models' / 'CosyVoice'),
        str(current_dir / 'models' / 'ClearVoice'),
        str(current_dir / 'models' / 'CosyVoice' / 'third_party' / 'Matcha-TTS')
    ]

    MODEL_DIR = project_dir / "models"

    @property
    def MODEL_PATH(self) -> Path:
        return Path(self.MODEL_DIR)

    @property
    def BASE_PATH(self) -> Path:
        return self.BASE_DIR

    @property
    def TASKS_PATH(self) -> Path:
        return self.TASKS_DIR

    @property
    def PUBLIC_PATH(self) -> Path:
        return self.PUBLIC_DIR

    @classmethod
    def init_directories(cls):
        directories = [
            cls.BASE_DIR,
            cls.TASKS_DIR,
            cls.PUBLIC_DIR,
            cls.PUBLIC_DIR / "playlists",
            cls.PUBLIC_DIR / "segments"
        ]
        for dir_path in directories:
            dir_path.mkdir(parents=True, exist_ok=True)
            os.chmod(str(dir_path), 0o755)

    MAX_GAP_MS = 2000
    SHORT_SENTENCE_MERGE_THRESHOLD_MS = 1000
    MAX_TOKENS_PER_SENTENCE = 80
    MIN_SENTENCE_LENGTH = 4
    SENTENCE_END_TOKENS = {9686, 9688, 9676, 9705, 9728, 9729, 20046, 24883, 24879}
    STRONG_END_TOKENS = {9688, 9676, 9705, 9729, 20046, 24883}
    WEAK_END_TOKENS = {9686, 9728, 24879}
    SPEAKER_AUDIO_TARGET_DURATION = 8.0
    TRANSLATION_BATCH_SIZE = 50
    MODELIN_BATCH_SIZE = 3
    # 控制同时处理多少个视频分段
    MAX_PARALLEL_SEGMENTS = 2

    # 只保留Actor配置，方便Ray资源管理
    ASR_ACTOR_NUM_GPUS = 0.3  # 分配GPU比例，根据需要调整

    # ASR流程配置
    ASR_BATCH_SIZE_S = 60  # 音频批处理大小(秒)
    ASR_USE_ITN = True     # 使用逆文本规范化
    ASR_MERGE_VAD = False  # 是否合并VAD结果

================
File: pipeline_scheduler.py
================
# -----------------------------------------
# backend/pipeline_scheduler.py (节选完整示例)
# -----------------------------------------
import asyncio
import logging
from typing import List
from utils.decorators import worker_decorator
from utils.task_state import TaskState
from core.sentence_tools import Sentence

logger = logging.getLogger(__name__)

class PipelineScheduler:
    """
    多个Worker的调度器，负责翻译->model_in->tts_token->时长对齐->音频生成->混音 ...
    """

    def __init__(
        self,
        translator,
        model_in,
        tts_token_generator,
        duration_aligner,
        audio_generator,
        timestamp_adjuster,
        mixer,
        config
    ):
        self.logger = logging.getLogger(__name__)
        self.translator = translator
        self.model_in = model_in
        self.tts_token_generator = tts_token_generator
        self.duration_aligner = duration_aligner
        self.audio_generator = audio_generator
        self.timestamp_adjuster = timestamp_adjuster
        self.mixer = mixer
        self.config = config

        self._workers = []

    async def start_workers(self, task_state: TaskState):
        self.logger.info(f"[PipelineScheduler] start_workers -> TaskID={task_state.task_id}")
        self._workers = [
            asyncio.create_task(self._translation_worker(task_state)),
            asyncio.create_task(self._modelin_worker(task_state)),
            asyncio.create_task(self._tts_token_worker(task_state)),
            asyncio.create_task(self._duration_align_worker(task_state)),
            asyncio.create_task(self._audio_generation_worker(task_state)),
            asyncio.create_task(self._mixing_worker(task_state))
        ]

    async def stop_workers(self, task_state: TaskState):
        self.logger.info(f"[PipelineScheduler] stop_workers -> TaskID={task_state.task_id}")
        # 向translation_queue发送None，让整条流水线停止
        await task_state.translation_queue.put(None)
        await asyncio.gather(*self._workers, return_exceptions=True)
        self.logger.info(f"[PipelineScheduler] 所有Worker已结束 -> TaskID={task_state.task_id}")

    async def push_sentences_to_pipeline(self, task_state: TaskState, sentences: List[Sentence]):
        self.logger.debug(f"[push_sentences_to_pipeline] 放入 {len(sentences)} 个句子到 translation_queue, TaskID={task_state.task_id}")
        await task_state.translation_queue.put(sentences)

    # ------------------------------
    # 各 Worker 的实现
    # ------------------------------

    @worker_decorator(
        input_queue_attr='translation_queue',
        next_queue_attr='modelin_queue',
        worker_name='翻译Worker',
        mode='stream'
    )
    async def _translation_worker(self, sentences_list: List[Sentence], task_state: TaskState):
        if not sentences_list:
            return
        self.logger.debug(f"[翻译Worker] 收到 {len(sentences_list)} 句子, TaskID={task_state.task_id}")

        async for translated_batch in self.translator.translate_sentences(
            sentences_list,
            batch_size=self.config.TRANSLATION_BATCH_SIZE,
            target_language=task_state.target_language
        ):
            yield translated_batch

    @worker_decorator(
        input_queue_attr='modelin_queue',
        next_queue_attr='tts_token_queue',
        worker_name='模型输入Worker',
        mode='stream'
    )
    async def _modelin_worker(self, sentences_batch: List[Sentence], task_state: TaskState):
        if not sentences_batch:
            return
        self.logger.debug(f"[模型输入Worker] 收到 {len(sentences_batch)} 句子, TaskID={task_state.task_id}")

        async for updated_batch in self.model_in.modelin_maker(
            sentences_batch,
            reuse_speaker=False,
            reuse_uuid=False,
            batch_size=self.config.MODELIN_BATCH_SIZE
        ):
            yield updated_batch

    @worker_decorator(
        input_queue_attr='tts_token_queue',
        next_queue_attr='duration_align_queue',
        worker_name='TTS Token生成Worker'
    )
    async def _tts_token_worker(self, sentences_batch: List[Sentence], task_state: TaskState):
        if not sentences_batch:
            return
        self.logger.debug(f"[TTS Token生成Worker] 收到 {len(sentences_batch)} 句子, TaskID={task_state.task_id}")

        # 确保sentences_batch不是协程
        if asyncio.iscoroutine(sentences_batch):
            sentences_batch = await sentences_batch
        
        await self.tts_token_generator.tts_token_maker(sentences_batch, reuse_uuid=False)
        return sentences_batch

    @worker_decorator(
        input_queue_attr='duration_align_queue',
        next_queue_attr='audio_gen_queue',
        worker_name='时长对齐Worker'
    )
    async def _duration_align_worker(self, sentences_batch: List[Sentence], task_state: TaskState):
        if not sentences_batch:
            return
        self.logger.debug(f"[时长对齐Worker] 收到 {len(sentences_batch)} 句子, TaskID={task_state.task_id}")

        await self.duration_aligner.align_durations(sentences_batch)
        return sentences_batch

    @worker_decorator(
        input_queue_attr='audio_gen_queue',
        next_queue_attr='mixing_queue',
        worker_name='音频生成Worker'
    )
    async def _audio_generation_worker(self, sentences_batch: List[Sentence], task_state: TaskState):
        if not sentences_batch:
            return
        self.logger.debug(f"[音频生成Worker] 收到 {len(sentences_batch)} 句子, TaskID={task_state.task_id}")

        await self.audio_generator.vocal_audio_maker(sentences_batch)
        task_state.current_time = self.timestamp_adjuster.update_timestamps(sentences_batch, start_time=task_state.current_time)
        valid = self.timestamp_adjuster.validate_timestamps(sentences_batch)
        if not valid:
            self.logger.warning(f"[音频生成Worker] 检测到时间戳不连续, TaskID={task_state.task_id}")
        return sentences_batch

    @worker_decorator(
        input_queue_attr='mixing_queue',
        worker_name='混音Worker'
    )
    async def _mixing_worker(self, sentences_batch: List[Sentence], task_state: TaskState):
        if not sentences_batch:
            return
        seg_index = sentences_batch[0].segment_index
        self.logger.debug(f"[混音Worker] 收到 {len(sentences_batch)} 句, segment={seg_index}, TaskID={task_state.task_id}")

        output_path = task_state.task_paths.segments_dir / f"segment_{task_state.batch_counter}.mp4"

        # ========== (修改) ============
        # 将task_state.generate_subtitle 传给MediaMixer
        success = await self.mixer.mixed_media_maker(
            sentences=sentences_batch,
            task_state=task_state,
            output_path=str(output_path),
            generate_subtitle=task_state.generate_subtitle
        )

        if success and task_state.hls_manager:
            await task_state.hls_manager.add_segment(str(output_path), task_state.batch_counter)
            self.logger.info(f"[混音Worker] 分段 {task_state.batch_counter} 已加入 HLS, TaskID={task_state.task_id}")

            task_state.merged_segments.append(str(output_path))

        task_state.batch_counter += 1
        return None

================
File: postcss.config.json
================
{ "plugins": { "postcss-preset-env": {}, "autoprefixer": {} } }

================
File: video_translator.py
================
# ---------------------------------
# backend/video_translator.py (节选完整示例)
# ---------------------------------
import logging
import os
from typing import List, Dict, Any
from pathlib import Path
import ray
import asyncio

from core.auto_sense import SenseAutoModel
from models.CosyVoice.cosyvoice.cli.cosyvoice import CosyVoice2
from core.translation.translator import Translator
from core.translation.deepseek_client import DeepSeekClient
from core.translation.gemini_client import GeminiClient
from core.tts_token_gener import TTSTokenGenerator
from core.audio_gener import AudioGenerator
from core.timeadjust.duration_aligner import DurationAligner
from core.timeadjust.timestamp_adjuster import TimestampAdjuster
from core.media_mixer import MediaMixer
from utils.media_utils import MediaUtils
from pipeline_scheduler import PipelineScheduler
from core.audio_separator import ClearVoiceSeparator
from core.model_in import ModelIn
from utils.task_storage import TaskPaths
from config import Config
from utils.task_state import TaskState

from utils.ffmpeg_utils import FFmpegTool
from utils.gpu_manager import GPUResourceManager

logger = logging.getLogger(__name__)

class ViTranslator:
    """
    全局持有大模型(ASR/TTS/翻译)对象, ...
    """
    def __init__(self, config: Config = None):
        self.logger = logger
        self.config = config or Config()
        
        # 初始化Ray（如果尚未初始化）
        if not ray.is_initialized():
            ray.init(ignore_reinit_error=True)
        
        # 初始化GPU资源管理器
        self.gpu_manager = GPUResourceManager(
            total_gpus=1.0,  # 总GPU资源
            min_gpu=0.1      # 最小分配单位
        )
            
        self._init_global_models()

    def _init_global_models(self):
        self.logger.info("[ViTranslator] 初始化模型和工具...")
        
        # 音频分离器
        self.audio_separator = ClearVoiceSeparator(model_name='MossFormer2_SE_48K')
        
        # 为ASR模型动态分配GPU资源
        asr_gpu = self.gpu_manager.get_allocation("sense_asr")
        self.logger.info(f"ASR模型分配GPU: {asr_gpu}")
        
        # 创建ASR模型Actor
        from core.asr_model_actor import SenseAutoModelActor
        self.sense_model_actor = SenseAutoModelActor.options(
            num_gpus=asr_gpu,
            name="sense_asr_model"
        ).remote()
        
        # 为TTS模型动态分配GPU资源
        tts_gpu = self.gpu_manager.get_allocation("cosyvoice")
        self.logger.info(f"TTS模型分配GPU: {tts_gpu}")
        
        # 创建CosyVoice模型Actor
        from core.cosyvoice_model_actor import CosyVoiceModelActor
        self.cosyvoice_model_actor = CosyVoiceModelActor.options(
            num_gpus=tts_gpu,
            name="cosyvoice_model"
        ).remote("models/CosyVoice/pretrained_models/CosyVoice2-0.5B")
        
        # 获取采样率
        self.target_sr = ray.get(self.cosyvoice_model_actor.get_sample_rate.remote())

        # 其他核心工具
        self.media_utils = MediaUtils(config=self.config, audio_separator=self.audio_separator, target_sr=self.target_sr)
        self.model_in = ModelIn(self.cosyvoice_model_actor)
        self.tts_generator = TTSTokenGenerator(self.cosyvoice_model_actor, Hz=25)
        self.audio_generator = AudioGenerator(self.cosyvoice_model_actor, sample_rate=self.target_sr)

        # 翻译
        translation_model = (self.config.TRANSLATION_MODEL or "deepseek").strip().lower()
        if translation_model == "deepseek":
            self.translator = Translator(DeepSeekClient(api_key=self.config.DEEPSEEK_API_KEY))
        elif translation_model == "gemini":
            self.translator = Translator(GeminiClient(api_key=self.config.GEMINI_API_KEY))
        else:
            raise ValueError(f"不支持的翻译模型：{translation_model}")

        self.duration_aligner = DurationAligner(
            model_in=self.model_in,
            simplifier=self.translator,
            tts_token_gener=self.tts_generator,
            max_speed=1.2
        )
        self.timestamp_adjuster = TimestampAdjuster(sample_rate=self.target_sr)
        self.mixer = MediaMixer(config=self.config, sample_rate=self.target_sr)

        self.ffmpeg_tool = FFmpegTool()
        self.logger.info("[ViTranslator] 初始化完成")

    async def trans_video(
        self,
        video_path: str,
        task_id: str,
        task_paths: TaskPaths,
        hls_manager=None,
        target_language="zh",
        # =========== (新增) ===========
        generate_subtitle: bool = False,
    ) -> Dict[str, Any]:
        """
        入口：对整段视频进行处理。包括分段、ASR、翻译、TTS、混音、生成 HLS 等。
        generate_subtitle: 是否需要在最终生成的视频里烧制字幕
        """
        self.logger.info(
            f"[trans_video] 开始处理视频: {video_path}, task_id={task_id}, target_language={target_language}, generate_subtitle={generate_subtitle}"
        )

        # 初始化任务状态 + 管线
        task_state = TaskState(
            task_id=task_id,
            video_path=video_path,
            task_paths=task_paths,
            hls_manager=hls_manager,
            target_language=target_language,
            # =========== (新增) ===========
            generate_subtitle=generate_subtitle
        )

        pipeline = PipelineScheduler(
            translator=self.translator,
            model_in=self.model_in,
            tts_token_generator=self.tts_generator,
            duration_aligner=self.duration_aligner,
            audio_generator=self.audio_generator,
            timestamp_adjuster=self.timestamp_adjuster,
            mixer=self.mixer,
            config=self.config
        )
        await pipeline.start_workers(task_state)

        try:
            # 1. 获取视频总时长
            duration = await self.media_utils.get_video_duration(video_path)
            # 2. 划分分段
            segments = await self.media_utils.get_audio_segments(duration)
            self.logger.info(f"总长度={duration:.2f}s, 分段数={len(segments)}, 任务ID={task_id}")

            if not segments:
                self.logger.warning(f"没有可用分段 -> 任务ID={task_id}")
                await pipeline.stop_workers(task_state)
                return {"status": "error", "message": "无法获取有效分段"}

            # 3. 遍历所有分段：提取、ASR、推送后续流水线
            for i, (seg_start, seg_dur) in enumerate(segments):
                await self._process_segment(pipeline, task_state, i, seg_start, seg_dur)

            # 4. 所有段结束后，停止流水线
            await pipeline.stop_workers(task_state)

            # 5. 如果有 HLS Manager，标记完成
            if hls_manager and hls_manager.has_segments:
                await hls_manager.finalize_playlist()
                self.logger.info(f"[trans_video] 任务ID={task_id} 完成并已生成HLS。")

            # 6. 现在合并 `_mixing_worker` 产出的所有 segment_xxx.mp4
            #    并在成功后自动删除它们
            final_video_path = await self._concat_segment_mp4s(task_state)
            if final_video_path is not None and final_video_path.exists():
                self.logger.info(f"翻译后的完整视频已生成: {final_video_path}")
                
                # 添加清理逻辑：
                import torch
                torch.cuda.empty_cache()
                self.logger.info("调用 torch.cuda.empty_cache()，已释放未使用的 GPU 显存")
                
                # 如果有临时目录需要清理（例如 task_state.task_paths 里存放了临时文件），可以进行删除：
                # import shutil
                # shutil.rmtree(task_state.task_paths.temp_dir, ignore_errors=True)
                # self.logger.info("已清理视频处理临时目录")

                return {
                    "status": "success",
                    "message": "视频翻译完成",
                    "final_video_path": str(final_video_path)
                }
            else:
                self.logger.warning("无法合并生成最终MP4文件")
                return {"status": "error", "message": "HLS完成，但无法合并出最终MP4"}

        except Exception as e:
            self.logger.exception(f"[trans_video] 任务ID={task_id} 出错: {e}")
            return {"status": "error", "message": str(e)}

    async def _process_segment(
        self,
        pipeline: PipelineScheduler,
        task_state: TaskState,
        segment_index: int,
        start: float,
        seg_duration: float,
    ):
        # 1. 提取并分离人声/背景
        media_files = await self.media_utils.extract_segment(
            video_path=task_state.video_path,
            start=start,
            duration=seg_duration,
            output_dir=task_state.task_paths.processing_dir,
            segment_index=segment_index
        )
        task_state.segment_media_files[segment_index] = media_files

        # 2. ASR - 使用与Ray官方示例一致的语法
        asr_result = await self.sense_model_actor.generate_async.remote(
            input=media_files['vocals'],
            cache={},
            language="auto",
            use_itn=True,
            batch_size_s=60,
            merge_vad=False
        )
        
        self.logger.info(f"[_process_segment] ASR识别到 {len(asr_result)} 条句子, seg={segment_index}, TaskID={task_state.task_id}")

        if not asr_result:
            return

        for s in asr_result:
            s.segment_index = segment_index
            s.segment_start = start
            s.task_id = task_state.task_id
            s.sentence_id = task_state.sentence_counter
            task_state.sentence_counter += 1

        await pipeline.push_sentences_to_pipeline(task_state, asr_result)

    async def _concat_segment_mp4s(self, task_state: TaskState) -> Path:
        """
        把 pipeline_scheduler _mixing_worker 产出的所有 segment_xxx.mp4
        用 ffmpeg concat 合并成 final_{task_state.task_id}.mp4
        如果成功再删除这些小片段。
        """
        if not task_state.merged_segments:
            self.logger.warning("无可合并的 segment MP4, 可能任务中断或没有生成混音段.")
            return None

        final_path = task_state.task_paths.output_dir / f"final_{task_state.task_id}.mp4"
        final_path.parent.mkdir(parents=True, exist_ok=True)

        list_txt = final_path.parent / f"concat_{task_state.task_id}.txt"
        with open(list_txt, 'w', encoding='utf-8') as f:
            for seg_mp4 in task_state.merged_segments:
                abs_path = Path(seg_mp4).resolve()
                f.write(f"file '{abs_path}'\n")

        cmd = [
            "ffmpeg", "-y",
            "-f", "concat",
            "-safe", "0",
            "-i", str(list_txt),
            "-c", "copy",
            str(final_path)
        ]
        try:
            self.logger.info(f"开始合并 {len(task_state.merged_segments)} 个MP4 -> {final_path}")
            await self.ffmpeg_tool.run_command(cmd)
            self.logger.info(f"合并完成: {final_path}")

            # 合并成功后，自动删除这些 segment
            for seg_mp4 in task_state.merged_segments:
                try:
                    Path(seg_mp4).unlink(missing_ok=True)
                    self.logger.debug(f"已删除分段文件: {seg_mp4}")
                except Exception as ex:
                    self.logger.warning(f"删除分段文件 {seg_mp4} 失败: {ex}")

            return final_path
        except Exception as e:
            self.logger.error(f"ffmpeg concat 失败: {e}")
            return None
        finally:
            if list_txt.exists():
                list_txt.unlink()

    async def adjust_gpu_allocation(self):
        """根据当前负载重新调整GPU资源分配"""
        try:
            # 重新获取每个模型的适当分配
            asr_gpu = self.gpu_manager.get_allocation("sense_asr")
            tts_gpu = self.gpu_manager.get_allocation("cosyvoice")
            
            # 向Ray报告新的资源需求
            # 注意：这需要模型支持在运行时调整资源，可能需要重新创建Actor
            self.logger.info(f"动态调整GPU分配 - ASR: {asr_gpu}, TTS: {tts_gpu}")
            
            # Ray目前不直接支持在运行时调整Actor资源，
            # 这里只是概念演示，实际使用时可能需要终止并重新创建Actor
            
            return {
                "asr_gpu": asr_gpu,
                "tts_gpu": tts_gpu,
                "total": asr_gpu + tts_gpu
            }
        except Exception as e:
            self.logger.error(f"调整GPU分配失败: {e}")
            return None



================================================================
End of Codebase
================================================================
