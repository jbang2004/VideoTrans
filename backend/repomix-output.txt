This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: core/*.py, services/cosyvoice/*.py, services/cosyvoice/*.proto
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
core/
  __init__.py
  audio_gener.py
  model_in.py
  tts_token_gener.py
services/
  cosyvoice/
    client.py
    service.py

================================================================
Files
================================================================

================
File: core/__init__.py
================
# 空文件即可，标识这是一个 Python 包

================
File: core/audio_gener.py
================
# core/audio_gener.py

import logging
import asyncio
import numpy as np
from services.cosyvoice.client import CosyVoiceClient
from utils import concurrency

class AudioGenerator:
    def __init__(self, cosyvoice_client: CosyVoiceClient, sample_rate: int = 24000):
        self.cosyvoice_client = cosyvoice_client
        self.sample_rate = sample_rate
        self.logger = logging.getLogger(__name__)

    async def vocal_audio_maker(self, sentences):
        tasks = []
        for s in sentences:
            tasks.append(self._generate_single_async(s))

        try:
            results = await asyncio.gather(*tasks)
            return results
        except Exception as e:
            self.logger.error(f"音频生成失败: {str(e)}")
            raise

    async def _generate_single_async(self, sentence):
        try:
            final_audio = await concurrency.run_sync(self._generate_audio_single, sentence)
            sentence.generated_audio = final_audio
            return sentence
        except Exception as e:
            self.logger.error(
                f"音频生成失败 (UUID: {sentence.model_input.get('uuid', 'unknown')}): {str(e)}"
            )
            sentence.generated_audio = None
            return sentence

    def _generate_audio_single(self, sentence):
        """
        直接使用 'tts_tokens_features' 来 token2wav
        """
        if 'tts_tokens_features' not in sentence.model_input:
            self.logger.warning("没有 tts_tokens_features，无法合成音频")
            return np.zeros(0, dtype=np.float32)

        model_input = sentence.model_input['tts_tokens_features']
        speed = getattr(sentence, 'speed', 1.0) or 1.0

        audio_np, dur_sec = self.cosyvoice_client.token2wav(model_input, speed=speed)

        # 首段静音
        if getattr(sentence, 'is_first', False) and getattr(sentence, 'start', 0) > 0:
            silence_samples = int(sentence.start * self.sample_rate / 1000)
            audio_np = np.concatenate([
                np.zeros(silence_samples, dtype=np.float32),
                audio_np
            ])

        # 尾部留白
        if hasattr(sentence, 'silence_duration') and sentence.silence_duration > 0:
            silence_samples = int(sentence.silence_duration * self.sample_rate / 1000)
            audio_np = np.concatenate([
                audio_np,
                np.zeros(silence_samples, dtype=np.float32)
            ])

        self.logger.debug(
            f"音频生成完成 (UUID: {sentence.model_input.get('uuid', 'unknown')}), "
            f"长度={len(audio_np)/self.sample_rate:.2f}s"
        )
        return audio_np

================
File: core/model_in.py
================
import logging
import asyncio
import torch
import numpy as np
import librosa
from typing import List
import os
import uuid
import soundfile as sf

from services.cosyvoice.client import CosyVoiceClient

class ModelIn:
    def __init__(self, cosyvoice_client: CosyVoiceClient, max_concurrent_tasks: int = 4):
        self.cosyvoice_client = cosyvoice_client
        self.logger = logging.getLogger(__name__)
        self.semaphore = asyncio.Semaphore(max_concurrent_tasks)
        # 缓存 speaker 特征提取任务，确保同一 speaker 只处理一次
        self.speaker_cache = {}  # speaker_id -> asyncio.Task
        self.max_val = 0.8
        self.cosy_sample_rate = 24000

    def postprocess(self, speech, top_db=60, hop_length=220, win_length=440):
        """
        对音频进行 trim、幅度归一化，并在结尾加 0.2s 静音
        """
        speech, _ = librosa.effects.trim(
            speech,
            top_db=top_db,
            frame_length=win_length,
            hop_length=hop_length
        )
        if np.abs(speech).max() > self.max_val:
            speech = speech / np.abs(speech).max() * self.max_val

        # 结尾加 0.2s 静音
        pad_samples = int(self.cosy_sample_rate * 0.2)
        speech = np.concatenate([speech, np.zeros(pad_samples, dtype=speech.dtype)])
        return speech

    async def get_speaker_features(self, speaker_id, sentence):
        """
        异步提取 speaker 特征，并将任务缓存，
        确保同一 speaker 只处理一次。
        """
        if speaker_id in self.speaker_cache:
            return await self.speaker_cache[speaker_id]

        async def extract_features():
            # 获取音频数据
            audio_np = (
                sentence.audio.squeeze(0).cpu().numpy()
                if isinstance(sentence.audio, torch.Tensor)
                else sentence.audio
            )
            postprocessed_audio = self.postprocess(audio_np)

            # 保存音频到本地（阻塞操作放入线程中执行）
            # save_dir = "debug_audio"
            # os.makedirs(save_dir, exist_ok=True)
            # file_id = str(uuid.uuid4())[:8]
            # audio_path = os.path.join(save_dir, f"{file_id}_speaker{speaker_id}.wav")
            # await asyncio.to_thread(sf.write, audio_path, processed, 16000)

            postprocessed_tensor = torch.from_numpy(postprocessed_audio).unsqueeze(0)
            # 提取 speaker 特征（阻塞操作放入线程中执行）
            speaker_feats = await asyncio.to_thread(self.cosyvoice_client.extract_speaker_features, postprocessed_tensor)
            return speaker_feats

        task = asyncio.create_task(extract_features())
        self.speaker_cache[speaker_id] = task
        return await task

    async def _process_one_sentence_async(self, sentence, reuse_speaker, reuse_uuid):
        async with self.semaphore:
            speaker_id = getattr(sentence, 'speaker_id', None)
            # speaker 特征提取
            if speaker_id is not None and not reuse_speaker:
                speaker_feats = await self.get_speaker_features(speaker_id, sentence)
                sentence.model_input['speaker_features'] = speaker_feats
            # 若 reuse_speaker 为 True 或 speaker_id 为 None，则跳过提取

            # uuid 处理
            if not reuse_uuid:
                sentence.model_input['uuid'] = ""

            # 文本特征提取
            if 'text_features' not in sentence.model_input or reuse_uuid:
                text_feats = await asyncio.to_thread(self.cosyvoice_client.normalize_text, sentence.trans_text or "")
                sentence.model_input['text_features'] = text_feats

            return sentence

    async def modelin_maker(self, sentences: List, reuse_speaker=False, reuse_uuid=False, batch_size=3):
        """
        对一批 sentence 进行文本与 speaker 特征提取，
        并按 batch_size 分批 yield 结果。
        """
        if not sentences:
            self.logger.warning("modelin_maker: 收到空句子列表")
            return

        tasks = [
            asyncio.create_task(self._process_one_sentence_async(s, reuse_speaker, reuse_uuid))
            for s in sentences
        ]

        results_batch = []
        try:
            for i, task in enumerate(tasks, start=1):
                updated = await task
                if updated is not None:
                    results_batch.append(updated)
                if i % batch_size == 0:
                    yield results_batch
                    results_batch = []
            if results_batch:
                yield results_batch
        except Exception as e:
            self.logger.error(f"modelin_maker处理失败: {e}")
            raise
        finally:
            if not reuse_speaker:
                self.speaker_cache.clear()

================
File: core/tts_token_gener.py
================
# core/tts_token_gener.py

import logging
import asyncio
import uuid
from services.cosyvoice.client import CosyVoiceClient
from utils import concurrency

class TTSTokenGenerator:
    def __init__(self, cosyvoice_client: CosyVoiceClient):
        self.cosyvoice_client = cosyvoice_client
        self.logger = logging.getLogger(__name__)

    async def tts_token_maker(self, sentences, reuse_uuid=False):
        """
        并发生成TTS tokens, 直接存到 sentence.model_input['tts_tokens_features']
        """
        if not sentences:
            return []

        tasks = []
        for s in sentences:
            if reuse_uuid and 'uuid' in s.model_input and s.model_input['uuid']:
                u = s.model_input['uuid']
            else:
                u = str(uuid.uuid1())
                s.model_input['uuid'] = u

            tasks.append(asyncio.create_task(self._generate_tts_single_async(s, u)))

        processed = await asyncio.gather(*tasks)
        return processed

    async def _generate_tts_single_async(self, sentence, main_uuid):
        return await concurrency.run_sync(
            self._generate_tts_single, sentence, main_uuid
        )

    def _generate_tts_single(self, sentence, main_uuid):
        text_f = sentence.model_input.get('text_features')
        speaker_f = sentence.model_input.get('speaker_features')

        merged_f = self._merge_features(text_f, speaker_f)
        out_features, duration_ms = self.cosyvoice_client.generate_tts_tokens(merged_f, main_uuid)
        sentence.duration = duration_ms

        # 直接存储到 'tts_tokens_features'
        sentence.model_input['tts_tokens_features'] = out_features

        self.logger.debug(f"[TTS Token] (UUID={main_uuid}) 生成完毕 => 估计时长 {duration_ms}ms")
        return sentence

    def _merge_features(self, text_f, speaker_f):
        merged = type(text_f)()
        # 拷贝 text_f
        merged.normalized_text_segments.extend(text_f.normalized_text_segments)
        for seg in text_f.text_segments:
            seg_msg = merged.text_segments.add()
            seg_msg.tokens.extend(seg.tokens)

        # 拷贝 speaker_f
        if speaker_f:
            merged.embedding.extend(speaker_f.embedding)
            merged.prompt_speech_feat.extend(speaker_f.prompt_speech_feat)
            merged.prompt_speech_feat_len = speaker_f.prompt_speech_feat_len
            merged.prompt_speech_token.extend(speaker_f.prompt_speech_token)
            merged.prompt_speech_token_len = speaker_f.prompt_speech_token_len

        return merged

================
File: services/cosyvoice/client.py
================
import grpc
import numpy as np
import torch
import logging

from .proto import cosyvoice_pb2
from .proto import cosyvoice_pb2_grpc

logger = logging.getLogger(__name__)

class CosyVoiceClient:
    def __init__(self, address="localhost:50052"):
        self.channel = grpc.insecure_channel(address)
        self.stub = cosyvoice_pb2_grpc.CosyVoiceServiceStub(self.channel)

    def normalize_text(self, text: str):
        """文本标准化"""
        try:
            req = cosyvoice_pb2.NormalizeTextRequest(text=text)
            resp = self.stub.NormalizeText(req)
            return resp.features
        except Exception as e:
            logger.error(f"NormalizeText调用失败: {e}")
            raise

    def extract_speaker_features(self, audio_tensor: torch.Tensor, sr=24000):
        """提取说话人特征"""
        try:
            # 保持原始格式，直接转换为numpy数组并转为二进制
            audio_np = audio_tensor.squeeze(0).cpu().numpy()
            req = cosyvoice_pb2.ExtractSpeakerFeaturesRequest(
                audio=audio_np.tobytes(),
                sample_rate=sr
            )
            resp = self.stub.ExtractSpeakerFeatures(req)
            return resp.features
        except Exception as e:
            logger.error(f"ExtractSpeakerFeatures调用失败: {e}")
            raise

    def generate_tts_tokens(self, features, uuid=""):
        """生成TTS tokens(分段)"""
        try:
            req = cosyvoice_pb2.GenerateTTSTokensRequest(features=features, uuid=uuid)
            resp = self.stub.GenerateTTSTokens(req)
            return resp.features, resp.duration_ms
        except Exception as e:
            logger.error(f"GenerateTTSTokens调用失败: {e}")
            raise

    def token2wav(self, features, speed=1.0):
        """tokens 转音频"""
        try:
            req = cosyvoice_pb2.Token2WavRequest(features=features, speed=speed)
            resp = self.stub.Token2Wav(req)
            audio_np = np.frombuffer(resp.audio, dtype=np.int16).astype(np.float32) / (2**15)
            return audio_np, resp.duration_sec
        except Exception as e:
            logger.error(f"Token2Wav调用失败: {e}")
            raise

================
File: services/cosyvoice/service.py
================
# services/cosyvoice/service.py

import logging
import numpy as np
import torch
import grpc
from concurrent import futures
import threading  # 导入 threading 模块

from .proto import cosyvoice_pb2
from .proto import cosyvoice_pb2_grpc

# 假设我们使用 CosyVoice2 作为模型
from models.CosyVoice.cosyvoice.cli.cosyvoice import CosyVoice2

logger = logging.getLogger(__name__)

class CosyVoiceServiceServicer(cosyvoice_pb2_grpc.CosyVoiceServiceServicer):
    def __init__(self, model_path="models/CosyVoice/pretrained_models/CosyVoice2-0.5B"):
        try:
            self.cosyvoice = CosyVoice2(model_path)
            self.frontend = self.cosyvoice.frontend
            self.model = self.cosyvoice.model
            self.sample_rate = self.cosyvoice.sample_rate
            self.lock = threading.Lock()  # 创建锁
            logger.info('CosyVoice服务初始化成功')
        except Exception as e:
            logger.error(f'CosyVoice服务初始化失败: {e}')
            raise

    # ========== 1) NormalizeText ==========
    def NormalizeText(self, request, context):
        """
        执行文本标准化
        """
        try:
            text = request.text or ""
            normalized_texts = self.frontend.text_normalize(text, split=True, text_frontend=True)

            text_features_msg = cosyvoice_pb2.Features()
            for seg in normalized_texts:
                text_features_msg.normalized_text_segments.append(seg)
                # 用模型的分词器抽取 tokens
                tokens, token_len = self.frontend._extract_text_token(seg)
                # 这里每段 tokens 存到一个 TextSegment
                text_seg_msg = text_features_msg.text_segments.add()
                text_seg_msg.tokens.extend(tokens.reshape(-1).tolist())

            return cosyvoice_pb2.NormalizeTextResponse(features=text_features_msg)
        except Exception as e:
            logger.error(f"NormalizeText失败: {e}")
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(str(e))
            return cosyvoice_pb2.NormalizeTextResponse()

    # ========== 2) ExtractSpeakerFeatures ==========
    def ExtractSpeakerFeatures(self, request, context):
        """
        接收音频并提取说话人相关的特征 (embedding、prompt_speech_feat、prompt_speech_token 等)
        """
        try:
            # 创建一个可写的数组副本
            audio_np = np.frombuffer(request.audio, dtype=np.float32).copy()
            audio = torch.from_numpy(audio_np).unsqueeze(0)

            # 示例: zero-shot / cross-lingual 提取
            result = self.frontend.frontend_cross_lingual("", audio, self.sample_rate)

            speaker_features_msg = cosyvoice_pb2.Features(
                embedding=result['llm_embedding'].reshape(-1).tolist(),
                prompt_speech_feat=result['prompt_speech_feat'].reshape(-1).tolist(),
                prompt_speech_feat_len=int(result['prompt_speech_feat_len'].item()),
                prompt_speech_token=result['flow_prompt_speech_token'].reshape(-1).tolist(),
                prompt_speech_token_len=int(result['flow_prompt_speech_token_len'].item())
            )
            return cosyvoice_pb2.ExtractSpeakerFeaturesResponse(features=speaker_features_msg)
        except Exception as e:
            logger.error(f"ExtractSpeakerFeatures失败: {e}")
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(str(e))
            return cosyvoice_pb2.ExtractSpeakerFeaturesResponse()

    # ========== 3) GenerateTTSTokens (分段) ==========
    def GenerateTTSTokens(self, request, context):
        """
        将输入文本特征转换为TTS tokens，并估算时长
        """
        try:
            features_in = request.features
            base_uuid = request.uuid or "no_uuid"
            
            # 1. 准备输出特征，复制输入的speaker相关特征
            tts_out_features = cosyvoice_pb2.Features()
            tts_out_features.embedding.extend(features_in.embedding)
            tts_out_features.prompt_speech_feat.extend(features_in.prompt_speech_feat)
            tts_out_features.prompt_speech_feat_len = features_in.prompt_speech_feat_len
            tts_out_features.prompt_speech_token.extend(features_in.prompt_speech_token)
            tts_out_features.prompt_speech_token_len = features_in.prompt_speech_token_len

            # 2. 准备模型输入
            embedding_tensor = torch.tensor(features_in.embedding, dtype=torch.float32).unsqueeze(0)
            prompt_token_tensor = torch.tensor(features_in.prompt_speech_token, dtype=torch.int32).unsqueeze(0)

            total_duration_ms = 0
            # 3. 处理每段文本
            for i, text_seg in enumerate(features_in.text_segments):
                if not text_seg.tokens:
                    continue

                seg_uuid = f"{base_uuid}_seg_{i}"
                text_tensor = torch.tensor(text_seg.tokens, dtype=torch.int32).unsqueeze(0)

                # 初始化LLM状态
                with self.lock:
                    self.model.tts_speech_token_dict[seg_uuid] = []
                    self.model.llm_end_dict[seg_uuid] = False

                try:
                    # 生成TTS tokens
                    self.model.llm_job(
                        text=text_tensor,
                        prompt_text=torch.zeros((1, 0), dtype=torch.int32),
                        llm_prompt_speech_token=prompt_token_tensor,
                        llm_embedding=embedding_tensor,
                        uuid=seg_uuid
                    )

                    # 获取生成的tokens并添加到输出
                    tts_tokens = self.model.tts_speech_token_dict[seg_uuid]
                    
                    tts_seg_msg = tts_out_features.tts_segments.add()
                    tts_seg_msg.uuid = seg_uuid
                    tts_seg_msg.tokens.extend(tts_tokens)  # 直接使用list

                    # 估算时长（每25个token约1秒）
                    total_duration_ms += len(tts_tokens) / 25.0 * 1000

                finally:
                    # 清理LLM状态
                    with self.lock:
                        self.model.tts_speech_token_dict.pop(seg_uuid, None)
                        self.model.llm_end_dict.pop(seg_uuid, None)

            return cosyvoice_pb2.GenerateTTSTokensResponse(
                features=tts_out_features,
                duration_ms=int(total_duration_ms)
            )

        except Exception as e:
            logger.error(f"GenerateTTSTokens失败: {e}", exc_info=True)
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(str(e))
            return cosyvoice_pb2.GenerateTTSTokensResponse()

    # ========== 4) Token2Wav (把分段 tokens 拼起来一次合成) ==========
    def Token2Wav(self, request, context):
        """
        将 GenerateTTSTokens 里返回的多段 tokens (features.tts_segments) 分段调用 token2wav，
        使用每个seg中的uuid（如果存在）作为token2wav的uuid，然后将各段生成的音频合并。
        """
        try:
            features = request.features
            speed = request.speed

            # 转换 embedding
            if len(features.embedding) > 0:
                embedding_tensor = torch.tensor(features.embedding, dtype=torch.float32).unsqueeze(0)
            else:
                embedding_tensor = torch.zeros((1, 0), dtype=torch.float32)
                logger.warning("Token2Wav: embedding 为空, 使用零向量代替.")

            # 转换 prompt_speech_feat
            if len(features.prompt_speech_feat) > 0:
                feat_tensor = torch.tensor(features.prompt_speech_feat, dtype=torch.float32).reshape(1, -1, 80)
            else:
                feat_tensor = torch.zeros((1, 0, 80), dtype=torch.float32)

            prompt_token_tensor = torch.tensor(features.prompt_speech_token, dtype=torch.int32).unsqueeze(0)

            audio_pieces = []
            total_duration_sec = 0.0

            # 针对每个tts segment单独生成音频
            for seg in features.tts_segments:
                tokens = seg.tokens
                if not tokens:
                    continue

                # 使用 seg 中的 uuid，如果为空则采用默认值
                seg_uuid = seg.uuid if seg.uuid else "token2wav_uuid"

                # 直接将tokens转为tensor
                token_tensor = torch.tensor(tokens, dtype=torch.int64).unsqueeze(0)

                # 设置缓存，保证线程安全
                with self.model.lock:
                    self.model.hift_cache_dict[seg_uuid] = None
                try:
                    seg_audio_out = self.model.token2wav(
                        token=token_tensor,
                        prompt_token=prompt_token_tensor,
                        prompt_feat=feat_tensor,
                        embedding=embedding_tensor,
                        uuid=seg_uuid,
                        token_offset=0,
                        finalize=True,
                        speed=speed
                    )
                except Exception as e:
                    error_msg = f"Token2Wav合成失败 (UUID: {seg_uuid}): {str(e)}"
                    logger.error(error_msg, exc_info=True)
                    context.set_code(grpc.StatusCode.INTERNAL)
                    context.set_details(error_msg)
                    return cosyvoice_pb2.Token2WavResponse()
                finally:
                    with self.model.lock:
                        self.model.hift_cache_dict.pop(seg_uuid, None)

                # 处理生成的音频：转换为 numpy 数组、去除无用维度，如果多通道取均值
                seg_audio = seg_audio_out.cpu().numpy().squeeze()
                if seg_audio.ndim > 1:
                    seg_audio = seg_audio.mean(axis=0)
                audio_pieces.append(seg_audio)
                total_duration_sec += len(seg_audio) / self.sample_rate

            if not audio_pieces:
                logger.warning("Token2Wav: 未生成任何音频，可能缺少有效 tokens")
                return cosyvoice_pb2.Token2WavResponse()

            # 合并所有段音频
            final_audio = np.concatenate(audio_pieces)
            audio_int16 = (final_audio * (2**15)).astype(np.int16).tobytes()

            return cosyvoice_pb2.Token2WavResponse(
                audio=audio_int16,
                duration_sec=total_duration_sec
            )
        except Exception as e:
            logger.error(f"Token2Wav失败: {e}")
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(str(e))
            return cosyvoice_pb2.Token2WavResponse()


def serve(args):
    host = getattr(args, 'host', '0.0.0.0')
    port = getattr(args, 'port', 50052)
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
    cosyvoice_pb2_grpc.add_CosyVoiceServiceServicer_to_server(
        CosyVoiceServiceServicer(args.model_dir), server
    )
    address = f'{host}:{port}'
    server.add_insecure_port(address)
    server.start()
    logger.info(f'CosyVoice服务已启动: {address}')
    server.wait_for_termination()



================================================================
End of Codebase
================================================================
