This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-01-25T23:18:47.618Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repomix, visit: https://github.com/yamadashy/repomix

================================================================
Repository Structure
================================================================
core/
  timeadjust/
    duration_aligner.py
    timestamp_adjuster.py
  translation/
    __init__.py
    deepseek_client.py
    gemini_client.py
    glm4_client.py
    prompt.py
    translator.py
  __init__.py
  audio_gener.py
  audio_separator.py
  auto_sense.py
  hls_manager.py
  media_mixer.py
  model_in.py
  sentence_tools.py
  tts_token_gener.py
utils/
  decorators.py
  media_utils.py
  sentence_logger.py
  task_state.py
  task_storage.py
  temp_file_manager.py
__init__.py
.cursorignore
.env.example
api.py
config.py
pipeline_scheduler.py
postcss.config.json
requirements.txt
video_translator.py

================================================================
Repository Files
================================================================

================
File: core/timeadjust/duration_aligner.py
================
# ===================== duration_aligner.py =====================
import logging

class DurationAligner:
    def __init__(self, model_in=None, simplifier=None, tts_token_gener=None, max_speed=1.1):
        self.model_in = model_in
        self.simplifier = simplifier
        self.tts_token_gener = tts_token_gener
        self.max_speed = max_speed
        self.logger = logging.getLogger(__name__)

    async def align_durations(self, sentences):
        """对整批句子进行时长对齐，并检查是否需要精简（语速过快）"""
        if not sentences:
            return

        # 第一次对齐
        self._align_batch(sentences)

        # 查找"语速过快"的句子（speed > max_speed）
        retry_sentences = [s for s in sentences if s.speed > self.max_speed]
        if retry_sentences:
            self.logger.info(f"{len(retry_sentences)} 个句子语速过快, 正在精简...")
            success = await self._retry_sentences_batch(retry_sentences)
            if success:
                # 若精简文本成功，再次对齐
                self._align_batch(sentences)
            else:
                self.logger.warning("精简过程失败, 保持原结果")

    def _align_batch(self, sentences):
        """同批次句子做"压缩/扩展"对齐"""
        if not sentences:
            return

        # 1) 计算每句与其 target_duration 的差值 diff
        for s in sentences:
            s.diff = s.duration - s.target_duration

        total_diff_to_adjust = sum(s.diff for s in sentences)
        current_time = sentences[0].start

        # 2) 按照 total_diff_to_adjust 做统一对齐
        for s in sentences:
            s.adjusted_start = current_time
            diff = s.diff

            # --- 统一先重置 speed 和 silence_duration ---
            # 这样无论后面的分支怎么走，都不会遗留旧的值
            s.speed = 1.0
            s.silence_duration = 0.0

            if total_diff_to_adjust == 0:
                # 批次总时长与目标一致，直接保持原时长
                s.adjusted_duration = s.duration
                s.diff = 0
                # 这里 speed/silence 已被上面重置为 1.0 / 0

            elif total_diff_to_adjust > 0:
                # 批次整体"过长"，需要压缩
                positive_diff_sum = sum(x.diff for x in sentences if x.diff > 0)
                if positive_diff_sum > 0 and diff > 0:
                    # 等比例压缩
                    proportion = diff / positive_diff_sum
                    adjustment = total_diff_to_adjust * proportion
                    s.adjusted_duration = s.duration - adjustment
                    s.diff = s.duration - s.adjusted_duration

                    # speed = 压缩前长度 / 压缩后长度
                    if s.adjusted_duration > 0:
                        s.speed = s.duration / s.adjusted_duration
                    else:
                        s.speed = 1.0
                else:
                    # 句子本身 diff <= 0，无需再压，就保留原值
                    s.adjusted_duration = s.duration
                    s.diff = 0
                    # speed = 1.0, silence_duration = 0.0 (已重置)

            else:
                # total_diff_to_adjust < 0 => 整体"过短"，需要扩展
                negative_diff_sum_abs = sum(abs(x.diff) for x in sentences if x.diff < 0)
                if negative_diff_sum_abs > 0 and diff < 0:
                    # 等比例扩展
                    proportion = abs(diff) / negative_diff_sum_abs
                    total_needed = abs(total_diff_to_adjust) * proportion

                    # 限制放慢，避免速度过低
                    max_slowdown = s.duration * 0.1
                    slowdown = min(total_needed, max_slowdown)

                    s.adjusted_duration = s.duration + slowdown
                    if s.adjusted_duration > 0:
                        s.speed = s.duration / s.adjusted_duration
                    else:
                        s.speed = 1.0

                    s.silence_duration = total_needed - slowdown
                    if s.silence_duration > 0:
                        s.adjusted_duration += s.silence_duration
                else:
                    # 句子本身 diff >= 0，无需扩展
                    s.adjusted_duration = s.duration
                    s.speed = 1.0
                    s.silence_duration = 0.0

                # "扩展"分支要及时更新 diff
                s.diff = s.duration - s.adjusted_duration

            current_time += s.adjusted_duration

            self.logger.info(f"对齐后: {s.trans_text}, duration: {s.duration}, target_duration: {s.target_duration}, diff: {s.diff}, speed: {s.speed}, silence_duration: {s.silence_duration}")

    async def _retry_sentences_batch(self, sentences):
        """精简文本 + 再次生成 TTS token"""
        try:
            # 1. 精简文本
            texts_to_simplify = {str(i): s.trans_text for i, s in enumerate(sentences)}
            simplified_texts = await self.simplifier.simplify(texts_to_simplify)

            # 2. 更新句子的文本
            for i, s in enumerate(sentences):
                new_text = simplified_texts.get(str(i))
                if new_text:
                    self.logger.info(f"精简: {s.trans_text} -> {new_text}")
                    s.trans_text = new_text

            # 3. 批量更新文本特征(复用 speaker+uuid)
            async for batch in self.model_in.modelin_maker(
                sentences,
                reuse_speaker=True,
                reuse_uuid=True,
                batch_size=3
            ):
                # 4. 再生成 token (复用 uuid)
                updated_batch = await self.tts_token_gener.tts_token_maker(
                    batch, reuse_uuid=True
                )

            return True

        except Exception as e:
            self.logger.error(f"_retry_sentences_batch 出错: {e}")
            return False

================
File: core/timeadjust/timestamp_adjuster.py
================
import logging
from typing import List, Optional
from dataclasses import dataclass

class TimestampAdjuster:
    """句子时间戳调整器"""
    
    def __init__(self, sample_rate: int):
        self.logger = logging.getLogger(__name__)
        self.sample_rate = sample_rate
        
    def update_timestamps(self, sentences: List, start_time: float = None) -> float:
        """更新句子的时间戳信息
        
        Args:
            sentences: 要更新的句子列表
            start_time: 起始时间（毫秒），如果为 None 则使用第一个句子的开始时间
            
        Returns:
            float: 最后一个句子结束的时间点（毫秒）
        """
        if not sentences:
            return start_time if start_time is not None else 0
            
        # 使用传入的起始时间或第一个句子的开始时间
        current_time = start_time if start_time is not None else sentences[0].start
        
        for i, sentence in enumerate(sentences):
            # 计算实际音频长度（毫秒）
            if sentence.generated_audio is not None:
                actual_duration = (len(sentence.generated_audio) / self.sample_rate) * 1000
            else:
                actual_duration = 0
                self.logger.warning(f"句子 {sentence.sentence_id} 没有生成音频")
            
            # 更新时间戳
            sentence.adjusted_start = current_time
            sentence.adjusted_duration = actual_duration
            
            # 更新差异值
            sentence.diff = sentence.duration - actual_duration
            
            # 更新下一个句子的开始时间
            current_time += actual_duration
            
        return current_time
        
    def validate_timestamps(self, sentences: List) -> bool:
        """验证时间戳的连续性和有效性
        
        Args:
            sentences: 要验证的句子列表
            
        Returns:
            bool: 时间戳是否有效
        """
        if not sentences:
            return True
            
        for i in range(len(sentences) - 1):
            current = sentences[i]
            next_sentence = sentences[i + 1]
            
            # 验证时间连续性
            expected_next_start = current.adjusted_start + current.adjusted_duration
            if abs(next_sentence.adjusted_start - expected_next_start) > 1:  # 允许1毫秒的误差
                self.logger.error(
                    f"时间戳不连续 - 句子 {current.sentence_id} 结束时间: {expected_next_start:.2f}ms, "
                    f"句子 {next_sentence.sentence_id} 开始时间: {next_sentence.adjusted_start:.2f}ms"
                )
                return False
                
            # 验证时长有效性
            if current.adjusted_duration <= 0:
                self.logger.error(f"句子 {current.sentence_id} 的时长无效: {current.adjusted_duration:.2f}ms")
                return False
                
        return True

================
File: core/translation/__init__.py
================
# 空文件，用于标识 translation 目录为一个 Python 包

================
File: core/translation/deepseek_client.py
================
# =========================== deepseek_client.py ===========================
import json
import logging
import re
from openai import OpenAI
from typing import Dict
from json_repair import loads

import asyncio  # [MODIFIED] 用于 asyncio.to_thread

logger = logging.getLogger(__name__)

class DeepSeekClient:
    def __init__(self, api_key: str):
        """初始化 DeepSeek 客户端"""
        if not api_key:
            raise ValueError("DeepSeek API key must be provided")
            
        self.client = OpenAI(
            api_key=api_key,
            base_url="https://api.deepseek.com"
        )
        logger.info("DeepSeek 客户端初始化成功")

    def _extract_output_content(self, text: str) -> str:
        """从响应中提取 <OUTPUT> 标签中的内容"""
        pattern = r"<OUTPUT>(.*?)</OUTPUT>"
        match = re.search(pattern, text, re.DOTALL)
        if match:
            return match.group(1).strip()
        logger.warning("未找到 <OUTPUT> 标签，返回原始内容")
        return text

    async def translate(
        self,
        texts: Dict[str, str],
        system_prompt: str,
        user_prompt: str
    ) -> Dict[str, str]:
        """
        将 sync 调用 self.client.chat.completions.create(...) 包装到线程池中执行，避免阻塞事件循环。
        """
        try:
            # [MODIFIED] 在默认线程池执行同步请求
            response = await asyncio.to_thread(
                self.client.chat.completions.create,
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
            )
            
            # 同步部分结束后，继续解析结果
            result = response.choices[0].message.content
            logger.debug(f"DeepSeek 请求结果: {result}")
            
            # 提取 <OUTPUT> 标签中的内容
            output_content = self._extract_output_content(result)
            logger.debug(f"提取的 OUTPUT 内容: {output_content}")
            
            parsed_result = loads(output_content)
            logger.debug("DeepSeek 请求成功")
            return parsed_result
            
        except Exception as e:
            logger.error(f"DeepSeek 请求失败: {str(e)}")
            if "503" in str(e):
                logger.error("连接错误：无法连接到 DeepSeek API，可能是代理或网络问题")
            raise

================
File: core/translation/gemini_client.py
================
import json
import logging
import google.generativeai as genai
from typing import Dict
import os

from .prompt import TRANSLATION_PROMPT, SYSTEM_PROMPT, LANGUAGE_MAP, EXAMPLE_OUTPUTS

logger = logging.getLogger(__name__)

class GeminiClient:
    def __init__(self, api_key: str):
        """初始化 Gemini 客户端"""
        if not api_key:
            raise ValueError("Gemini API key must be provided")
            
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('gemini-1.5-flash')
        logger.info("Gemini 客户端初始化成功")

    async def translate(self, texts: Dict[str, str], target_language: str = "zh") -> str:
        """调用 Gemini 模型进行翻译，返回 JSON 字符串"""
        try:
            if target_language not in LANGUAGE_MAP:
                raise ValueError(f"不支持的目标语言: {target_language}")
                
            prompt = TRANSLATION_PROMPT.format(
                json_content=json.dumps(texts, ensure_ascii=False, indent=2),
                target_language=LANGUAGE_MAP[target_language],
                example_output=EXAMPLE_OUTPUTS[target_language]
            )
            
            response = self.model.generate_content(
                [SYSTEM_PROMPT.format(target_language=LANGUAGE_MAP[target_language]), prompt],
                generation_config=genai.types.GenerationConfig(temperature=0.3)
            )
            logger.debug(f"Gemini 翻译请求成功, 目标语言: {LANGUAGE_MAP[target_language]}")
            return response.text
        except Exception as e:
            logger.error(f"Gemini 翻译请求失败: {str(e)}")
            if "503" in str(e):
                logger.error("连接错误：无法连接到 Gemini API，可能是代理或网络问题")
            raise

================
File: core/translation/glm4_client.py
================
import json
import logging
from zhipuai import ZhipuAI
from .prompt import GLM4_TRANSLATION_PROMPT, GLM4_SYSTEM_PROMPT

logger = logging.getLogger(__name__)

class GLM4Client:
    def __init__(self, api_key: str):
        """初始化 GLM-4 客户端"""
        if not api_key:
            raise ValueError("API key must be provided")
        self.client = ZhipuAI(api_key=api_key)
        logger.info("GLM-4 客户端初始化成功")

    async def translate(self, texts: dict) -> str:
        """调用 GLM-4 模型进行翻译，返回 JSON 字符串"""
        prompt = GLM4_TRANSLATION_PROMPT.format(json_content=json.dumps(texts, ensure_ascii=False, indent=2))
        try:
            logger.debug(f"需要翻译的JSON: {json.dumps(texts, ensure_ascii=False, indent=2)}")
            response = self.client.chat.completions.create(
                model="glm-4-flash",
                messages=[
                    {"role": "system", "content": GLM4_SYSTEM_PROMPT},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                top_p=0.8
            )
            content = response.choices[0].message.content
            logger.debug(f"翻译结果: {content}")
            return content
        except Exception as e:
            logger.error(f"GLM-4 翻译请求失败: {str(e)}")
            raise

================
File: core/translation/prompt.py
================
# 支持的语言映射
LANGUAGE_MAP = {
    "zh": "Chinese",
    "en": "English",
    "ja": "Japanese",
    "ko": "Korean"
}

TRANSLATION_USER_PROMPT = """
**角色设定：**
- **经历：** 游学四方、博览群书、翻译官、外交官
- **性格：**严谨、好奇、坦率、睿智、求真、惜墨如金
- **技能：**精通{target_language}、博古通今、斟字酌句、精确传达
- **表达方式：** 精炼、简洁、最优化、避免冗余
**执行规则：**
1.无论何时，只翻译JSON格式中的value，*严格保持原 JSON 结构与各层级字段key数量完全一致*
2.在翻译前，请数一数 JSON 中所有的key总数，并在 <Thinking></Thinking> 中说明「共计 N 个 key，切勿增删」，以提醒自己保持 JSON 的 key 数不变。
2.value中出现的数字，翻译成*{target_language}数字*，而非阿拉伯数字。
3.对提供的原文内容深思熟虑，总结上下文，将你的总结和思考放入<Thinking></Thinking>中。
4.确保译文精炼、简洁，与原文意思保持一致。
5.把实际的输出JSON译文放在<OUTPUT></OUTPUT>中。

以下是JSON格式原文：
{json_content}
"""

TRANSLATION_SYSTEM_PROMPT = """你将扮演久经历练的翻译官，致力于将提供的JSON格式原文，翻译成地道的{target_language}，打破语言界限，促进两国交流。"""

SIMPLIFICATION_USER_PROMPT = """
**角色设定：**
- **性格：**严谨克制、精确表达、追求简约
- **技能：**咬文嚼字、斟字酌句、去芜存菁
- **表达方式：** 精炼、清晰、最优化、避免冗余
**执行规则：**
1.无论何时，只精简JSON格式中的value，*保持key不变，不要进行合并*。
2.value中出现的数字，保留当前语言的数字形式，而非阿拉伯数字。
3.首先对value内容进行深度分析，进行3种不同层次的精简：
-轻微精简：去除重复和冗余词汇，保持原意不变，放入<Slight JSON></Slight JSON>标签中。
-中度精简：进一步简化句子结构，去除不必要的修饰词，放入<Moderate JSON>></Moderate JSON>标签中。
-极度精简：仅保留核心信息，去除所有修饰和冗余，放入<Extreme JSON>></Extreme JSON>标签中。
4.选择轻微精简，并修正表达，把实际的输出JSON放在<OUTPUT></OUTPUT>中。
以下是JSON格式原文：
{json_content}
"""

SIMPLIFICATION_SYSTEM_PROMPT = """你将扮演克制严谨的语言专家，致力于将提供的JSON格式原文进行恰当的精简。"""

================
File: core/translation/translator.py
================
import asyncio
import logging
from typing import Dict, List, AsyncGenerator, Protocol
from json_repair import loads
from .prompt import (
    TRANSLATION_SYSTEM_PROMPT,
    TRANSLATION_USER_PROMPT,
    SIMPLIFICATION_SYSTEM_PROMPT,
    SIMPLIFICATION_USER_PROMPT,
    LANGUAGE_MAP
)

logger = logging.getLogger(__name__)

class TranslationClient(Protocol):
    async def translate(
        self,
        texts: Dict[str, str],
        system_prompt: str,
        user_prompt: str
    ) -> Dict[str, str]:
        ...

class Translator:
    def __init__(self, translation_client: TranslationClient):
        self.translation_client = translation_client
        self.logger = logging.getLogger(__name__)

    async def translate(self, texts: Dict[str, str], target_language: str = "zh") -> Dict[str, str]:
        """执行翻译并处理结果"""
        try:
            system_prompt = TRANSLATION_SYSTEM_PROMPT.format(
                target_language=LANGUAGE_MAP.get(target_language, target_language)
            )
            user_prompt = TRANSLATION_USER_PROMPT.format(
                target_language=LANGUAGE_MAP.get(target_language, target_language),
                json_content=texts
            )
            result = await self.translation_client.translate(
                texts=texts,
                system_prompt=system_prompt,
                user_prompt=user_prompt
            )
            return result
        except Exception as e:
            self.logger.error(f"翻译失败: {str(e)}")
            raise

    async def simplify(self, texts: Dict[str, str], batch_size: int = 4) -> Dict[str, str]:
        """执行文本简化并处理结果，支持批量处理和错误恢复"""
        if not texts:
            return {}

        result = {}
        keys = list(texts.keys())
        i = 0
        success_count = 0
        current_batch_size = batch_size

        while i < len(keys):
            try:
                batch_keys = keys[i:i+current_batch_size]
                batch_texts = {k: texts[k] for k in batch_keys}
                
                self.logger.debug(f"简化批次: {len(batch_texts)}条文本, 大小: {current_batch_size}, 位置: {i}")

                system_prompt = SIMPLIFICATION_SYSTEM_PROMPT
                user_prompt = SIMPLIFICATION_USER_PROMPT.format(json_content=batch_texts)
                
                batch_result = await self.translation_client.translate(
                    texts=batch_texts,
                    system_prompt=system_prompt,
                    user_prompt=user_prompt
                )
                
                if len(batch_result) == len(batch_texts):
                    success_count += 1
                    result.update(batch_result)
                    i += len(batch_texts)
                    self.logger.debug(f"简化成功: {len(batch_texts)}条文本, 连续成功: {success_count}次")
                    
                    if current_batch_size < batch_size and success_count >= 2:
                        self.logger.debug(f"连续成功{success_count}次，恢复到初始批次大小: {batch_size}")
                        current_batch_size = batch_size
                        success_count = 0
                else:
                    raise ValueError(f"简化结果不完整 (输入: {len(batch_texts)}, 内容: {batch_texts}, 输出: {len(batch_result)}, 内容: {batch_result})")

                if i < len(keys):
                    await asyncio.sleep(0.1)

            except Exception as e:
                self.logger.error(f"简化失败: {str(e)}")
                if current_batch_size > 1:
                    current_batch_size = max(current_batch_size // 2, 1)
                    success_count = 0
                    self.logger.debug(f"出错后减小批次大小到: {current_batch_size}")
                    continue
                else:
                    for k in batch_keys:
                        result[k] = texts[k]
                    i += len(batch_keys)

        return result

    async def translate_sentences(
        self,
        sentences: List,
        batch_size: int = 100,
        target_language: str = "zh"
    ) -> AsyncGenerator[List, None]:
        """批量翻译处理"""
        if not sentences:
            self.logger.warning("收到空的句子列表")
            return

        i = 0
        initial_size = batch_size
        success_count = 0
        required_successes = 2

        while i < len(sentences):
            if batch_size < initial_size and success_count >= required_successes:
                self.logger.debug(f"连续成功{success_count}次，恢复到初始批次大小: {initial_size}")
                batch_size = initial_size
                success_count = 0

            success = False
            pos = i

            while not success and batch_size >= 1:
                try:
                    batch = sentences[pos:pos+batch_size]
                    if not batch:
                        break

                    texts = {str(j): s.raw_text for j, s in enumerate(batch)}
                    self.logger.debug(f"翻译批次: {len(texts)}条文本, 大小: {batch_size}, 位置: {pos}")

                    translated = await self.translate(texts, target_language)
                    
                    if len(translated) == len(texts):
                        success = True
                        success_count += 1
                        self.logger.info(f"翻译成功: {len(batch)}条文本, 连续成功: {success_count}次")

                        self.logger.debug(f"原文: {texts}，翻译结果: {translated}")
                        results = []
                        for j, sentence in enumerate(batch):
                            sentence.trans_text = translated[str(j)]
                            results.append(sentence)

                        yield results
                        i += len(batch)
                    else:
                        batch_size = max(batch_size // 2, 1)
                        success_count = 0
                        self.logger.warning(f"翻译不完整 (输入: {len(texts)}, 输出: {len(translated)}), 减小到: {batch_size}")
                        self.logger.debug(f"原文: {texts}，翻译结果: {translated}")
                        continue

                    if i < len(sentences):
                        await asyncio.sleep(0.1)

                except Exception as e:
                    self.logger.error(f"翻译失败: {str(e)}")
                    if batch_size > 1:
                        batch_size = max(batch_size // 2, 1)
                        success_count = 0
                        self.logger.debug(f"出错后减小批次大小到: {batch_size}")
                        continue
                    else:
                        results = []
                        for sentence in batch:
                            sentence.trans_text = sentence.raw_text
                            results.append(sentence)
                        yield results
                        i += 1

================
File: core/__init__.py
================
# 空文件即可，标识这是一个 Python 包

================
File: core/audio_gener.py
================
import logging
import asyncio
from concurrent.futures import ThreadPoolExecutor
import torch
import numpy as np
import os
import inspect

class AudioGenerator:
    def __init__(self, cosyvoice_model, sample_rate: int = None, max_workers=None):
        """
        Args:
            cosyvoice_model: CosyVoice模型
            sample_rate: 采样率，如果为None则使用cosyvoice_model的采样率
            max_workers: 并行处理的最大工作线程数，默认为None（将根据CPU核心数自动设置）
        """
        self.cosyvoice_model = cosyvoice_model.model
        self.sample_rate = sample_rate or cosyvoice_model.sample_rate
        cpu_count = os.cpu_count()
        self.max_workers = min(max_workers or cpu_count, cpu_count)
        self.executor = None
        self.logger = logging.getLogger(__name__)

    async def vocal_audio_maker(self, batch_sentences):
        """异步批量生成音频"""
        try:
            if self.executor is None:
                self.executor = ThreadPoolExecutor(max_workers=self.max_workers)
                self.logger.debug(f"创建音频生成线程池，max_workers={self.max_workers}")
            
            tasks = [
                self._generate_single_async(sentence)
                for sentence in batch_sentences
            ]
            await asyncio.gather(*tasks)
            
        except Exception as e:
            self.logger.error(f"音频生成失败: {str(e)}")
            raise

    async def _generate_single_async(self, sentence):
        """异步生成单个音频"""
        loop = asyncio.get_event_loop()
        try:
            audio_np = await loop.run_in_executor(
                self.executor, 
                self._generate_audio_single, 
                sentence
            )
            sentence.generated_audio = audio_np
            
        except Exception as e:
            self.logger.error(f"音频生成失败 (UUID: {sentence.model_input.get('uuid', 'unknown')}): {str(e)}")
            sentence.generated_audio = None

    def _generate_audio_single(self, sentence):
        """生成单个音频或静音，并拼接必要的前后静音。"""

        model_input = sentence.model_input
        self.logger.debug(f"开始生成音频 (主UUID: {model_input.get('uuid', 'unknown')})")

        try:
            segment_audio_list = []

            # 如果 tokens/uuids 为空，我们不直接 return，而是给出一个空数组让后续逻辑继续执行
            tokens_list = model_input.get('segment_speech_tokens', [])
            uuids_list = model_input.get('segment_uuids', [])

            if not tokens_list or not uuids_list:
                # 在这里仅记录日志，并在 segment_audio_list 放一个零长度数组
                self.logger.debug(f"空的语音标记, 仅生成空波形，后续仍可添加静音 (UUID: {model_input.get('uuid', 'unknown')})")
                segment_audio_list.append(np.zeros(0, dtype=np.float32))
            else:
                # 否则逐段生成音频
                for i, (tokens, segment_uuid) in enumerate(zip(tokens_list, uuids_list)):
                    if not tokens:
                        # 如果某个段 token 为空，也放一个零长度数组占位
                        segment_audio_list.append(np.zeros(0, dtype=np.float32))
                        continue

                    token2wav_kwargs = {
                        'token': torch.tensor(tokens).unsqueeze(dim=0),
                        'token_offset': 0,
                        'finalize': True,
                        'prompt_token': model_input.get('flow_prompt_speech_token', torch.zeros(1, 0, dtype=torch.int32)),
                        'prompt_feat': model_input.get('prompt_speech_feat', torch.zeros(1, 0, 80)),
                        'embedding': model_input.get('flow_embedding', torch.zeros(0)),
                        'uuid': segment_uuid,
                        'speed': sentence.speed if sentence.speed else 1.0
                    }

                    segment_output = self.cosyvoice_model.token2wav(**token2wav_kwargs)
                    segment_audio = segment_output.cpu().numpy()

                    # 如果是多通道，转单通道
                    if segment_audio.ndim > 1:
                        segment_audio = segment_audio.mean(axis=0)
                    
                    segment_audio_list.append(segment_audio)
                    self.logger.debug(
                        f"段落 {i+1}/{len(uuids_list)} 生成完成，"
                        f"时长: {len(segment_audio)/self.sample_rate:.2f}秒"
                    )

            # 拼接所有段落
            if segment_audio_list:
                final_audio = np.concatenate(segment_audio_list)
            else:
                # 理论上不会出现，因为最少有一个空数组
                final_audio = np.zeros(0, dtype=np.float32)

            # ----- 添加首句静音（若是本分段第一句且 start>0） -----
            if sentence.is_first and sentence.start > 0:
                silence_samples = int(sentence.start * self.sample_rate / 1000)
                final_audio = np.concatenate([np.zeros(silence_samples, dtype=np.float32), final_audio])

            # ----- 添加尾部静音 -----
            if sentence.silence_duration > 0:
                silence_samples = int(sentence.silence_duration * self.sample_rate / 1000)
                final_audio = np.concatenate([final_audio, np.zeros(silence_samples, dtype=np.float32)])

            self.logger.debug(
                f"音频生成完成 (UUID: {model_input.get('uuid', 'unknown')}, "
                f"段落数: {len(segment_audio_list)}, 最终长度: {len(final_audio)/self.sample_rate:.2f}秒)"
            )

            return final_audio

        except Exception as e:
            self.logger.error(f"音频生成失败 (UUID: {model_input.get('uuid', 'unknown')}): {str(e)}")
            raise

================
File: core/audio_separator.py
================
from abc import ABC, abstractmethod
from typing import Tuple
import numpy as np

from models.ClearerVoice.clearvoice import ClearVoice

class AudioSeparator(ABC):
    """音频分离器接口"""
    @abstractmethod
    def separate_audio(self, input_path: str, **kwargs) -> Tuple[np.ndarray, np.ndarray]:
        pass

class ClearVoiceSeparator(AudioSeparator):
    """使用 ClearVoice 实现的音频分离器"""
    def __init__(self, model_name: str = 'MossFormer2_SE_48K'):
        self.model_name = model_name
        self.clearvoice = ClearVoice(
            task='speech_enhancement',
            model_names=[model_name]
        )
    
    def separate_audio(self, input_path: str) -> Tuple[np.ndarray, np.ndarray, int]:
        enhanced_audio, background_audio = self.clearvoice(
            input_path=input_path,
            online_write=False,
            extract_noise=True
        )
        
        if self.model_name.endswith('16K'):
            sr = 16000
        elif self.model_name.endswith('48K'):
            sr = 48000
        else:
            sr = 48000
        
        return enhanced_audio, background_audio, sr

================
File: core/auto_sense.py
================
import logging
import os
import importlib.util
import sys
import torch
import numpy as np
from tqdm import tqdm
from pathlib import Path
import time
from funasr.register import tables
from funasr.auto.auto_model import AutoModel as BaseAutoModel
from funasr.auto.auto_model import prepare_data_iterator
from funasr.utils.misc import deep_update
from funasr.models.campplus.utils import sv_chunk, postprocess
from funasr.models.campplus.cluster_backend import ClusterBackend
from .sentence_tools import get_sentences
from funasr.utils.vad_utils import slice_padding_audio_samples
from funasr.utils.load_utils import load_audio_text_image_video

# [MODIFIED] 新增以下导入，用于在 async 函数中包装同步调用
import asyncio
from functools import partial

class SenseAutoModel(BaseAutoModel):
    def __init__(self, config, **kwargs):
        super().__init__(**kwargs)
        self.logger = logging.getLogger(__name__)
        self.config = config
        
        if self.spk_model is not None:
            self.cb_model = ClusterBackend().to(kwargs["device"])
            spk_mode = kwargs.get("spk_mode", "punc_segment")
            if spk_mode not in ["default", "vad_segment", "punc_segment"]:
                self.logger.error("spk_mode 应该是 'default', 'vad_segment' 或 'punc_segment' 之一。")
            self.spk_mode = spk_mode

    def inference_with_vad(self, input, input_len=None, **cfg):
        kwargs = self.kwargs
        self.tokenizer = kwargs.get("tokenizer")
        deep_update(self.vad_kwargs, cfg)
        
        res = self.inference(input, input_len=input_len, model=self.vad_model, kwargs=self.vad_kwargs, **cfg)
        model = self.model
        deep_update(kwargs, cfg)
        kwargs["batch_size"] = max(int(kwargs.get("batch_size_s", 300)) * 1000, 1)
        batch_size_threshold_ms = int(kwargs.get("batch_size_threshold_s", 60)) * 1000

        key_list, data_list = prepare_data_iterator(input, input_len=input_len, data_type=kwargs.get("data_type", None))
        results_ret_list = []

        pbar_total = tqdm(total=len(res), dynamic_ncols=True, disable=kwargs.get("disable_pbar", False))

        for i, item in enumerate(res):
            key, vadsegments = item["key"], item["value"]
            input_i = data_list[i]
            fs = kwargs["frontend"].fs if hasattr(kwargs["frontend"], "fs") else 16000
            speech = load_audio_text_image_video(input_i, fs=fs, audio_fs=kwargs.get("fs", 16000))
            speech_lengths = len(speech)
            self.logger.debug(f"音频长度: {speech_lengths} 样本")

            if speech_lengths < 400:
                self.logger.warning(f"音频太短（{speech_lengths} 样本），可能导致处理错误")

            sorted_data = sorted([(seg, idx) for idx, seg in enumerate(vadsegments)], key=lambda x: x[0][1] - x[0][0])
            if not sorted_data:
                self.logger.info(f"解码, utt: {key}, 空语音")
                continue

            results_sorted = []
            all_segments = []
            beg_idx, end_idx = 0, 1
            max_len_in_batch = 0

            for j in range(len(sorted_data)):
                sample_length = sorted_data[j][0][1] - sorted_data[j][0][0]
                potential_batch_length = max(max_len_in_batch, sample_length) * (j + 1 - beg_idx)

                if (j < len(sorted_data) - 1 and 
                    sample_length < batch_size_threshold_ms and 
                    potential_batch_length < kwargs["batch_size"]):
                    max_len_in_batch = max(max_len_in_batch, sample_length)
                    end_idx += 1
                    continue

                speech_j, _ = slice_padding_audio_samples(speech, speech_lengths, sorted_data[beg_idx:end_idx])
                results = self.inference(speech_j, input_len=None, model=model, kwargs=kwargs, **cfg)

                if self.spk_model is not None:
                    for _b, speech_segment in enumerate(speech_j):
                        vad_segment = sorted_data[beg_idx:end_idx][_b][0]
                        segments = sv_chunk([[vad_segment[0] / 1000.0, vad_segment[1] / 1000.0, np.array(speech_segment)]])
                        all_segments.extend(segments)
                        speech_b = [seg[2] for seg in segments]
                        spk_res = self.inference(speech_b, input_len=None, model=self.spk_model, kwargs=kwargs, **cfg)
                        results[_b]["spk_embedding"] = spk_res[0]["spk_embedding"]
                beg_idx, end_idx = end_idx, end_idx + 1
                max_len_in_batch = sample_length
                results_sorted.extend(results)

            if len(results_sorted) != len(sorted_data):
                self.logger.info(f"解码，utt: {key}，空结果")
                continue

            restored_data = [0] * len(sorted_data)
            for j, (_, idx) in enumerate(sorted_data):
                restored_data[idx] = results_sorted[j]

            result = self.combine_results(restored_data, vadsegments)

            if self.spk_model is not None and kwargs.get("return_spk_res", True):
                all_segments.sort(key=lambda x: x[0])
                spk_embedding = result["spk_embedding"]
                labels = self.cb_model(spk_embedding.cpu(), oracle_num=kwargs.get("preset_spk_num", None))
                sv_output = postprocess(all_segments, None, labels, spk_embedding.cpu())

                if "timestamp" not in result:
                    self.logger.error(f"speaker diarization 依赖于时间戳对于 utt: {key}")
                    sentence_list = []
                else:
                    sentence_list = get_sentences(
                        tokens=result["token"],
                        timestamps=result["timestamp"],
                        tokenizer=self.tokenizer,
                        speech=speech,
                        sd_time_list=sv_output,
                        sample_rate=fs,
                        config=self.config
                    )
                    results_ret_list = sentence_list
            else:
                sentence_list = []
            pbar_total.update(1)

        pbar_total.close()
        return results_ret_list

    def combine_results(self, restored_data, vadsegments):
        result = {}
        for j, data in enumerate(restored_data):
            for k, v in data.items():
                if k.startswith("timestamp"):
                    if k not in result:
                        result[k] = []
                    for t in v:
                        t[0] += vadsegments[j][0]
                        t[1] += vadsegments[j][0]
                    result[k].extend(v)
                elif k == "spk_embedding":
                    if k not in result:
                        result[k] = v
                    else:
                        result[k] = torch.cat([result[k], v], dim=0)
                elif "token" in k:
                    if k not in result:
                        result[k] = v
                    else:
                        result[k].extend(v)
                else:
                    if k not in result:
                        result[k] = v
                    else:
                        result[k] += v
        return result

    # ----------------------------- #
    # [MODIFIED] 新增异步方法
    # ----------------------------- #
    async def generate_async(self, input, input_len=None, **cfg):
        """
        将原先 self.sense_model.generate(...) 的同步调用, 包装成异步:
        在线程池中执行, 以防止阻塞事件循环.
        """
        loop = asyncio.get_running_loop()
        func = partial(self.generate, input, input_len, **cfg)
        return await loop.run_in_executor(None, func)

================
File: core/hls_manager.py
================
import logging
import asyncio
from pathlib import Path
import m3u8
from typing import List, Union
from utils.decorators import handle_errors
from utils.task_storage import TaskPaths
import os.path
import shutil

logger = logging.getLogger(__name__)

class HLSManager:
    """处理 HLS 流媒体相关的功能"""
    def __init__(self, config, task_id: str, task_paths: TaskPaths):
        self.config = config
        self.task_id = task_id
        self.task_paths = task_paths
        self.logger = logging.getLogger(__name__)
        
        self.playlist_path = Path(task_paths.playlist_path)
        self.segments_dir = Path(task_paths.segments_dir)
        self.sequence_number = 0
        self._lock = asyncio.Lock()
        
        self.playlist = m3u8.M3U8()
        self.playlist.version = 3
        self.playlist.target_duration = 20
        self.playlist.media_sequence = 0
        self.playlist.playlist_type = 'VOD'
        self.playlist.is_endlist = False
        
        self.has_segments = False
        
        self._save_playlist()
    
    def _save_playlist(self) -> None:
        """保存播放列表到文件"""
        try:
            for segment in self.playlist.segments:
                if segment.uri is not None and not segment.uri.startswith('/'):
                    segment.uri = '/' + segment.uri
            
            with open(self.playlist_path, 'w', encoding='utf-8') as f:
                f.write(self.playlist.dumps())
            self.logger.info("播放列表已更新")
        except Exception as e:
            self.logger.error(f"保存播放列表失败: {e}")
            raise
    
    @handle_errors(None)
    async def add_segment(self, video_path: Union[str, Path], part_index: int) -> None:
        """添加新的视频片段到播放列表"""
        async with self._lock:
            try:
                self.segments_dir.mkdir(parents=True, exist_ok=True)
                
                segment_filename = f'segment_{self.sequence_number:04d}_%03d.ts'
                segment_pattern = str(self.segments_dir / segment_filename)
                temp_playlist_path = self.task_paths.processing_dir / f'temp_{part_index}.m3u8'
                
                cmd = [
                    'ffmpeg', '-y',
                    '-i', str(video_path),
                    '-c', 'copy',
                    '-f', 'hls',
                    '-hls_time', '10',
                    '-hls_list_size', '0',
                    '-hls_segment_type', 'mpegts',
                    '-hls_segment_filename', segment_pattern,
                    str(temp_playlist_path)
                ]
                
                process = await asyncio.create_subprocess_exec(
                    *cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                stdout, stderr = await process.communicate()
                
                if process.returncode != 0:
                    self.logger.error(f"FFmpeg 错误: {stderr.decode()}")
                    raise RuntimeError(f"FFmpeg 错误: {stderr.decode()}")

                temp_m3u8 = m3u8.load(str(temp_playlist_path))
                discontinuity_segment = m3u8.Segment(discontinuity=True)
                self.playlist.add_segment(discontinuity_segment)
                
                for segment in temp_m3u8.segments:
                    segment.uri = f"segments/{self.task_id}/{Path(segment.uri).name}"
                    self.playlist.segments.append(segment)
                
                self.sequence_number += len(temp_m3u8.segments)
                self.has_segments = True
                self._save_playlist()
                
            finally:
                if os.path.exists(str(temp_playlist_path)):
                    try:
                        os.unlink(str(temp_playlist_path))
                    except Exception as e:
                        self.logger.warning(f"清理临时文件失败: {e}")
    
    async def finalize_playlist(self) -> None:
        """标记播放列表为完成状态"""
        if self.has_segments:
            self.playlist.is_endlist = True
            self._save_playlist()
            self.logger.info("播放列表已保存，并标记为完成状态")
        else:
            self.logger.warning("播放列表为空，不标记为结束状态")

================
File: core/media_mixer.py
================
import numpy as np
import logging
import soundfile as sf
import os
import shutil
import asyncio
from contextlib import ExitStack
from tempfile import NamedTemporaryFile
from pathlib import Path
from typing import Optional, List
from utils.decorators import handle_errors

logger = logging.getLogger(__name__)

class MediaMixer:
    def __init__(self, config, sample_rate: int):
        self.config = config
        self.sample_rate = sample_rate
        self.max_val = 1.0
        self.overlap = self.config.AUDIO_OVERLAP
        self.vocals_volume = self.config.VOCALS_VOLUME
        self.background_volume = self.config.BACKGROUND_VOLUME
        self.full_audio_buffer = np.array([], dtype=np.float32)

    @handle_errors(logger)
    async def mixed_media_maker(self, sentences, task_state=None, output_path=None):
        """
        处理一批句子的音频和视频
        """
        if not sentences:
            logger.warning("接收到空的句子列表")
            return False

        full_audio = np.array([], dtype=np.float32)

        segment_index = sentences[0].segment_index
        segment_files = task_state.segment_media_files.get(segment_index)
        if not segment_files:
            logger.error(f"找不到分段 {segment_index} 的媒体文件")
            return False

        for sentence in sentences:
            if sentence.generated_audio is not None:
                audio_data = np.asarray(sentence.generated_audio, dtype=np.float32)
                if len(full_audio) > 0:
                    audio_data = self._apply_fade_effect(audio_data)
                full_audio = np.concatenate((full_audio, audio_data))
            else:
                logger.warning(
                    f"句子音频生成失败: '{sentence.raw_text[:30]}...', "
                    f"UUID: {sentence.model_input.get('uuid', 'unknown')}"
                )

        if len(full_audio) == 0:
            logger.error("没有有效的音频数据")
            return False

        start_time = 0.0 if sentences[0].is_first else (sentences[0].adjusted_start - sentences[0].segment_start * 1000) / 1000.0
        duration = sum(s.adjusted_duration for s in sentences) / 1000.0

        background_audio_path = segment_files['background']
        if background_audio_path is not None:
            full_audio = self._mix_with_background(background_audio_path, start_time, duration, full_audio)
            full_audio = self._normalize_audio(full_audio)

        self.full_audio_buffer = np.concatenate((self.full_audio_buffer, full_audio))

        video_path = segment_files['video']
        if video_path:
            await self._add_video_segment(video_path, start_time, duration, full_audio, output_path)
            return True

        return False

    def _apply_fade_effect(self, audio_data: np.ndarray) -> np.ndarray:
        """应用淡入淡出效果，自然处理重叠"""
        if audio_data is None or len(audio_data) == 0:
            return np.array([], dtype=np.float32)
        
        if len(audio_data) > self.overlap * 2:
            audio_data = audio_data.copy()
            fade_in = np.linspace(0, 1, self.overlap)
            fade_out = np.linspace(1, 0, self.overlap)
            
            audio_data[:self.overlap] *= fade_in
            audio_data[-self.overlap:] *= fade_out
            
            if len(self.full_audio_buffer) > 0:
                overlap_region = self.full_audio_buffer[-self.overlap:]
                audio_data[:self.overlap] = np.add(
                    overlap_region,
                    audio_data[:self.overlap],
                    dtype=np.float32
                )
        return audio_data

    def _mix_with_background(self, background_audio_path: str, start_time: float, duration: float, audio_data: np.ndarray) -> np.ndarray:
        background_audio, _ = sf.read(background_audio_path)
        background_audio = np.asarray(background_audio, dtype=np.float32)
        
        target_length = int(duration * self.sample_rate)
        start_sample = int(start_time * self.sample_rate)
        end_sample = start_sample + target_length
        background_segment = background_audio[start_sample:end_sample] if end_sample <= len(background_audio) else background_audio[start_sample:]
        
        result = np.zeros(target_length, dtype=np.float32)
        audio_length = min(len(audio_data), target_length)
        background_length = min(len(background_segment), target_length)
        
        if audio_length > 0:
            result[:audio_length] = audio_data[:audio_length] * self.vocals_volume
        if background_length > 0:
            result[:background_length] += background_segment[:background_length] * self.background_volume
        
        return result

    def _normalize_audio(self, audio_data: np.ndarray) -> np.ndarray:
        if audio_data is None or len(audio_data) == 0:
            return np.array([], dtype=np.float32)
        
        max_val = np.abs(audio_data).max()
        if max_val > self.max_val:
            audio_data = audio_data * (self.max_val / max_val)
        return audio_data

    @handle_errors(logger)
    async def _add_video_segment(self, video_path: str, start_time: float, duration: float, audio_data: np.ndarray, output_path: str) -> None:
        """添加视频片段"""
        if not os.path.exists(video_path):
            logger.error("视频文件不存在")
            raise FileNotFoundError("视频文件不存在")
        
        if audio_data is None or len(audio_data) == 0:
            logger.error("无有效音频数据")
            raise ValueError("无有效音频数据")
        
        if duration <= 0:
            logger.error("无效的持续时间")
            raise ValueError("无效的持续时间")

        with ExitStack() as stack:
            temp_video = stack.enter_context(NamedTemporaryFile(suffix='.mp4'))
            temp_audio = stack.enter_context(NamedTemporaryFile(suffix='.wav'))

            end_time = start_time + duration
            
            cmd = [
                'ffmpeg', '-y',
                '-i', video_path,
                '-ss', str(start_time),
                '-to', str(end_time),
                '-c:v', 'libx264',
                '-preset', 'superfast',
                '-an',
                '-vsync', 'vfr',
                temp_video.name
            ]
            await self._run_ffmpeg_command(cmd)

            await asyncio.to_thread(sf.write, temp_audio.name, audio_data, self.sample_rate)

            cmd = [
                'ffmpeg', '-y',
                '-i', temp_video.name,
                '-i', temp_audio.name,
                '-c:v', 'copy',
                '-c:a', 'aac',
                output_path
            ]
            await self._run_ffmpeg_command(cmd)

    @handle_errors(logger)
    async def _run_ffmpeg_command(self, command: List[str]) -> None:
        process = await asyncio.create_subprocess_exec(
            *command,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        stdout, stderr = await process.communicate()
        
        if process.returncode != 0:
            raise RuntimeError(f"FFmpeg 命令执行失败: {stderr.decode()}")

    async def reset(self):
        self.full_audio_buffer = np.array([], dtype=np.float32)
        logger.debug("已重置 mixer 状态")

================
File: core/model_in.py
================
import os
import logging
import torch
import numpy as np
import librosa
from typing import List, Optional
import asyncio
from concurrent.futures import ThreadPoolExecutor

class ModelIn:
    def __init__(self, cosy_model,
                 max_workers: Optional[int] = None,
                 max_concurrent_tasks: int = 4):
        self.cosy_frontend = cosy_model.frontend
        self.cosy_sample_rate = cosy_model.sample_rate
        self.logger = logging.getLogger(__name__)

        self.speaker_cache = {}
        self.max_val = 0.8

        cpu_count = os.cpu_count() or 1
        self.max_workers = min(max_workers or cpu_count, cpu_count)
        self.executor = ThreadPoolExecutor(max_workers=self.max_workers)
        self.semaphore = asyncio.Semaphore(max_concurrent_tasks)

        self.logger.info(
            f"ModelIn initialized with max_workers={self.max_workers}, "
            f"max_concurrent_tasks={max_concurrent_tasks}"
        )

    def postprocess(self, speech, top_db=60, hop_length=220, win_length=440):
        speech, _ = librosa.effects.trim(
            speech, top_db=top_db,
            frame_length=win_length,
            hop_length=hop_length
        )
        if speech.abs().max() > self.max_val:
            speech = speech / speech.abs().max() * self.max_val
        
        speech = torch.concat([speech, torch.zeros(1, int(self.cosy_sample_rate * 0.2))], dim=1)
        return speech

    def _update_text_features_sync(self, sentence):
        try:
            tts_text = sentence.trans_text
            normalized_segments = self.cosy_frontend.text_normalize(tts_text, split=True)

            segment_tokens = []
            segment_token_lens = []

            for seg in normalized_segments:
                txt, txt_len = self.cosy_frontend._extract_text_token(seg)
                segment_tokens.append(txt)
                segment_token_lens.append(txt_len)

            sentence.model_input['text'] = segment_tokens
            sentence.model_input['text_len'] = segment_token_lens
            sentence.model_input['normalized_text_segments'] = normalized_segments

            self.logger.debug(f"成功更新文本特征: {normalized_segments}")
            return sentence
        except Exception as e:
            self.logger.error(f"更新文本特征失败: {str(e)}")
            raise

    def _process_sentence_sync(self, sentence, reuse_speaker=False, reuse_uuid=False):
        speaker_id = sentence.speaker_id

        # 1) Speaker处理
        if not reuse_speaker:
            if speaker_id not in self.speaker_cache:
                processed_audio = self.postprocess(sentence.audio)
                self.speaker_cache[speaker_id] = self.cosy_frontend.frontend_cross_lingual(
                    "",
                    processed_audio,
                    self.cosy_sample_rate
                )
            speaker_features = self.speaker_cache[speaker_id].copy()
            sentence.model_input = speaker_features

        # 2) UUID处理
        if not reuse_uuid:
            sentence.model_input['uuid'] = ""

        # 3) 更新文本特征
        self._update_text_features_sync(sentence)
        return sentence

    async def _process_sentence_async(self, sentence, reuse_speaker=False, reuse_uuid=False):
        loop = asyncio.get_event_loop()
        async with self.semaphore:
            return await loop.run_in_executor(
                self.executor,
                self._process_sentence_sync,
                sentence,
                reuse_speaker,
                reuse_uuid
            )

    async def modelin_maker(self,
                            sentences,
                            reuse_speaker=False,
                            reuse_uuid=False,
                            batch_size=3):
        if not sentences:
            self.logger.warning("modelin_maker: 收到空的句子列表")
            return

        tasks = [
            asyncio.create_task(
                self._process_sentence_async(s, reuse_speaker, reuse_uuid)
            )
            for s in sentences
        ]

        try:
            results = []
            for i, task in enumerate(tasks, start=1):
                updated_sentence = await task
                results.append(updated_sentence)

                if i % batch_size == 0:
                    yield results
                    results = []

            if results:
                yield results

        except Exception as e:
            self.logger.error(f"modelin_maker处理失败: {str(e)}")
            raise

        finally:
            if not reuse_speaker:
                self.speaker_cache.clear()
                self.logger.debug("modelin_maker: 已清理 speaker_cache")

================
File: core/sentence_tools.py
================
import os
import torch
import torchaudio
import numpy as np
from typing import List, Tuple, Dict
from dataclasses import dataclass, field
from pathlib import Path
from config import Config

Token = int
Timestamp = Tuple[float, float]
SpeakerSegment = Tuple[float, float, int]

@dataclass
class Sentence:
    raw_text: str
    start: float
    end: float
    speaker_id: int
    trans_text: str = field(default="")
    sentence_id: int = field(default=-1)
    audio: torch.Tensor = field(default=None)
    target_duration: float = field(default=None)
    duration: float = field(default=0.0)
    diff: float = field(default=0.0)
    silence_duration: float = field(default=0.0)
    speed: float = field(default=1.0)
    is_first: bool = field(default=False)
    is_last: bool = field(default=False)
    model_input: Dict = field(default_factory=dict)
    generated_audio: np.ndarray = field(default=None)
    adjusted_start: float = field(default=0.0)
    adjusted_duration: float = field(default=0.0)
    segment_index: int = field(default=-1)
    segment_start: float = field(default=0.0)
    task_id: str = field(default="")

def tokens_timestamp_sentence(tokens: List[Token], timestamps: List[Timestamp], speaker_segments: List[SpeakerSegment], tokenizer, config: Config) -> List[Tuple[List[Token], List[Timestamp], int]]:
    sentences = []
    current_tokens = []
    current_timestamps = []
    token_index = 0

    for segment in speaker_segments:
        seg_start_ms = int(segment[0] * 1000)
        seg_end_ms = int(segment[1] * 1000)
        speaker_id = segment[2]

        while token_index < len(tokens):
            token = tokens[token_index]
            token_start, token_end = timestamps[token_index]

            if token_start >= seg_end_ms:
                break
            if token_end <= seg_start_ms:
                token_index += 1
                continue

            current_tokens.append(token)
            current_timestamps.append(timestamps[token_index])
            token_index += 1

            if token in config.STRONG_END_TOKENS and len(current_tokens) <= config.MIN_SENTENCE_LENGTH:
                if sentences:
                    previous_end_time = sentences[-1][1][-1][1]
                    current_start_time = current_timestamps[0][0]
                    time_gap = current_start_time - previous_end_time

                    if time_gap > config.SHORT_SENTENCE_MERGE_THRESHOLD_MS:
                        continue

                    sentences[-1] = (
                        sentences[-1][0] + current_tokens[:],
                        sentences[-1][1] + current_timestamps[:],
                        sentences[-1][2]
                    )
                    current_tokens.clear()
                    current_timestamps.clear()
                continue

            if (token in config.STRONG_END_TOKENS or len(current_tokens) > config.MAX_TOKENS_PER_SENTENCE):
                sentences.append((current_tokens[:], current_timestamps[:], speaker_id))
                current_tokens.clear()
                current_timestamps.clear()

        if current_tokens:
            if len(current_tokens) >= config.MIN_SENTENCE_LENGTH or not sentences:
                sentences.append((current_tokens[:], current_timestamps[:], speaker_id))
                current_tokens.clear()
                current_timestamps.clear()
            else:
                continue

    if current_tokens:
        if len(current_tokens) >= config.MIN_SENTENCE_LENGTH or not sentences:
            sentences.append((current_tokens[:], current_timestamps[:], speaker_id))
            current_tokens.clear()
            current_timestamps.clear()
        else:
            sentences[-1] = (
                sentences[-1][0] + current_tokens[:],
                sentences[-1][1] + current_timestamps[:],
                sentences[-1][2]
            )
            current_tokens.clear()
            current_timestamps.clear()

    return sentences

def merge_sentences(raw_sentences: List[Tuple[List[Token], List[Timestamp], int]], 
                   tokenizer,
                   input_duration: float,
                   config: Config) -> List[Sentence]:
    merged_sentences = []
    current = None
    current_tokens_count = 0

    for tokens, timestamps, speaker_id in raw_sentences:
        time_gap = timestamps[0][0] - current.end if current else float('inf')
        
        if (current and 
            current.speaker_id == speaker_id and 
            current_tokens_count + len(tokens) <= config.MAX_TOKENS_PER_SENTENCE and
            time_gap <= config.MAX_GAP_MS):
            current.raw_text += tokenizer.decode(tokens)
            current.end = timestamps[-1][1]
            current_tokens_count += len(tokens)
        else:
            if current:
                current.target_duration = timestamps[0][0] - current.start
                merged_sentences.append(current)
            
            text = tokenizer.decode(tokens)
            current = Sentence(
                raw_text=text, 
                start=timestamps[0][0], 
                end=timestamps[-1][1], 
                speaker_id=speaker_id,
            )
            current_tokens_count = len(tokens)

    if current:
        current.target_duration = input_duration - current.start
        merged_sentences.append(current)

    if merged_sentences:
        merged_sentences[0].is_first = True
        merged_sentences[-1].is_last = True

    return merged_sentences

def extract_audio(sentences: List[Sentence], speech: torch.Tensor, sr: int, config: Config) -> List[Sentence]:
    target_samples = int(config.SPEAKER_AUDIO_TARGET_DURATION * sr)
    speech = speech.unsqueeze(0) if speech.dim() == 1 else speech

    speaker_segments: Dict[int, List[Tuple[int, int, int]]] = {}
    for idx, s in enumerate(sentences):
        start_sample = int(s.start * sr / 1000)
        end_sample = int(s.end * sr / 1000)
        speaker_segments.setdefault(s.speaker_id, []).append((start_sample, end_sample, idx))

    speaker_audio_cache: Dict[int, torch.Tensor] = {}

    for speaker_id, segments in speaker_segments.items():
        segments.sort(key=lambda x: x[1] - x[0], reverse=True)
        longest_start, longest_end, _ = segments[0]

        ignore_samples = int(0.5 * sr)
        adjusted_start = longest_start + ignore_samples
        available_length_adjusted = longest_end - adjusted_start

        if available_length_adjusted > 0:
            audio_length = min(target_samples, available_length_adjusted)
            speaker_audio = speech[:, adjusted_start:adjusted_start + audio_length]
        else:
            available_length_original = longest_end - longest_start
            audio_length = min(target_samples, available_length_original)
            speaker_audio = speech[:, longest_start:longest_start + audio_length]

        speaker_audio_cache[speaker_id] = speaker_audio

    for sentence in sentences:
        sentence.audio = speaker_audio_cache.get(sentence.speaker_id)

    output_dir = Path(config.TASKS_DIR) / sentences[0].task_id / 'speakers'
    output_dir.mkdir(parents=True, exist_ok=True)

    for speaker_id, audio in speaker_audio_cache.items():
        if audio is not None:
            output_path = output_dir / f'speaker_{speaker_id}.wav'
            torchaudio.save(str(output_path), audio, sr)

    return sentences

def get_sentences(tokens: List[Token],
                  timestamps: List[Timestamp],
                  speech: torch.Tensor,
                  tokenizer,
                  sd_time_list: List[SpeakerSegment],
                  sample_rate: int = 16000,
                  config: Config = None) -> List[Sentence]:
    if config is None:
        config = Config()

    input_duration = (speech.shape[-1] / sample_rate) * 1000

    raw_sentences = tokens_timestamp_sentence(tokens, timestamps, sd_time_list, tokenizer, config)
    merged_sentences = merge_sentences(raw_sentences, tokenizer, input_duration, config)
    sentences_with_audio = extract_audio(merged_sentences, speech, sample_rate, config)

    return sentences_with_audio

================
File: core/tts_token_gener.py
================
import logging
import asyncio
from concurrent.futures import ThreadPoolExecutor
import uuid
import torch
import os

class TTSTokenGenerator:
    def __init__(self, cosyvoice_model, Hz=25, max_workers=None):
        self.cosyvoice_model = cosyvoice_model.model
        self.Hz = Hz
        cpu_count = os.cpu_count()
        self.max_workers = min(max_workers or cpu_count, cpu_count)
        self.executor = None
        self.logger = logging.getLogger(__name__)

    async def tts_token_maker(self, sentences, reuse_uuid=False):
        """并发为句子生成 TTS token。"""
        try:
            if self.executor is None:
                self.executor = ThreadPoolExecutor(max_workers=self.max_workers)
                self.logger.debug(f"创建线程池, max_workers={self.max_workers}")

            loop = asyncio.get_event_loop()
            tasks = []

            for s in sentences:
                current_uuid = (
                    s.model_input.get('uuid') if reuse_uuid and s.model_input.get('uuid')
                    else str(uuid.uuid1())
                )
                task = loop.run_in_executor(
                    self.executor,
                    self._generate_tts_single,
                    s,
                    current_uuid
                )
                tasks.append(task)

            processed = await asyncio.gather(*tasks)

            for sen in processed:
                if not sen.model_input.get('segment_speech_tokens'):
                    self.logger.error(f"TTS token 生成失败: {sen.trans_text}")

            return processed

        except Exception as e:
            self.logger.error(f"TTS token 生成失败: {e}")
            raise

    def _generate_tts_single(self, sentence, main_uuid):
        model_input = sentence.model_input
        segment_tokens_list = []
        segment_uuids = []
        total_token_count = 0

        try:
            for i, (text, text_len) in enumerate(zip(model_input['text'], model_input['text_len'])):
                seg_uuid = f"{main_uuid}_seg_{i}"
                with self.cosyvoice_model.lock:
                    self.cosyvoice_model.tts_speech_token_dict[seg_uuid] = []
                    self.cosyvoice_model.llm_end_dict[seg_uuid] = False
                    if hasattr(self.cosyvoice_model, 'mel_overlap_dict'):
                        self.cosyvoice_model.mel_overlap_dict[seg_uuid] = None
                    self.cosyvoice_model.hift_cache_dict[seg_uuid] = None

                self.cosyvoice_model.llm_job(
                    text,
                    model_input.get('prompt_text', torch.zeros(1, 0, dtype=torch.int32)),
                    model_input.get('llm_prompt_speech_token', torch.zeros(1, 0, dtype=torch.int32)),
                    model_input.get('llm_embedding', torch.zeros(0, 192)),
                    seg_uuid
                )

                seg_tokens = self.cosyvoice_model.tts_speech_token_dict[seg_uuid]
                segment_tokens_list.append(seg_tokens)
                segment_uuids.append(seg_uuid)
                total_token_count += len(seg_tokens)

            total_duration_s = total_token_count / self.Hz
            sentence.duration = total_duration_s * 1000

            model_input['segment_speech_tokens'] = segment_tokens_list
            model_input['segment_uuids'] = segment_uuids
            model_input['uuid'] = main_uuid

            self.logger.debug(
                f"TTS token 生成完成 (UUID={main_uuid}, 时长={total_duration_s:.2f}s, 段数={len(segment_uuids)})"
            )
            return sentence

        except Exception as e:
            self.logger.error(f"生成失败 (UUID={main_uuid}): {e}")
            with self.cosyvoice_model.lock:
                for seg_uuid in segment_uuids:
                    self.cosyvoice_model.tts_speech_token_dict.pop(seg_uuid, None)
                    self.cosyvoice_model.llm_end_dict.pop(seg_uuid, None)
                    self.cosyvoice_model.hift_cache_dict.pop(seg_uuid, None)
                    if hasattr(self.cosyvoice_model, 'mel_overlap_dict'):
                        self.cosyvoice_model.mel_overlap_dict.pop(seg_uuid, None)
            raise

================
File: utils/decorators.py
================
import logging
import functools
from typing import Callable, Any, Optional, Coroutine, AsyncGenerator, TypeVar, Union, Literal
import asyncio
import time

logger = logging.getLogger(__name__)

T = TypeVar('T')
WorkerResult = Union[T, AsyncGenerator[T, None]]
WorkerMode = Literal['base', 'stream']

def handle_errors(custom_logger: Optional[logging.Logger] = None) -> Callable:
    """
    错误处理装饰器。可应用于需要统一捕获日志的异步函数。
    """
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        async def wrapper(*args, **kwargs) -> Any:
            actual_logger = custom_logger
            if not actual_logger and args and hasattr(args[0], 'logger'):
                actual_logger = args[0].logger
            if not actual_logger:
                actual_logger = logger

            start_time = time.time()
            try:
                result = await func(*args, **kwargs)
                elapsed = time.time() - start_time
                actual_logger.debug(f"{func.__name__} 正常结束，耗时 {elapsed:.2f}s")
                return result
            except Exception as e:
                elapsed = time.time() - start_time
                actual_logger.error(f"{func.__name__} 执行出错，耗时 {elapsed:.2f}s, 错误: {e}", exc_info=True)
                raise
        return wrapper
    return decorator

def worker_decorator(
    input_queue_attr: str,
    next_queue_attr: Optional[str] = None,
    worker_name: Optional[str] = None,
    mode: WorkerMode = 'base'
) -> Callable:
    """
    通用 Worker 装饰器
    """
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        async def wrapper(self, task_state, *args, **kwargs):
            worker_display_name = worker_name or func.__name__
            wlogger = getattr(self, 'logger', logger)

            input_queue = getattr(task_state, input_queue_attr)
            next_queue = getattr(task_state, next_queue_attr) if next_queue_attr else None

            wlogger.info(f"[{worker_display_name}] 启动 (TaskID={task_state.task_id}). "
                         f"输入队列: {input_queue_attr}, 下游队列: {next_queue_attr if next_queue_attr else '无'}")

            processed_count = 0
            try:
                while True:
                    try:
                        queue_size_before = input_queue.qsize()
                        item = await input_queue.get()
                        if item is None:
                            if next_queue:
                                await next_queue.put(None)
                            wlogger.info(f"[{worker_display_name}] 收到停止信号。已处理 {processed_count} 个item。")
                            break

                        wlogger.debug(f"[{worker_display_name}] 从 {input_queue_attr} 取出一个item. 队列剩余: {queue_size_before}")

                        start_time = time.time()
                        if mode == 'stream':
                            async for result in func(self, item, task_state, *args, **kwargs):
                                if result is not None and next_queue:
                                    await next_queue.put(result)
                        else:
                            result = await func(self, item, task_state, *args, **kwargs)
                            if result is not None and next_queue:
                                await next_queue.put(result)

                        processed_count += 1
                        elapsed = time.time() - start_time
                        wlogger.debug(f"[{worker_display_name}] item处理完成，耗时 {elapsed:.2f}s. "
                                      f"TaskID={task_state.task_id}, 已处理计数: {processed_count}")

                    except asyncio.CancelledError:
                        wlogger.warning(f"[{worker_display_name}] 被取消 (TaskID={task_state.task_id}). "
                                        f"已处理 {processed_count} 个item")
                        if next_queue:
                            await next_queue.put(None)
                        break
                    except Exception as e:
                        wlogger.error(f"[{worker_display_name}] 发生异常: {e} (TaskID={task_state.task_id}). "
                                      f"已处理 {processed_count} 个item", exc_info=True)
                        if next_queue:
                            await next_queue.put(None)
                        break
            finally:
                wlogger.info(f"[{worker_display_name}] 结束 (TaskID={task_state.task_id}). 共处理 {processed_count} 个item.")
        return wrapper
    return decorator

================
File: utils/media_utils.py
================
import logging
import asyncio
import numpy as np
import torch
import torchaudio
import librosa
import soundfile as sf
from pathlib import Path
from typing import List, Tuple, Dict, Union
from utils.decorators import handle_errors
from utils.temp_file_manager import TempFileManager

logger = logging.getLogger(__name__)

class MediaUtils:
    def __init__(self, config, audio_separator, target_sr: int = 24000):
        self.config = config
        self.target_sr = target_sr
        self.logger = logging.getLogger(__name__)
        # audio_separator应为实现了 async 方法的实例 (ClearVoiceSeparator等)
        self.audio_separator = audio_separator

    def normalize_and_resample(
        self,
        audio_input: Union[Tuple[int, np.ndarray], np.ndarray],
        target_sr: int = None
    ) -> np.ndarray:
        """
        同步方式的重采样和归一化。
        若音频比较大，建议用 asyncio.to_thread(...) 包装本函数，以防阻塞事件循环。
        """
        if isinstance(audio_input, tuple):
            fs, audio_input = audio_input
        else:
            fs = target_sr
        
        audio_input = audio_input.astype(np.float32)
        
        max_val = np.abs(audio_input).max()
        if max_val > 0:
            audio_input = audio_input / max_val
        
        # 如果多通道, 转单通道
        if len(audio_input.shape) > 1:
            audio_input = audio_input.mean(axis=-1)
        
        # 如果源采样率与目标采样率不一致, 用torchaudio进行重采样
        if fs != target_sr:
            audio_input = np.ascontiguousarray(audio_input)
            resampler = torchaudio.transforms.Resample(
                orig_freq=fs,
                new_freq=target_sr,
                dtype=torch.float32
            )
            audio_input = resampler(torch.from_numpy(audio_input)[None, :])[0].numpy()
        
        return audio_input

    @handle_errors(logger)
    async def _run_ffmpeg_command(self, cmd: List[str]) -> Tuple[bytes, bytes]:
        """
        启动ffmpeg子进程并异步等待执行完毕，不会阻塞事件循环
        """
        process = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        stdout, stderr = await process.communicate()
        
        if process.returncode != 0:
            error_msg = stderr.decode() if stderr else "Unknown error"
            raise RuntimeError(f"FFmpeg 命令执行失败: {error_msg}")
        return stdout, stderr

    @handle_errors(logger)
    async def extract_audio(
        self,
        video_path: str,
        output_path: str,
        start: float = 0,
        duration: float = None
    ) -> str:
        """
        调用 FFmpeg 提取音频 (异步)
        """
        cmd = [
            'ffmpeg', '-y',
            '-i', video_path,
            '-ss', str(start)
        ]
        if duration is not None:
            cmd.extend(['-t', str(duration)])
        
        cmd.extend([
            '-vn',
            '-acodec', 'pcm_f32le',
            '-ac', '1',
            output_path
        ])
        await self._run_ffmpeg_command(cmd)
        return output_path

    @handle_errors(logger)
    async def extract_video(
        self,
        video_path: str,
        output_path: str,
        start: float = 0,
        duration: float = None
    ) -> str:
        """
        调用 FFmpeg 提取无音视频 (异步)
        """
        if start == 0:
            cmd = [
                'ffmpeg', '-y',
                '-i', video_path,
                '-an',
                '-c:v', 'copy',
                output_path
            ]
        else:
            cmd = [
                'ffmpeg', '-y',
                '-i', video_path,
                '-ss', str(start)
            ]
            if duration is not None:
                cmd.extend(['-t', str(duration)])
            
            cmd.extend([
                '-an',
                '-c:v', 'libx264',
                '-preset', 'ultrafast',
                '-crf', '18',
                '-tune', 'fastdecode',
                output_path
            ])
        
        self.logger.debug(f"执行 FFmpeg 命令: {' '.join(cmd)}")
        await self._run_ffmpeg_command(cmd)
        return output_path

    @handle_errors(logger)
    async def get_video_duration(self, video_path: str) -> float:
        """
        用 ffprobe 查询视频时长 (异步)
        """
        cmd = [
            'ffprobe',
            '-v', 'error',
            '-show_entries', 'format=duration',
            '-of', 'default=noprint_wrappers=1:nokey=1',
            video_path
        ]
        process = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        stdout, stderr = await process.communicate()
        
        if process.returncode != 0:
            raise RuntimeError(f"获取视频时长失败: {stderr.decode()}")

        duration = float(stdout.decode().strip())
        return duration

    @handle_errors(logger)
    async def get_audio_segments(self, duration: float) -> List[Tuple[float, float]]:
        """
        按照配置中的SEGMENT_MINUTES分割时间片。仅仅是一些轻量级计算，不会阻塞。
        """
        segment_length = self.config.SEGMENT_MINUTES * 60
        min_length = self.config.MIN_SEGMENT_MINUTES * 60
        
        if duration <= min_length:
            return [(0, duration)]
        
        segments = []
        current_pos = 0
        
        while current_pos < duration:
            remaining_duration = duration - current_pos
            
            if remaining_duration <= segment_length:
                if remaining_duration < min_length and segments:
                    start = segments[-1][0]
                    new_duration = duration - start
                    segments[-1] = (start, new_duration)
                else:
                    segments.append((current_pos, remaining_duration))
                break
            
            segments.append((current_pos, segment_length))
            current_pos += segment_length
        
        return segments

    @handle_errors(logger)
    async def extract_segment(
        self,
        video_path: str,
        start: float,
        duration: float,
        output_dir: Path,
        segment_index: int
    ) -> Dict[str, Union[str, float]]:
        """
        1) 提取纯视频 + 音频
        2) 调用 audio_separator 分离人声/背景
        3) 重采样+写音频文件
        4) 返回分段文件信息 (video, vocals, background, duration)
        """
        temp_files = {}
        try:
            silent_video = str(output_dir / f"video_silent_{segment_index}.mp4")
            full_audio = str(output_dir / f"audio_full_{segment_index}.wav")
            vocals_audio = str(output_dir / f"vocals_{segment_index}.wav")
            background_audio = str(output_dir / f"background_{segment_index}.wav")
            
            # (1) 并发提取音频 & 视频
            await asyncio.gather(
                self.extract_audio(video_path, full_audio, start, duration),
                self.extract_video(video_path, silent_video, start, duration)
            )
            
            # (2) 异步分离人声
            vocals, background, sr = await asyncio.to_thread(
                self.audio_separator.separate_audio,
                full_audio
            )


            # (3) 重采样 & 写文件，都用 to_thread 包装，避免阻塞事件循环
            background = await asyncio.to_thread(
                self.normalize_and_resample,
                (sr, background),
                self.target_sr
            )
            
            await asyncio.to_thread(sf.write, vocals_audio, vocals, sr, subtype='FLOAT')
            await asyncio.to_thread(sf.write, background_audio, background, self.target_sr, subtype='FLOAT')
            
            segment_duration = len(vocals) / sr

            # optional: 删除完整音频
            Path(full_audio).unlink(missing_ok=True)

            temp_files = {
                'video': silent_video,
                'vocals': vocals_audio,
                'background': background_audio,
                'duration': segment_duration
            }
            return temp_files
            
        except Exception as e:
            # 清理已生成的临时文件
            for file_path in temp_files.values():
                if isinstance(file_path, str) and Path(file_path).exists():
                    Path(file_path).unlink()
            raise

================
File: utils/sentence_logger.py
================
import logging
import json
from pathlib import Path
from typing import List, Dict, Any
import asyncio
from utils.decorators import handle_errors

logger = logging.getLogger(__name__)

class SentenceLogger:
    """句子日志记录器"""
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self._lock = asyncio.Lock()
    
    def _format_sentence(self, sentence: Dict[str, Any]) -> Dict[str, Any]:
        return {
            'id': getattr(sentence, 'sentence_id', -1),
            'start_time': getattr(sentence, 'start', 0),
            'end_time': getattr(sentence, 'end', 0),
            'text': getattr(sentence, 'text', ''),
            'translation': getattr(sentence, 'translation', ''),
            'duration': getattr(sentence, 'duration', 0),
            'speaker_id': getattr(sentence, 'speaker_id', 0),
            'speaker_similarity': getattr(sentence, 'speaker_similarity', 0),
            'speaker_embedding': (
                getattr(sentence, 'speaker_embedding', []).tolist() 
                if hasattr(getattr(sentence, 'speaker_embedding', []), 'tolist') 
                else getattr(sentence, 'speaker_embedding', [])
            )
        }
    
    @handle_errors(logger)
    async def save_sentences(self, sentences: List[Dict[str, Any]], output_path: Path, task_id: str) -> None:
        async with self._lock:
            try:
                formatted_sentences = [self._format_sentence(s) for s in sentences]
                output_path.parent.mkdir(parents=True, exist_ok=True)
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(formatted_sentences, f, ensure_ascii=False, indent=2)
                
                self.logger.debug(f"已保存 {len(sentences)} 个句子到 {output_path}")
            except Exception as e:
                self.logger.error(f"保存句子信息失败: {e}")
                raise

================
File: utils/task_state.py
================
from dataclasses import dataclass, field
from typing import Any, Dict
import asyncio
from utils.task_storage import TaskPaths

@dataclass
class TaskState:
    """
    每个任务的独立状态：包括队列、处理进度、分段信息等
    """
    task_id: str
    video_path: str
    task_paths: TaskPaths
    hls_manager: Any = None
    target_language: str = "zh"

    # 已处理到的句子计数
    sentence_counter: int = 0

    # 时间戳记录
    current_time: float = 0

    # 第几个 HLS 批次 (混音后输出)
    batch_counter: int = 0

    # 每个分段对应的媒体文件信息
    segment_media_files: Dict[int, Dict[str, Any]] = field(default_factory=dict)

    # 各个异步队列 (翻译->模型输入->tts_token->时长对齐->音频生成->混音)
    translation_queue: asyncio.Queue = field(default_factory=asyncio.Queue)
    modelin_queue: asyncio.Queue = field(default_factory=asyncio.Queue)
    tts_token_queue: asyncio.Queue = field(default_factory=asyncio.Queue)
    duration_align_queue: asyncio.Queue = field(default_factory=asyncio.Queue)
    audio_gen_queue: asyncio.Queue = field(default_factory=asyncio.Queue)
    mixing_queue: asyncio.Queue = field(default_factory=asyncio.Queue)

================
File: utils/task_storage.py
================
import shutil
import logging
import os
from pathlib import Path
from config import Config

logger = logging.getLogger(__name__)

class TaskPaths:
    def __init__(self, config: Config, task_id: str):
        self.config = config
        self.task_id = task_id

        self.task_dir = config.TASKS_DIR / task_id
        self.input_dir = self.task_dir / "input"
        self.processing_dir = self.task_dir / "processing"
        self.output_dir = self.task_dir / "output"

        self.segments_dir = config.PUBLIC_DIR / "segments" / task_id
        self.playlist_path = config.PUBLIC_DIR / "playlists" / f"playlist_{task_id}.m3u8"

        self.media_dir = self.processing_dir / "media"
        self.processing_segments_dir = self.processing_dir / "segments"

    def create_directories(self):
        dirs = [
            self.task_dir,
            self.input_dir,
            self.processing_dir,
            self.output_dir,
            self.segments_dir,
            self.media_dir,
            self.processing_segments_dir
        ]
        for d in dirs:
            d.mkdir(parents=True, exist_ok=True)
            logger.debug(f"[TaskPaths] 创建目录: {d}")

    async def cleanup(self, keep_output: bool = False):
        try:
            if keep_output:
                logger.info(f"[TaskPaths] 保留输出目录, 即将清理输入/processing/segments")
                dirs_to_clean = [self.input_dir, self.processing_dir, self.segments_dir]
                for d in dirs_to_clean:
                    if d.exists():
                        shutil.rmtree(d)
                        logger.debug(f"[TaskPaths] 已清理: {d}")
            else:
                logger.info(f"[TaskPaths] 全量清理任务目录: {self.task_dir}")
                if self.task_dir.exists():
                    shutil.rmtree(str(self.task_dir))
                    logger.debug(f"[TaskPaths] 已删除: {self.task_dir}")

                if self.segments_dir.exists():
                    shutil.rmtree(str(self.segments_dir))
                    logger.debug(f"[TaskPaths] 已删除: {self.segments_dir}")
        except Exception as e:
            logger.error(f"[TaskPaths] 清理任务目录失败: {e}", exc_info=True)
            raise

================
File: utils/temp_file_manager.py
================
import logging
from pathlib import Path
from typing import Set

logger = logging.getLogger(__name__)

class TempFileManager:
    """临时文件管理器"""
    def __init__(self, base_dir: Path):
        self.base_dir = base_dir
        self.temp_files: Set[Path] = set()
    
    def add_file(self, file_path: Path) -> None:
        self.temp_files.add(Path(file_path))
    
    async def cleanup(self) -> None:
        for file_path in self.temp_files:
            try:
                if file_path.exists():
                    file_path.unlink()
                    logger.debug(f"已删除临时文件: {file_path}")
            except Exception as e:
                logger.warning(f"清理临时文件失败: {file_path}, 错误: {e}")
        self.temp_files.clear()

================
File: __init__.py
================
# 空文件即可，标识这是一个 Python 包

================
File: .cursorignore
================
# 忽略模型文件夹，因为包含大量模型文件和第三方代码
models/

================
File: .env.example
================
# ================================
# .env.example
# ================================

# 【可选】修改 FLASK_ENV，默认使用 development 方便调试
FLASK_ENV=development

# 翻译模型选择 (可选值: deepseek, glm4, gemini)
TRANSLATION_MODEL=deepseek

# API Keys (请替换为实际的密钥)
ZHIPUAI_API_KEY=your_zhipuai_key_here
GEMINI_API_KEY=your_gemini_key_here
DEEPSEEK_API_KEY=your_deepseek_key_here

================
File: api.py
================
import sys
from pathlib import Path
import logging
import uuid
import asyncio
from typing import Dict

import uvicorn
from fastapi import FastAPI, File, UploadFile, HTTPException, Request, Form
from fastapi.responses import JSONResponse, FileResponse
from fastapi.templating import Jinja2Templates
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
import aiofiles

from config import Config
config = Config()
config.init_directories()

sys.path.extend(config.SYSTEM_PATHS)

logging.basicConfig(
    level=logging.INFO,
    format="%(levelname)s | %(asctime)s | %(name)s | L%(lineno)d | %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)
logger = logging.getLogger(__name__)

from video_translator import ViTranslator
from core.hls_manager import HLSManager
from utils.task_storage import TaskPaths
from fastapi import BackgroundTasks

app = FastAPI(debug=True)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

current_dir = Path(__file__).parent
templates = Jinja2Templates(directory=str(current_dir / "templates"))

vi_translator = ViTranslator(config=config)
task_results: Dict[str, dict] = {}

@app.get("/")
async def index(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

@app.post("/upload")
async def upload_video(
    video: UploadFile = File(...),
    target_language: str = Form("zh")
):
    try:
        if not video:
            raise HTTPException(status_code=400, detail="没有文件上传")
        
        if not video.content_type.startswith('video/'):
            raise HTTPException(status_code=400, detail="只支持视频文件")
            
        if target_language not in ["zh", "en", "ja", "ko"]:
            raise HTTPException(status_code=400, detail=f"不支持的目标语言: {target_language}")
        
        task_id = str(uuid.uuid4())
        task_paths = TaskPaths(config, task_id)
        task_paths.create_directories()
        
        video_path = task_paths.input_dir / f"original_{video.filename}"
        try:
            async with aiofiles.open(video_path, "wb") as f:
                content = await video.read()
                await f.write(content)
        except Exception as e:
            logger.error(f"保存文件失败: {str(e)}")
            await task_paths.cleanup()
            raise HTTPException(status_code=500, detail="文件保存失败")
        
        hls_manager = HLSManager(config, task_id, task_paths)
        
        task = asyncio.create_task(vi_translator.trans_video(
            video_path=str(video_path),
            task_id=task_id,
            task_paths=task_paths,
            hls_manager=hls_manager,
            target_language=target_language
        ))
        
        task_results[task_id] = {
            "status": "processing",
            "message": "视频处理中",
            "progress": 0
        }
        
        async def on_task_complete(t):
            try:
                result = await t
                if result.get('status') == 'success':
                    task_results[task_id].update({
                        "status": "success",
                        "message": "处理完成",
                        "progress": 100
                    })
                else:
                    task_results[task_id].update({
                        "status": "error",
                        "message": result.get('message', '处理失败'),
                        "progress": 0
                    })
                    await task_paths.cleanup()
            except Exception as e:
                logger.error(f"任务处理失败: {str(e)}")
                task_results[task_id].update({
                    "status": "error",
                    "message": str(e),
                    "progress": 0
                })
                await task_paths.cleanup()
        
        task.add_done_callback(lambda t: asyncio.create_task(on_task_complete(t)))
        
        return {
            'status': 'processing',
            'task_id': task_id,
            'message': '视频上传成功，开始处理'
        }
    except HTTPException as e:
        raise e
    except Exception as e:
        logger.error(f"上传处理失败: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/task/{task_id}")
async def get_task_status(task_id: str):
    result = task_results.get(task_id)
    if not result:
        return {
            "status": "error",
            "message": "任务不存在",
            "progress": 0
        }
    return result

app.mount("/playlists", StaticFiles(directory=str(config.PUBLIC_DIR / "playlists")), name="playlists")

@app.get("/playlists/{task_id}/{filename}")
async def serve_playlist(task_id: str, filename: str):
    try:
        playlist_path = config.PUBLIC_DIR / "playlists" / filename
        if not playlist_path.exists():
            logger.error(f"播放列表未找到: {playlist_path}")
            raise HTTPException(status_code=404, detail="播放列表未找到")
        
        logger.info(f"提供播放列表: {playlist_path}")
        return FileResponse(
            str(playlist_path), 
            media_type='application/vnd.apple.mpegurl',
            headers={
                "Cache-Control": "public, max-age=3600",
                "Access-Control-Allow-Origin": "*"
            }
        )
    except Exception as e:
        logger.error(f"服务播放列表失败: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/segments/{task_id}/{filename}")
async def serve_segments(task_id: str, filename: str):
    try:
        segment_path = config.PUBLIC_DIR / "segments" / task_id / filename
        if not segment_path.exists():
            logger.error(f"片段文件未找到: {segment_path}")
            raise HTTPException(status_code=404, detail="片段文件未找到")
        
        logger.debug(f"提供视频片段: {segment_path}")
        return FileResponse(
            str(segment_path),
            media_type='video/MP2T',
            headers={
                "Cache-Control": "no-cache, no-store, must-revalidate",
                "Pragma": "no-cache",
                "Expires": "0",
                "Access-Control-Allow-Origin": "*"
            }
        )
    except Exception as e:
        logger.error(f"服务视频片段失败: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    uvicorn.run(
        app,
        host=config.SERVER_HOST,
        port=config.SERVER_PORT,
        log_level="info"
    )

================
File: config.py
================
import os
from pathlib import Path
from dotenv import load_dotenv

current_dir = Path(__file__).parent
env_path = current_dir / '.env'
load_dotenv(env_path)

project_dir = current_dir.parent
storage_dir = project_dir / 'storage'

class Config:
    SERVER_HOST = "0.0.0.0"
    SERVER_PORT = 8000
    LOG_LEVEL = "DEBUG"

    BASE_DIR = storage_dir
    TASKS_DIR = BASE_DIR / "tasks"
    PUBLIC_DIR = BASE_DIR / "public"

    BATCH_SIZE = 6
    TARGET_SPEAKER_AUDIO_DURATION = 8
    VAD_SR = 16000
    VOCALS_VOLUME = 0.7
    BACKGROUND_VOLUME = 0.3
    AUDIO_OVERLAP = 1024
    NORMALIZATION_THRESHOLD = 0.9

    SEGMENT_MINUTES = 5
    MIN_SEGMENT_MINUTES = 3

    TRANSLATION_MODEL = os.getenv("TRANSLATION_MODEL", "deepseek")
    ZHIPUAI_API_KEY = os.getenv("ZHIPUAI_API_KEY", "")
    GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "")
    DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY", "")

    SYSTEM_PATHS = [
        str(current_dir / 'models' / 'CosyVoice'),
        str(current_dir / 'models' / 'ClearVoice'),
        str(current_dir / 'models' / 'CosyVoice' / 'third_party' / 'Matcha-TTS')
    ]

    MODEL_DIR = project_dir / "models"

    @property
    def MODEL_PATH(self) -> Path:
        return Path(self.MODEL_DIR)

    @property
    def BASE_PATH(self) -> Path:
        return self.BASE_DIR

    @property
    def TASKS_PATH(self) -> Path:
        return self.TASKS_DIR

    @property
    def PUBLIC_PATH(self) -> Path:
        return self.PUBLIC_DIR

    @classmethod
    def init_directories(cls):
        directories = [
            cls.BASE_DIR,
            cls.TASKS_DIR,
            cls.PUBLIC_DIR,
            cls.PUBLIC_DIR / "playlists",
            cls.PUBLIC_DIR / "segments"
        ]
        for dir_path in directories:
            dir_path.mkdir(parents=True, exist_ok=True)
            os.chmod(str(dir_path), 0o755)

    MAX_GAP_MS = 2000
    SHORT_SENTENCE_MERGE_THRESHOLD_MS = 1000
    MAX_TOKENS_PER_SENTENCE = 80
    MIN_SENTENCE_LENGTH = 4
    SENTENCE_END_TOKENS = {9686, 9688, 9676, 9705, 9728, 9729, 20046, 24883, 24879}
    STRONG_END_TOKENS = {9688, 9676, 9705, 9729, 20046, 24883}
    WEAK_END_TOKENS = {9686, 9728, 24879}
    SPEAKER_AUDIO_TARGET_DURATION = 8.0
    TRANSLATION_BATCH_SIZE = 50
    MODELIN_BATCH_SIZE = 3
    # 控制同时处理多少个视频分段
    MAX_PARALLEL_SEGMENTS = 2

================
File: pipeline_scheduler.py
================
import asyncio
import logging
from typing import List
from utils.decorators import worker_decorator
from utils.task_state import TaskState
from core.sentence_tools import Sentence

logger = logging.getLogger(__name__)

class PipelineScheduler:
    """
    多个Worker的调度器，负责翻译->model_in->tts_token->时长对齐->音频生成->混音 等流水线。
    """

    def __init__(
        self,
        translator,
        model_in,
        tts_token_generator,
        duration_aligner,
        audio_generator,
        timestamp_adjuster,
        mixer,
        config
    ):
        self.logger = logging.getLogger(__name__)
        self.translator = translator
        self.model_in = model_in
        self.tts_token_generator = tts_token_generator
        self.duration_aligner = duration_aligner
        self.audio_generator = audio_generator
        self.timestamp_adjuster = timestamp_adjuster
        self.mixer = mixer
        self.config = config

        self._workers = []

    async def start_workers(self, task_state: TaskState):
        self.logger.info(f"[PipelineScheduler] start_workers -> TaskID={task_state.task_id}")
        self._workers = [
            asyncio.create_task(self._translation_worker(task_state)),
            asyncio.create_task(self._modelin_worker(task_state)),
            asyncio.create_task(self._tts_token_worker(task_state)),
            asyncio.create_task(self._duration_align_worker(task_state)),
            asyncio.create_task(self._audio_generation_worker(task_state)),
            asyncio.create_task(self._mixing_worker(task_state))
        ]

    async def stop_workers(self, task_state: TaskState):
        self.logger.info(f"[PipelineScheduler] stop_workers -> TaskID={task_state.task_id}")
        # 向translation_queue发送None，让整条流水线停止
        await task_state.translation_queue.put(None)
        await asyncio.gather(*self._workers, return_exceptions=True)
        self.logger.info(f"[PipelineScheduler] 所有Worker已结束 -> TaskID={task_state.task_id}")

    async def push_sentences_to_pipeline(self, task_state: TaskState, sentences: List[Sentence]):
        """
        把新产生的一批句子放进翻译队列
        """
        self.logger.debug(f"[push_sentences_to_pipeline] 放入 {len(sentences)} 个句子到 translation_queue, TaskID={task_state.task_id}")
        await task_state.translation_queue.put(sentences)

    # ------------------------------
    # 以下是各个 Worker 的实现
    # ------------------------------

    @worker_decorator(
        input_queue_attr='translation_queue',
        next_queue_attr='modelin_queue',
        worker_name='翻译Worker',
        mode='stream'
    )
    async def _translation_worker(self, sentences_list: List[Sentence], task_state: TaskState):
        if not sentences_list:
            return
        self.logger.debug(f"[翻译Worker] 收到 {len(sentences_list)} 句子, TaskID={task_state.task_id}")

        async for translated_batch in self.translator.translate_sentences(
            sentences_list,
            batch_size=self.config.TRANSLATION_BATCH_SIZE,
            target_language=task_state.target_language
        ):
            self.logger.debug(f"[翻译Worker] 翻译完成一批 -> size={len(translated_batch)}, TaskID={task_state.task_id}")
            yield translated_batch

    @worker_decorator(
        input_queue_attr='modelin_queue',
        next_queue_attr='tts_token_queue',
        worker_name='模型输入Worker',
        mode='stream'
    )
    async def _modelin_worker(self, sentences_batch: List[Sentence], task_state: TaskState):
        if not sentences_batch:
            return
        self.logger.debug(f"[模型输入Worker] 收到 {len(sentences_batch)} 句子, TaskID={task_state.task_id}")

        async for updated_batch in self.model_in.modelin_maker(
            sentences_batch,
            reuse_speaker=False,
            reuse_uuid=False,
            batch_size=self.config.MODELIN_BATCH_SIZE
        ):
            self.logger.debug(f"[模型输入Worker] 处理完成一批 -> size={len(updated_batch)}, TaskID={task_state.task_id}")
            yield updated_batch

    @worker_decorator(
        input_queue_attr='tts_token_queue',
        next_queue_attr='duration_align_queue',
        worker_name='TTS Token生成Worker'
    )
    async def _tts_token_worker(self, sentences_batch: List[Sentence], task_state: TaskState):
        if not sentences_batch:
            return
        self.logger.debug(f"[TTS Token生成Worker] 收到 {len(sentences_batch)} 句子, TaskID={task_state.task_id}")

        await self.tts_token_generator.tts_token_maker(sentences_batch, reuse_uuid=False)
        self.logger.debug(f"[TTS Token生成Worker] 已为本批次生成token -> size={len(sentences_batch)}, TaskID={task_state.task_id}")
        return sentences_batch

    @worker_decorator(
        input_queue_attr='duration_align_queue',
        next_queue_attr='audio_gen_queue',
        worker_name='时长对齐Worker'
    )
    async def _duration_align_worker(self, sentences_batch: List[Sentence], task_state: TaskState):
        if not sentences_batch:
            return
        self.logger.debug(f"[时长对齐Worker] 收到 {len(sentences_batch)} 句子, TaskID={task_state.task_id}")

        await self.duration_aligner.align_durations(sentences_batch)
        self.logger.debug(f"[时长对齐Worker] 对齐完成 -> size={len(sentences_batch)}, TaskID={task_state.task_id}")
        return sentences_batch

    @worker_decorator(
        input_queue_attr='audio_gen_queue',
        next_queue_attr='mixing_queue',
        worker_name='音频生成Worker'
    )
    async def _audio_generation_worker(self, sentences_batch: List[Sentence], task_state: TaskState):
        if not sentences_batch:
            return
        self.logger.debug(f"[音频生成Worker] 收到 {len(sentences_batch)} 句子, TaskID={task_state.task_id}")

        await self.audio_generator.vocal_audio_maker(sentences_batch)
        task_state.current_time = self.timestamp_adjuster.update_timestamps(sentences_batch, start_time=task_state.current_time)
        if not self.timestamp_adjuster.validate_timestamps(sentences_batch):
            self.logger.warning(f"[音频生成Worker] 检测到时间戳不连续, TaskID={task_state.task_id}")

        self.logger.debug(f"[音频生成Worker] 音频生成完成 -> size={len(sentences_batch)}, TaskID={task_state.task_id}")
        return sentences_batch

    @worker_decorator(
        input_queue_attr='mixing_queue',
        worker_name='混音Worker'
    )
    async def _mixing_worker(self, sentences_batch: List[Sentence], task_state: TaskState):
        if not sentences_batch:
            return
        seg_index = sentences_batch[0].segment_index
        self.logger.debug(f"[混音Worker] 收到 {len(sentences_batch)} 句, segment={seg_index}, TaskID={task_state.task_id}")

        output_path = task_state.task_paths.segments_dir / f"segment_{task_state.batch_counter}.mp4"

        success = await self.mixer.mixed_media_maker(
            sentences=sentences_batch,
            task_state=task_state,
            output_path=str(output_path)
        )

        if success and task_state.hls_manager:
            await task_state.hls_manager.add_segment(str(output_path), task_state.batch_counter)
            self.logger.info(f"[混音Worker] 分段 {task_state.batch_counter} 已加入 HLS, TaskID={task_state.task_id}")

        # 批次计数自增
        task_state.batch_counter += 1
        self.logger.debug(f"[混音Worker] 本批次混音完成 -> batch_counter={task_state.batch_counter}, TaskID={task_state.task_id}")
        return None

================
File: postcss.config.json
================
{ "plugins": { "postcss-preset-env": {}, "autoprefixer": {} } }

================
File: requirements.txt
================
--extra-index-url https://download.pytorch.org/whl/cu118
conformer==0.3.2
deepspeed==0.14.2; sys_platform == 'linux'
diffusers==0.27.2
gdown==5.1.0
gradio==4.32.2
grpcio==1.57.0
grpcio-tools==1.57.0
hydra-core==1.3.2
HyperPyYAML==1.2.2
inflect==7.3.1
librosa==0.10.2
lightning==2.2.4
matplotlib==3.7.5
modelscope==1.15.0
networkx==3.1
omegaconf==2.3.0
onnx==1.16.0
onnxruntime-gpu
openai-whisper==20231117
protobuf==4.25
pydantic==2.7.0
rich==13.7.1
soundfile==0.12.1
tensorboard==2.14.0
torch==2.0.1
torchaudio==2.0.2
uvicorn==0.30.0
wget==3.2
fastapi==0.111.0
fastapi-cli==0.0.4
WeTextProcessing==1.0.3
celery
redis
python-multipart
numpy
torch
torchaudio
soundfile
audio-separator
pydantic
pydantic-settings
python-magic
python-magic-bin; platform_system == "Windows"

================
File: video_translator.py
================
import logging
from typing import List, Dict, Any
from pathlib import Path

from core.auto_sense import SenseAutoModel
from models.CosyVoice.cosyvoice.cli.cosyvoice import CosyVoice2
from core.translation.translator import Translator
from core.translation.deepseek_client import DeepSeekClient
from core.tts_token_gener import TTSTokenGenerator
from core.audio_gener import AudioGenerator
from core.timeadjust.duration_aligner import DurationAligner
from core.timeadjust.timestamp_adjuster import TimestampAdjuster
from core.media_mixer import MediaMixer
from utils.media_utils import MediaUtils
from pipeline_scheduler import PipelineScheduler
from core.audio_separator import ClearVoiceSeparator
from core.model_in import ModelIn
from utils.task_storage import TaskPaths
from config import Config
from utils.task_state import TaskState

logger = logging.getLogger(__name__)

class ViTranslator:
    """
    全局持有大模型(ASR/TTS/翻译)对象, 每次 trans_video 时创建新的 TaskState + PipelineScheduler
    """
    def __init__(self, config: Config = None):
        self.logger = logger
        self.config = config or Config()
        self._init_global_models()

    def _init_global_models(self):
        self.logger.info("[ViTranslator] 初始化模型和工具...")

        # 音频分离器
        self.audio_separator = ClearVoiceSeparator(model_name='MossFormer2_SE_48K')

        # ASR + VAD + Speaker
        self.sense_model = SenseAutoModel(
            config=self.config,
            model="iic/SenseVoiceSmall",
            remote_code="./models/SenseVoice/model.py",
            vad_model="iic/speech_fsmn_vad_zh-cn-16k-common-pytorch",
            vad_kwargs={"max_single_segment_time": 30000},
            spk_model="cam++",
            trust_remote_code=True,
            disable_update=True,
            device="cuda"
        )

        # TTS 模型
        self.cosyvoice_model = CosyVoice2("models/CosyVoice/pretrained_models/CosyVoice2-0.5B")
        self.target_sr = self.cosyvoice_model.sample_rate

        # 媒体与管线相关工具
        self.media_utils = MediaUtils(config=self.config, audio_separator=self.audio_separator, target_sr=self.target_sr)
        self.model_in = ModelIn(self.cosyvoice_model)
        self.tts_generator = TTSTokenGenerator(self.cosyvoice_model, Hz=25)
        self.audio_generator = AudioGenerator(self.cosyvoice_model, sample_rate=self.target_sr)

        # 翻译模型
        translation_model = (self.config.TRANSLATION_MODEL or "deepseek").strip().lower()
        if translation_model == "deepseek":
            self.translator = Translator(DeepSeekClient(api_key=self.config.DEEPSEEK_API_KEY))
        else:
            raise ValueError(f"不支持的翻译模型：{translation_model}")

        # 其他处理
        self.duration_aligner = DurationAligner(
            model_in=self.model_in,
            simplifier=self.translator,
            tts_token_gener=self.tts_generator,
            max_speed=1.2
        )
        self.timestamp_adjuster = TimestampAdjuster(sample_rate=self.target_sr)
        self.mixer = MediaMixer(config=self.config, sample_rate=self.target_sr)

        self.logger.info("[ViTranslator] 初始化完成")

    async def trans_video(
        self,
        video_path: str,
        task_id: str,
        task_paths: TaskPaths,
        hls_manager=None,
        target_language="zh"
    ) -> Dict[str, Any]:
        """
        入口：对整段视频进行处理。包括分段、ASR、翻译、TTS、混音、生成 HLS 等。
        """
        self.logger.info(
            f"[trans_video] 开始处理视频: {video_path}, task_id={task_id}, target_language={target_language}"
        )

        # 初始化任务状态 + 管线
        task_state = TaskState(
            task_id=task_id,
            video_path=video_path,
            task_paths=task_paths,
            hls_manager=hls_manager,
            target_language=target_language
        )

        pipeline = PipelineScheduler(
            translator=self.translator,
            model_in=self.model_in,
            tts_token_generator=self.tts_generator,
            duration_aligner=self.duration_aligner,
            audio_generator=self.audio_generator,
            timestamp_adjuster=self.timestamp_adjuster,
            mixer=self.mixer,
            config=self.config
        )
        await pipeline.start_workers(task_state)

        try:
            # 1. 获取视频总时长
            duration = await self.media_utils.get_video_duration(video_path)
            # 2. 划分分段
            segments = await self.media_utils.get_audio_segments(duration)
            self.logger.info(f"总长度={duration:.2f}s, 分段数={len(segments)}, 任务ID={task_id}")

            if not segments:
                self.logger.warning(f"没有可用分段 -> 任务ID={task_id}")
                await pipeline.stop_workers(task_state)
                return {"status": "error", "message": "无法获取有效分段"}

            # 3. 遍历所有分段：提取、ASR、推送后续流水线
            for i, (seg_start, seg_dur) in enumerate(segments):
                await self._process_segment(pipeline, task_state, i, seg_start, seg_dur)

            # 4. 所有段结束后，停止流水线
            await pipeline.stop_workers(task_state)

            # 5. 如果有 HLS Manager，标记完成
            if hls_manager and hls_manager.has_segments:
                await hls_manager.finalize_playlist()
                self.logger.info(f"[trans_video] 任务ID={task_id} 完成并已生成HLS。")
                return {"status": "success", "message": "视频翻译完成"}
            else:
                self.logger.warning(f"任务ID={task_id} 没有可用片段写入HLS")
                return {"status": "error", "message": "没有成功写入HLS片段"}

        except Exception as e:
            self.logger.exception(f"[trans_video] 任务ID={task_id} 出错: {e}")
            return {"status": "error", "message": str(e)}

    async def _process_segment(
        self,
        pipeline: PipelineScheduler,
        task_state: TaskState,
        segment_index: int,
        start: float,
        seg_duration: float,
    ):
        """
        用于处理单个分段：提取音频/视频 -> ASR -> 推送后续翻译/合成的异步流水线
        """
        self.logger.info(
            f"[_process_segment] 任务ID={task_state.task_id}, segment_index={segment_index}, "
            f"start={start:.2f}, dur={seg_duration:.2f}"
        )
        # 1) 提取视频/音频/人声/背景音
        media_files = await self.media_utils.extract_segment(
            video_path=task_state.video_path,
            start=start,
            duration=seg_duration,
            output_dir=task_state.task_paths.processing_dir,
            segment_index=segment_index
        )
        task_state.segment_media_files[segment_index] = media_files

        # 2) 进行ASR（异步包装）
        sentences = await self.sense_model.generate_async(
            input=media_files['vocals'],
            cache={},
            language="auto",
            use_itn=True,
            batch_size_s=60,
            merge_vad=False
        )
        self.logger.info(f"[_process_segment] ASR识别到 {len(sentences)} 条句子, seg={segment_index}, TaskID={task_state.task_id}")

        if not sentences:
            return

        # 3) 给每条句子打上分段相关信息，再放入后续流水线
        for s in sentences:
            s.segment_index = segment_index
            s.segment_start = start
            s.task_id = task_state.task_id
            s.sentence_id = task_state.sentence_counter
            task_state.sentence_counter += 1

        await pipeline.push_sentences_to_pipeline(task_state, sentences)
