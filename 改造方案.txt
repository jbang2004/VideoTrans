================================================================
改造方案
================================================================
解耦 ViTranslator：移除对 Worker 的实例化和直接管理，仅推送任务到队列。
移除 PipelineScheduler：Worker 独立运行，依赖队列通信。
分布式 Worker：每个 Worker 监听特定队列，处理任务后推送到下游队列。
队列系统：使用 Redis 替代 asyncio.Queue，实现分布式任务管理。
状态管理：TaskState 移除队列属性，状态存储在文件或 Redis 中。
改造方案
1. 改造 video_translator.py
目标：ViTranslator 只负责初始化任务并推送到 segment_init_queue，等待任务完成。
步骤：
删除 _init_workers 方法和 Worker 实例化逻辑。
修改 trans_video 方法，仅推送任务到队列并等待 HLS 完成。
添加 Redis 队列推送方法和状态保存/加载逻辑。
改造后完整代码：
python
# video_translator.py
import logging
from typing import Dict, Any
from pathlib import Path
import asyncio
import json
import pickle
import aioredis
from config import Config
from utils.task_storage import TaskPaths
from utils.task_state import TaskState
from services.hls import HLSClient

logger = logging.getLogger(__name__)

class ViTranslator:
    def __init__(self, config: Config):
        self.config = config
        self.hls_client = None  # 异步初始化
        self.logger = logger

    async def trans_video(
        self,
        video_path: str,
        task_id: str,
        task_paths: TaskPaths,
        target_language: str = "zh",
        generate_subtitle: bool = False,
    ) -> Dict[str, Any]:
        """翻译视频并返回结果"""
        try:
            # 异步初始化 HLS 客户端
            self.hls_client = await HLSClient.create(self.config)

            # 初始化任务状态并存储
            task_state = TaskState(
                task_id=task_id,
                task_paths=task_paths,
                target_language=target_language,
                generate_subtitle=generate_subtitle
            )
            await self._save_task_state(task_state)

            # 初始化 HLS
            if not await self.hls_client.init_task(task_id):
                raise RuntimeError("HLS任务初始化失败")

            # 推送初始任务到队列
            await self._push_to_queue('segment_init_queue', {
                'task_id': task_id,
                'video_path': video_path
            })

            # 等待任务完成
            final_video_path = await self._wait_task_completion(task_id)
            if final_video_path and final_video_path.exists():
                return {
                    "status": "success",
                    "message": "视频翻译完成",
                    "final_video_path": str(final_video_path)
                }
            return {"status": "error", "message": "任务未完成"}
        except Exception as e:
            self.logger.exception(f"处理失败: {e}")
            if self.hls_client:
                await self.hls_client.cleanup_task(task_id)
            return {"status": "error", "message": str(e)}
        finally:
            if self.hls_client:
                await self.hls_client.close()

    async def _push_to_queue(self, queue_name: str, item: dict):
        """将任务推送到指定的 Redis 队列"""
        redis = await aioredis.create_redis_pool('redis://localhost')
        try:
            await redis.rpush(queue_name, json.dumps(item))
        finally:
            redis.close()
            await redis.wait_closed()

    async def _save_task_state(self, task_state: TaskState):
        """保存任务状态到文件"""
        task_dir = task_state.task_paths.task_dir
        task_dir.mkdir(parents=True, exist_ok=True)
        with open(task_dir / "task_state.pkl", "wb") as f:
            pickle.dump(task_state, f)

    async def _wait_task_completion(self, task_id: str) -> Path:
        """等待任务完成并返回最终视频路径"""
        while True:
            if await self.hls_client.finalize_task(task_id):
                task_state = await self._load_task_state(task_id)
                final_path = task_state.task_paths.output_dir / f"final_{task_id}.mp4"
                if final_path.exists():
                    return final_path
            await asyncio.sleep(1)

    async def _load_task_state(self, task_id: str) -> TaskState:
        """从文件加载任务状态"""
        state_file = self.config.TASKS_DIR / task_id / "task_state.pkl"
        with open(state_file, "rb") as f:
            return pickle.load(f)


if __name__ == "__main__":
    # 示例用法
    async def main():
        config = Config()
        translator = ViTranslator(config)
        task_id = "test_task"
        task_paths = TaskPaths(config, task_id)
        task_paths.create_directories()
        result = await translator.trans_video(
            video_path="sample.mp4",
            task_id=task_id,
            task_paths=task_paths
        )
        print(result)

    asyncio.run(main())
注意事项：
确保 Redis 服务已运行（redis://localhost）。
wait_task_completion 使用轮询检查 HLS 状态，可优化为监听 Redis 键值变化。
2. 移除 pipeline_scheduler.py
目标：由于 Worker 独立运行，PipelineScheduler 不再需要。
步骤：
删除 pipeline_scheduler.py 文件，或将其内容清空并添加注释。
改造后完整代码：
python
# pipeline_scheduler.py
"""PipelineScheduler 已移除，Worker 现独立运行并通过分布式队列管理任务。"""
注意事项：
检查项目中是否有其他文件引用 PipelineScheduler，需一并移除相关引用。
3. 改造 workers/segment_worker/worker.py
目标：SegmentWorker 独立监听 segment_init_queue 和 segment_queue，处理任务并推送到下游。
步骤：
添加 start 方法，监听队列并分别运行 run_init 和 run_extract。
使用 Redis 替代 asyncio.Queue，添加队列推送和状态管理方法。
移除对 PipelineScheduler 的依赖。
改造后完整代码：
根据您的要求，我将提供所有Worker的改造方案，确保从SegmentWorker到MixerWorker的每个模块都完整改造。改造后的Worker将基于Redis队列实现分布式任务处理，彼此解耦，并通过Redis存储任务状态。以下是改造后的完整方案和代码。
改造方案总览
1. 通用改造原则
队列监听：每个Worker通过start方法监听特定Redis队列（如segment_init_queue）。
任务处理：从队列中获取任务，加载TaskState，执行run方法，并将结果推送到下游队列。
状态管理：使用Redis存储和加载TaskState，确保Worker间状态一致性。
Redis连接：在start方法中创建连接池，任务完成后关闭。
2. 队列流转
任务从初始队列开始，依次流转至下游队列：
segment_init_queue → segment_queue → asr_queue → translation_queue → modelin_queue → tts_token_queue → duration_align_queue → audio_gen_queue → mixing_queue
3. Worker列表
以下是所有需要改造的Worker及其职责：
SegmentWorker：分段初始化和音频/视频提取
ASRWorker：语音识别
TranslationWorker：翻译
ModelInWorker：模型输入准备
TTSTokenWorker：TTS Token生成
DurationWorker：时长对齐
AudioGenWorker：音频生成
MixerWorker：混音与视频合并
改造后的Worker代码
1. SegmentWorker
功能：视频分段初始化和音频/视频提取
队列：segment_init_queue → segment_queue → asr_queue
python
# workers/segment_worker/worker.py
import asyncio
import json
import logging
import pickle
from pathlib import Path
import numpy as np
import soundfile as sf
import aioredis
from utils.decorators import worker_decorator
from utils.task_state import TaskState
from .audio_separator import ClearVoiceSeparator
from .video_segmenter import VideoSegmenter
from utils.ffmpeg_utils import FFmpegTool

logger = logging.getLogger(__name__)

class SegmentWorker:
    def __init__(self, config):
        self.config = config
        self.audio_separator = ClearVoiceSeparator(model_name='MossFormer2_SE_48K')
        self.ffmpeg_tool = FFmpegTool()
        self.video_segmenter = VideoSegmenter(config=config, ffmpeg_tool=self.ffmpeg_tool)
        self.logger = logger
        self.redis_pool = None

    async def start(self):
        """启动Worker，监听队列并处理任务"""
        self.redis_pool = await aioredis.create_redis_pool('redis://localhost')
        init_queue = 'segment_init_queue'
        extract_queue = 'segment_queue'
        asr_queue = 'asr_queue'

        async def run_init():
            """处理分段初始化任务"""
            while True:
                _, item_json = await self.redis_pool.blpop(init_queue)
                item = json.loads(item_json)
                if item is None:
                    await self._push_to_queue(extract_queue, None)
                    break
                task_state = await self._load_task_state(item['task_id'])
                async for segment in self.run_init(item, task_state):
                    await self._push_to_queue(extract_queue, segment)

        async def run_extract():
            """处理分段提取任务"""
            while True:
                _, item_json = await self.redis_pool.blpop(extract_queue)
                item = json.loads(item_json)
                if item is None:
                    await self._push_to_queue(asr_queue, None)
                    break
                task_state = await self._load_task_state(item['task_id'])
                result = await self.run_extract(item, task_state)
                await self._push_to_queue(asr_queue, result)

        await asyncio.gather(run_init(), run_extract())
        self.redis_pool.close()
        await self.redis_pool.wait_closed()

    @worker_decorator('segment_init_queue', 'segment_queue', '分段初始化Worker', mode='stream')
    async def run_init(self, item, task_state: TaskState):
        """初始化视频分段"""
        try:
            video_path = item['video_path']
            duration = await self.video_segmenter.get_video_duration(video_path)
            if duration <= 0:
                raise ValueError(f"无效的视频时长: {duration}s")
            segments = await self.video_segmenter.get_audio_segments(duration)
            self.logger.info(f"视频总长={duration:.2f}s, 分段数={len(segments)}")
            for i, (start, seg_duration) in enumerate(segments):
                yield {
                    'index': i,
                    'start': start,
                    'duration': seg_duration,
                    'video_path': video_path,
                    'task_id': task_state.task_id
                }
        except Exception as e:
            self.logger.error(f"分段初始化失败: {e}", exc_info=True)
            task_state.errors.append({'stage': 'segment_initialization', 'error': str(e)})
            await self._save_task_state(task_state)

    @worker_decorator('segment_queue', 'asr_queue', '分段提取Worker')
    async def run_extract(self, item, task_state: TaskState) -> dict:
        """提取视频分段的音频和视频"""
        try:
            index = item['index']
            start = item['start']
            duration = item['duration']
            video_path = item['video_path']

            self.logger.debug(f"开始处理分段 {index}, start={start:.2f}s, duration={duration:.2f}s")

            silent_video = str(task_state.task_paths.processing_dir / f"video_silent_{index}.mp4")
            full_audio = str(task_state.task_paths.processing_dir / f"audio_full_{index}.wav")
            vocals_audio = str(task_state.task_paths.processing_dir / f"vocals_{index}.wav")
            background_audio = str(task_state.task_paths.processing_dir / f"background_{index}.wav")

            await asyncio.gather(
                self.ffmpeg_tool.extract_audio(video_path, full_audio, start, duration),
                self.ffmpeg_tool.extract_video(video_path, silent_video, start, duration)
            )

            vocals, background, sr = await asyncio.to_thread(self.audio_separator.separate_audio, full_audio)
            background = await asyncio.to_thread(self._resample_audio_sync, sr, background, self.config.SAMPLE_RATE)

            await asyncio.gather(
                asyncio.to_thread(sf.write, vocals_audio, vocals, sr, subtype='FLOAT'),
                asyncio.to_thread(sf.write, background_audio, background, self.config.SAMPLE_RATE, subtype='FLOAT')
            )

            Path(full_audio).unlink(missing_ok=True)

            media_files = {
                'video': silent_video,
                'vocals': vocals_audio,
                'background': background_audio,
                'duration': len(vocals) / sr
            }
            task_state.segment_media_files[index] = media_files
            await self._save_task_state(task_state)

            return {
                'segment_index': index,
                'vocals_path': vocals_audio,
                'start': start,
                'duration': media_files['duration'],
                'task_id': task_state.task_id
            }
        except Exception as e:
            self.logger.error(f"分段提取失败: {e}", exc_info=True)
            task_state.errors.append({'stage': 'segment_extraction', 'error': str(e)})
            await self._save_task_state(task_state)
            return None

    async def _push_to_queue(self, queue_name: str, item):
        """推送任务到队列"""
        if item is not None:
            item_json = json.dumps(item)
        else:
            item_json = 'null'
        await self.redis_pool.rpush(queue_name, item_json)

    async def _load_task_state(self, task_id: str) -> TaskState:
        """从Redis加载任务状态"""
        data = await self.redis_pool.get(f'task_state:{task_id}')
        if data:
            return pickle.loads(data)
        raise ValueError(f"任务状态未找到: {task_id}")

    async def _save_task_state(self, task_state: TaskState):
        """保存任务状态到Redis"""
        await self.redis_pool.set(f'task_state:{task_state.task_id}', pickle.dumps(task_state))

    def _resample_audio_sync(self, fs: int, audio: np.ndarray, target_sr: int) -> np.ndarray:
        """同步重采样音频"""
        import torchaudio
        audio = audio.astype(np.float32)
        max_val = np.abs(audio).max()
        if max_val > 0:
            audio = audio / max_val
        if len(audio.shape) > 1:
            audio = audio.mean(axis=-1)
        if fs != target_sr:
            audio = np.ascontiguousarray(audio)
            resampler = torchaudio.transforms.Resample(orig_freq=fs, new_freq=target_sr, dtype=torch.float32)
            audio = resampler(torch.from_numpy(audio)[None, :])[0].numpy()
        return audio

if __name__ == "__main__":
    from config import Config
    config = Config()
    worker = SegmentWorker(config)
    asyncio.run(worker.start())
2. ASRWorker
功能：语音识别
队列：asr_queue → translation_queue
python
# workers/asr_worker/worker.py
import asyncio
import json
import logging
import pickle
import aioredis
from utils.decorators import worker_decorator
from utils.task_state import TaskState
from .auto_sense import SenseAutoModel

logger = logging.getLogger(__name__)

class ASRWorker:
    def __init__(self, config):
        self.config = config
        self.logger = logger
        self.sense_model = SenseAutoModel(config=config, **config.ASR_MODEL_KWARGS)
        self.redis_pool = None

    async def start(self):
        """启动Worker，监听ASR队列"""
        self.redis_pool = await aioredis.create_redis_pool('redis://localhost')
        asr_queue = 'asr_queue'
        translation_queue = 'translation_queue'

        async def run_asr():
            while True:
                _, item_json = await self.redis_pool.blpop(asr_queue)
                item = json.loads(item_json)
                if item is None:
                    await self._push_to_queue(translation_queue, None)
                    break
                task_state = await self._load_task_state(item['task_id'])
                result = await self.run(item, task_state)
                await self._push_to_queue(translation_queue, result)

        await run_asr()
        self.redis_pool.close()
        await self.redis_pool.wait_closed()

    @worker_decorator('asr_queue', 'translation_queue', 'ASR Worker')
    async def run(self, item: dict, task_state: TaskState):
        """执行ASR处理"""
        try:
            segment_info = item
            self.logger.debug(f"开始处理分段 {segment_info['segment_index']}")
            sentences = await self.sense_model.generate_async(
                input=segment_info['vocals_path'],
                cache={},
                language="auto",
                use_itn=True,
                batch_size_s=60,
                merge_vad=False
            )
            if not sentences:
                self.logger.warning(f"分段 {segment_info['segment_index']} 未识别到语音")
                return None

            self.logger.info(f"识别到 {len(sentences)} 条句子, seg={segment_info['segment_index']}")
            for s in sentences:
                s.segment_index = segment_info['segment_index']
                s.segment_start = segment_info['start']
                s.task_id = task_state.task_id
                s.sentence_id = task_state.sentence_counter
                task_state.sentence_counter += 1
            await self._save_task_state(task_state)
            return [s.__dict__ for s in sentences]
        except Exception as e:
            self.logger.error(f"ASR处理失败: {e}", exc_info=True)
            task_state.errors.append({
                'segment_index': item['segment_index'],
                'stage': 'asr',
                'error': str(e)
            })
            await self._save_task_state(task_state)
            return None

    async def _push_to_queue(self, queue_name: str, item):
        """推送任务到队列"""
        if item is not None:
            item_json = json.dumps(item)
        else:
            item_json = 'null'
        await self.redis_pool.rpush(queue_name, item_json)

    async def _load_task_state(self, task_id: str) -> TaskState:
        """从Redis加载任务状态"""
        data = await self.redis_pool.get(f'task_state:{task_id}')
        if data:
            return pickle.loads(data)
        raise ValueError(f"任务状态未找到: {task_id}")

    async def _save_task_state(self, task_state: TaskState):
        """保存任务状态到Redis"""
        await self.redis_pool.set(f'task_state:{task_state.task_id}', pickle.dumps(task_state))

if __name__ == "__main__":
    from config import Config
    config = Config()
    worker = ASRWorker(config)
    asyncio.run(worker.start())
3. TranslationWorker
功能：翻译
队列：translation_queue → modelin_queue
python
# workers/translation_worker/worker.py
import asyncio
import json
import logging
import pickle
import aioredis
from utils.decorators import worker_decorator
from utils.task_state import TaskState
from .translation.translator import Translator
from .translation.deepseek_client import DeepSeekClient
from .translation.gemini_client import GeminiClient

logger = logging.getLogger(__name__)

class TranslationWorker:
    def __init__(self, config):
        self.config = config
        self.logger = logger
        translation_model = config.TRANSLATION_MODEL.lower()
        if translation_model == "deepseek":
            client = DeepSeekClient(api_key=config.DEEPSEEK_API_KEY)
        elif translation_model == "gemini":
            client = GeminiClient(api_key=config.GEMINI_API_KEY)
        else:
            raise ValueError(f"不支持的翻译模型: {translation_model}")
        self.translator = Translator(translation_client=client)
        self.redis_pool = None

    async def start(self):
        """启动Worker，监听翻译队列"""
        self.redis_pool = await aioredis.create_redis_pool('redis://localhost')
        translation_queue = 'translation_queue'
        modelin_queue = 'modelin_queue'

        async def run_translation():
            while True:
                _, item_json = await self.redis_pool.blpop(translation_queue)
                item = json.loads(item_json)
                if item is None:
                    await self._push_to_queue(modelin_queue, None)
                    break
                task_state = await self._load_task_state(item[0]['task_id'])
                async for batch in self.run(item, task_state):
                    await self._push_to_queue(modelin_queue, batch)

        await run_translation()
        self.redis_pool.close()
        await self.redis_pool.wait_closed()

    @worker_decorator('translation_queue', 'modelin_queue', '翻译Worker', mode='stream')
    async def run(self, sentences_list, task_state: TaskState):
        """执行翻译任务"""
        if not sentences_list:
            return
        self.logger.debug(f"收到 {len(sentences_list)} 句子")
        async for translated_batch in self.translator.translate_sentences(
            sentences_list,
            batch_size=self.config.TRANSLATION_BATCH_SIZE,
            target_language=task_state.target_language
        ):
            yield translated_batch

    async def _push_to_queue(self, queue_name: str, item):
        """推送任务到队列"""
        if item is not None:
            item_json = json.dumps(item)
        else:
            item_json = 'null'
        await self.redis_pool.rpush(queue_name, item_json)

    async def _load_task_state(self, task_id: str) -> TaskState:
        """从Redis加载任务状态"""
        data = await self.redis_pool.get(f'task_state:{task_id}')
        if data:
            return pickle.loads(data)
        raise ValueError(f"任务状态未找到: {task_id}")

if __name__ == "__main__":
    from config import Config
    config = Config()
    worker = TranslationWorker(config)
    asyncio.run(worker.start())
4. ModelInWorker
功能：模型输入准备
队列：modelin_queue → tts_token_queue
python
# workers/modelin_worker/worker.py
import asyncio
import json
import logging
import pickle
import aioredis
from utils.decorators import worker_decorator
from utils.task_state import TaskState
from .model_in import ModelIn
from services.cosyvoice.client import CosyVoiceClient

logger = logging.getLogger(__name__)

class ModelInWorker:
    def __init__(self, config):
        self.config = config
        self.logger = logger
        cosyvoice_address = f"{config.COSYVOICE_SERVICE_HOST}:{config.COSYVOICE_SERVICE_PORT}"
        cosyvoice_client = CosyVoiceClient(address=cosyvoice_address)
        self.model_in = ModelIn(cosyvoice_client=cosyvoice_client)
        self.redis_pool = None

    async def start(self):
        """启动Worker，监听模型输入队列"""
        self.redis_pool = await aioredis.create_redis_pool('redis://localhost')
        modelin_queue = 'modelin_queue'
        tts_token_queue = 'tts_token_queue'

        async def run_modelin():
            while True:
                _, item_json = await self.redis_pool.blpop(modelin_queue)
                item = json.loads(item_json)
                if item is None:
                    await self._push_to_queue(tts_token_queue, None)
                    break
                task_state = await self._load_task_state(item[0]['task_id'])
                async for batch in self.run(item, task_state):
                    await self._push_to_queue(tts_token_queue, batch)

        await run_modelin()
        self.redis_pool.close()
        await self.redis_pool.wait_closed()

    @worker_decorator('modelin_queue', 'tts_token_queue', '模型输入Worker', mode='stream')
    async def run(self, sentences_batch, task_state: TaskState):
        """执行模型输入准备"""
        if not sentences_batch:
            return
        self.logger.debug(f"收到 {len(sentences_batch)} 句子")
        async for updated_batch in self.model_in.modelin_maker(
            sentences_batch,
            reuse_speaker=False,
            batch_size=self.config.MODELIN_BATCH_SIZE
        ):
            yield updated_batch

    async def _push_to_queue(self, queue_name: str, item):
        """推送任务到队列"""
        if item is not None:
            item_json = json.dumps(item)
        else:
            item_json = 'null'
        await self.redis_pool.rpush(queue_name, item_json)

    async def _load_task_state(self, task_id: str) -> TaskState:
        """从Redis加载任务状态"""
        data = await self.redis_pool.get(f'task_state:{task_id}')
        if data:
            return pickle.loads(data)
        raise ValueError(f"任务状态未找到: {task_id}")

if __name__ == "__main__":
    from config import Config
    config = Config()
    worker = ModelInWorker(config)
    asyncio.run(worker.start())
5. TTSTokenWorker
功能：TTS Token生成
队列：tts_token_queue → duration_align_queue
python
# workers/tts_worker/worker.py
import asyncio
import json
import logging
import pickle
import aioredis
from utils.decorators import worker_decorator
from utils.task_state import TaskState
from .tts_token_gener import TTSTokenGenerator
from services.cosyvoice.client import CosyVoiceClient

logger = logging.getLogger(__name__)

class TTSTokenWorker:
    def __init__(self, config):
        self.config = config
        self.logger = logger
        cosyvoice_address = f"{config.COSYVOICE_SERVICE_HOST}:{config.COSYVOICE_SERVICE_PORT}"
        cosyvoice_client = CosyVoiceClient(address=cosyvoice_address)
        self.tts_token_generator = TTSTokenGenerator(cosyvoice_client=cosyvoice_client)
        self.redis_pool = None

    async def start(self):
        """启动Worker，监听TTS Token队列"""
        self.redis_pool = await aioredis.create_redis_pool('redis://localhost')
        tts_token_queue = 'tts_token_queue'
        duration_align_queue = 'duration_align_queue'

        async def run_tts_token():
            while True:
                _, item_json = await self.redis_pool.blpop(tts_token_queue)
                item = json.loads(item_json)
                if item is None:
                    await self._push_to_queue(duration_align_queue, None)
                    break
                task_state = await self._load_task_state(item[0]['task_id'])
                result = await self.run(item, task_state)
                await self._push_to_queue(duration_align_queue, result)

        await run_tts_token()
        self.redis_pool.close()
        await self.redis_pool.wait_closed()

    @worker_decorator('tts_token_queue', 'duration_align_queue', 'TTS Token生成Worker')
    async def run(self, sentences_batch, task_state: TaskState):
        """执行TTS Token生成"""
        if not sentences_batch:
            return
        self.logger.debug(f"收到 {len(sentences_batch)} 句子")
        await self.tts_token_generator.tts_token_maker(sentences_batch)
        return sentences_batch

    async def _push_to_queue(self, queue_name: str, item):
        """推送任务到队列"""
        if item is not None:
            item_json = json.dumps(item)
        else:
            item_json = 'null'
        await self.redis_pool.rpush(queue_name, item_json)

    async def _load_task_state(self, task_id: str) -> TaskState:
        """从Redis加载任务状态"""
        data = await self.redis_pool.get(f'task_state:{task_id}')
        if data:
            return pickle.loads(data)
        raise ValueError(f"任务状态未找到: {task_id}")

if __name__ == "__main__":
    from config import Config
    config = Config()
    worker = TTSTokenWorker(config)
    asyncio.run(worker.start())
6. DurationWorker
功能：时长对齐
队列：duration_align_queue → audio_gen_queue
python
# workers/duration_worker/worker.py
import asyncio
import json
import logging
import pickle
import aioredis
from utils.decorators import worker_decorator
from utils.task_state import TaskState
from .duration_aligner import DurationAligner
from workers.modelin_worker.model_in import ModelIn
from workers.tts_worker.tts_token_gener import TTSTokenGenerator
from workers.translation_worker.translation.translator import Translator
from workers.translation_worker.translation.deepseek_client import DeepSeekClient
from workers.translation_worker.translation.gemini_client import GeminiClient
from services.cosyvoice.client import CosyVoiceClient

logger = logging.getLogger(__name__)

class DurationWorker:
    def __init__(self, config):
        self.config = config
        self.logger = logger
        cosyvoice_address = f"{config.COSYVOICE_SERVICE_HOST}:{config.COSYVOICE_SERVICE_PORT}"
        cosyvoice_client = CosyVoiceClient(address=cosyvoice_address)
        model_in = ModelIn(cosyvoice_client=cosyvoice_client, max_concurrent_tasks=config.MAX_PARALLEL_SEGMENTS)
        tts_token_generator = TTSTokenGenerator(cosyvoice_client=cosyvoice_client)
        translation_model = config.TRANSLATION_MODEL.lower()
        if translation_model == "deepseek":
            client = DeepSeekClient(api_key=config.DEEPSEEK_API_KEY)
        elif translation_model == "gemini":
            client = GeminiClient(api_key=config.GEMINI_API_KEY)
        else:
            raise ValueError(f"不支持的翻译模型: {translation_model}")
        simplifier = Translator(translation_client=client)
        self.duration_aligner = DurationAligner(
            model_in=model_in,
            simplifier=simplifier,
            tts_token_gener=tts_token_generator,
            max_speed=1.2
        )
        self.redis_pool = None

    async def start(self):
        """启动Worker，监听时长对齐队列"""
        self.redis_pool = await aioredis.create_redis_pool('redis://localhost')
        duration_align_queue = 'duration_align_queue'
        audio_gen_queue = 'audio_gen_queue'

        async def run_duration():
            while True:
                _, item_json = await self.redis_pool.blpop(duration_align_queue)
                item = json.loads(item_json)
                if item is None:
                    await self._push_to_queue(audio_gen_queue, None)
                    break
                task_state = await self._load_task_state(item[0]['task_id'])
                result = await self.run(item, task_state)
                await self._push_to_queue(audio_gen_queue, result)

        await run_duration()
        self.redis_pool.close()
        await self.redis_pool.wait_closed()

    @worker_decorator('duration_align_queue', 'audio_gen_queue', '时长对齐Worker')
    async def run(self, sentences_batch, task_state: TaskState):
        """执行时长对齐"""
        if not sentences_batch:
            return
        self.logger.debug(f"收到 {len(sentences_batch)} 句子")
        await self.duration_aligner.align_durations(sentences_batch)
        return sentences_batch

    async def _push_to_queue(self, queue_name: str, item):
        """推送任务到队列"""
        if item is not None:
            item_json = json.dumps(item)
        else:
            item_json = 'null'
        await self.redis_pool.rpush(queue_name, item_json)

    async def _load_task_state(self, task_id: str) -> TaskState:
        """从Redis加载任务状态"""
        data = await self.redis_pool.get(f'task_state:{task_id}')
        if data:
            return pickle.loads(data)
        raise ValueError(f"任务状态未找到: {task_id}")

if __name__ == "__main__":
    from config import Config
    config = Config()
    worker = DurationWorker(config)
    asyncio.run(worker.start())
7. AudioGenWorker
功能：音频生成
队列：audio_gen_queue → mixing_queue
python
# workers/audio_gen_worker/worker.py
import asyncio
import json
import logging
import pickle
import aioredis
from utils.decorators import worker_decorator
from utils.task_state import TaskState
from .audio_gener import AudioGenerator
from .timestamp_adjuster import TimestampAdjuster
from services.cosyvoice.client import CosyVoiceClient

logger = logging.getLogger(__name__)

class AudioGenWorker:
    def __init__(self, config):
        self.config = config
        self.logger = logger
        cosyvoice_address = f"{config.COSYVOICE_SERVICE_HOST}:{config.COSYVOICE_SERVICE_PORT}"
        cosyvoice_client = CosyVoiceClient(address=cosyvoice_address)
        self.audio_generator = AudioGenerator(cosyvoice_client=cosyvoice_client, sample_rate=config.SAMPLE_RATE)
        self.timestamp_adjuster = TimestampAdjuster(config=config)
        self.redis_pool = None

    async def start(self):
        """启动Worker，监听音频生成队列"""
        self.redis_pool = await aioredis.create_redis_pool('redis://localhost')
        audio_gen_queue = 'audio_gen_queue'
        mixing_queue = 'mixing_queue'

        async def run_audio_gen():
            while True:
                _, item_json = await self.redis_pool.blpop(audio_gen_queue)
                item = json.loads(item_json)
                if item is None:
                    await self._push_to_queue(mixing_queue, None)
                    break
                task_state = await self._load_task_state(item[0]['task_id'])
                result = await self.run(item, task_state)
                await self._push_to_queue(mixing_queue, result)

        await run_audio_gen()
        self.redis_pool.close()
        await self.redis_pool.wait_closed()

    @worker_decorator('audio_gen_queue', 'mixing_queue', '音频生成Worker')
    async def run(self, sentences_batch, task_state: TaskState):
        """执行音频生成"""
        if not sentences_batch:
            return
        self.logger.debug(f"收到 {len(sentences_batch)} 句子")
        await self.audio_generator.vocal_audio_maker(sentences_batch)
        task_state.current_time = self.timestamp_adjuster.update_timestamps(sentences_batch, start_time=task_state.current_time)
        valid = self.timestamp_adjuster.validate_timestamps(sentences_batch)
        if not valid:
            self.logger.warning("检测到时间戳不连续")
        return sentences_batch

    async def _push_to_queue(self, queue_name: str, item):
        """推送任务到队列"""
        if item is not None:
            item_json = json.dumps(item)
        else:
            item_json = 'null'
        await self.redis_pool.rpush(queue_name, item_json)

    async def _load_task_state(self, task_id: str) -> TaskState:
        """从Redis加载任务状态"""
        data = await self.redis_pool.get(f'task_state:{task_id}')
        if data:
            return pickle.loads(data)
        raise ValueError(f"任务状态未找到: {task_id}")

if __name__ == "__main__":
    from config import Config
    config = Config()
    worker = AudioGenWorker(config)
    asyncio.run(worker.start())
8. MixerWorker
功能：混音与视频合并
队列：mixing_queue（无下游）
python
# workers/mixer_worker/worker.py
import asyncio
import json
import logging
import pickle
from pathlib import Path
import aioredis
from utils.decorators import worker_decorator
from utils.task_state import TaskState
from .media_mixer import MediaMixer
from services.hls import HLSClient

logger = logging.getLogger(__name__)

class MixerWorker:
    def __init__(self, config, hls_service: HLSClient):
        self.config = config
        self.hls_service = hls_service
        self.logger = logger
        self.mixer = MediaMixer(config=config)
        self.redis_pool = None

    async def start(self):
        """启动Worker，监听混音队列"""
        self.redis_pool = await aioredis.create_redis_pool('redis://localhost')
        mixing_queue = 'mixing_queue'

        async def run_mixing():
            while True:
                _, item_json = await self.redis_pool.blpop(mixing_queue)
                item = json.loads(item_json)
                if item is None:
                    break
                task_state = await self._load_task_state(item[0]['task_id'])
                result = await self.run(item, task_state)
                if result:
                    task_state.merged_segments.append(result)
                    await self._save_task_state(task_state)

        await run_mixing()
        self.redis_pool.close()
        await self.redis_pool.wait_closed()

    @worker_decorator('mixing_queue', worker_name='混音Worker')
    async def run(self, sentences_batch, task_state: TaskState):
        """执行混音与视频合并"""
        if not sentences_batch:
            return
        seg_index = sentences_batch[0]['segment_index']
        self.logger.debug(f"收到 {len(sentences_batch)} 句, segment={seg_index}")

        output_path = task_state.task_paths.segments_dir / f"segment_{task_state.batch_counter}.mp4"

        success = await self.mixer.mixed_media_maker(
            sentences=sentences_batch,
            task_state=task_state,
            output_path=str(output_path),
            generate_subtitle=task_state.generate_subtitle
        )

        if success:
            added = await self.hls_service.add_segment(
                task_state.task_id,
                output_path,
                task_state.batch_counter
            )
            if added:
                self.logger.info(f"分段 {task_state.batch_counter} 已加入HLS")
                task_state.merged_segments.append(str(output_path))
            else:
                self.logger.error(f"分段 {task_state.batch_counter} 添加到HLS流失败")
        task_state.batch_counter += 1
        return str(output_path) if success else None

    async def _load_task_state(self, task_id: str) -> TaskState:
        """从Redis加载任务状态"""
        data = await self.redis_pool.get(f'task_state:{task_id}')
        if data:
            return pickle.loads(data)
        raise ValueError(f"任务状态未找到: {task_id}")

    async def _save_task_state(self, task_state: TaskState):
        """保存任务状态到Redis"""
        await self.redis_pool.set(f'task_state:{task_state.task_id}', pickle.dumps(task_state))

if __name__ == "__main__":
    from config import Config
    from services.hls import HLSClient
    config = Config()
    hls_client = HLSClient(config)
    worker = MixerWorker(config, hls_client)
    asyncio.run(worker.start())

所有Worker已按分布式队列管理方案改造完成：
独立性：每个Worker监听特定队列，彼此解耦。
状态管理：通过Redis存储TaskState，确保并发安全。
改造后完整代码：
python
# utils/task_state.py
from dataclasses import dataclass, field
from typing import Any, Dict, List
from utils.task_storage import TaskPaths

@dataclass
class TaskState:
    task_id: str
    task_paths: TaskPaths
    hls_manager: Any = None
    target_language: str = "zh"
    generate_subtitle: bool = False
    sentence_counter: int = 0
    current_time: float = 0
    batch_counter: int = 0
    segment_media_files: Dict[int, Dict[str, Any]] = field(default_factory=dict)
    merged_segments: List = field(default_factory=list)
    errors: List = field(default_factory=list)
注意事项：
确保 TaskState 可通过 pickle 序列化存储。

7. 改造 api.py
目标：保持 API 功能，调用改造后的 ViTranslator。
步骤：
更新 upload_video 方法，确保任务 ID 和路径正确传递。
改造后完整代码：
python
# api.py
import sys
from pathlib import Path
import logging
import uuid
import asyncio
from typing import Dict
import uvicorn
from fastapi import FastAPI, File, UploadFile, HTTPException, Request, Form
from fastapi.responses import JSONResponse, FileResponse
from fastapi.templating import Jinja2Templates
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
import aiofiles
from config import Config
from video_translator import ViTranslator
from utils.task_storage import TaskPaths

config = Config()
config.init_directories()

logging.basicConfig(
    level=logging.INFO,
    format="%(levelname)s | %(asctime)s | %(name)s | L%(lineno)d | %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)
logger = logging.getLogger(__name__)

app = FastAPI(debug=True)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

current_dir = Path(__file__).parent
templates = Jinja2Templates(directory=str(current_dir / "templates"))

vi_translator = ViTranslator(config=config)
task_results: Dict[str, dict] = {}

@app.get("/")
async def index(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

@app.post("/upload")
async def upload_video(
    video: UploadFile = File(...),
    target_language: str = Form("zh"),
    generate_subtitle: bool = Form(False),
):
    try:
        if not video:
            raise HTTPException(status_code=400, detail="没有文件上传")
        if not video.content_type.startswith('video/'):
            raise HTTPException(status_code=400, detail="只支持视频文件")
        if target_language not in ["zh", "en", "ja", "ko"]:
            raise HTTPException(status_code=400, detail=f"不支持的目标语言: {target_language}")

        task_id = str(uuid.uuid4())
        task_paths = TaskPaths(config, task_id)
        task_paths.create_directories()

        video_path = task_paths.input_dir / f"original_{video.filename}"
        async with aiofiles.open(video_path, "wb") as f:
            content = await video.read()
            await f.write(content)

        task = asyncio.create_task(vi_translator.trans_video(
            video_path=str(video_path),
            task_id=task_id,
            task_paths=task_paths,
            target_language=target_language,
            generate_subtitle=generate_subtitle,
        ))

        task_results[task_id] = {
            "status": "processing",
            "message": "视频处理中",
            "progress": 0
        }

        async def on_task_complete(t):
            try:
                result = await t
                if result.get('status') == 'success':
                    task_results[task_id].update({
                        "status": "success",
                        "message": "处理完成",
                        "progress": 100
                    })
                else:
                    task_results[task_id].update({
                        "status": "error",
                        "message": result.get('message', '处理失败'),
                        "progress": 0
                    })
            except Exception as e:
                logger.error(f"任务处理失败: {str(e)}")
                task_results[task_id].update({
                    "status": "error",
                    "message": str(e),
                    "progress": 0
                })

        task.add_done_callback(lambda t: asyncio.create_task(on_task_complete(t)))

        return {
            'status': 'processing',
            'task_id': task_id,
            'message': '视频上传成功，开始处理'
        }
    except HTTPException as e:
        raise e
    except Exception as e:
        logger.error(f"上传处理失败: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/task/{task_id}")
async def get_task_status(task_id: str):
    result = task_results.get(task_id)
    if not result:
        return {"status": "error", "message": "任务不存在", "progress": 0}
    return result

app.mount("/playlists", StaticFiles(directory=str(config.PUBLIC_DIR / "playlists")), name="playlists")
app.mount("/segments", StaticFiles(directory=str(config.PUBLIC_DIR / "segments")), name="segments")

@app.get("/download/{task_id}")
async def download_translated_video(task_id: str):
    final_video_path = config.TASKS_DIR / task_id / "output" / f"final_{task_id}.mp4"
    if not final_video_path.exists():
        raise HTTPException(status_code=404, detail="最终视频文件尚未生成或已被删除")
    return FileResponse(
        str(final_video_path),
        media_type='video/mp4',
        filename=f"final_{task_id}.mp4",
    )

if __name__ == "__main__":
    uvicorn.run(
        app,
        host=config.SERVER_HOST,
        port=config.SERVER_PORT,
        log_level="info"
    )
注意事项：
确保 task_results 与 trans_video 的状态更新一致。
运行方式
依赖项：确保所有Worker的依赖（如TaskState、Config、工具类等）已正确导入。
HLSClient：MixerWorker需要HLSClient实例，可在启动前初始化。
序列化：Sentence对象需确保可序列化为JSON（例如，将audio字段存储为文件路径）。
错误处理：每个Worker在异常时记录错误并继续处理后续任务。
运行方式
启动Redis：
bash
redis-server
启动所有Worker（每个Worker独立运行）：
bash
python -m workers.segment_worker.worker
python -m workers.asr_worker.worker
python -m workers.translation_worker.worker
python -m workers.modelin_worker.worker
python -m workers.tts_worker.worker
python -m workers.duration_worker.worker
python -m workers.audio_gen_worker.worker
python -m workers.mixer_worker.worker
推送初始任务：通过video_translator或其他入口推送任务到segment_init_queue。
安装 Redis：
安装并启动 Redis 服务（默认 localhost:6379）。
启动 Worker：
bash
python -m workers.segment_worker.worker
python -m workers.asr_worker.worker
python -m workers.translation_worker.worker
# ... 启动其他 Worker
启动 API：
bash
python api.py
运行服务：
确保 cosyvoice 和 hls 服务已启动（run_cosyvoice_service.py 和 run_hls_service.py）。
================================================================
end of 改造方案
================================================================